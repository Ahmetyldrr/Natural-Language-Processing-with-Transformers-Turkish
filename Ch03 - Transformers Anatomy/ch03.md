## The Transformer Architecture

# Transformer Mimarisi (Transformer Architecture)

Transformer mimarisi, makine çevirisi (machine translation) gibi görevler için yaygın olarak kullanılan kodlayıcı-kod çözücü (encoder-decoder) mimarisine dayanmaktadır. Bu mimari, iki ana bileşenden oluşur:

*   Kodlayıcı (Encoder): Giriş dizisini (input sequence) embedding vektörleri (embedding vectors) dizisine dönüştürür, buna gizli durum (hidden state) veya bağlam (context) denir.
*   Kod Çözücü (Decoder): Kodlayıcının gizli durumunu kullanarak, çıkış dizisini (output sequence) tek tek tokenler halinde oluşturur.

## Kodlayıcı ve Kod Çözücü Yapısı

Şekil 3-1'de gösterildiği gibi, kodlayıcı ve kod çözücü, birkaç yapı taşından oluşur. Kodlayıcı, kodlayıcı katmanları (encoder layers) veya "bloklar" (blocks) yığını olarak düşünülebilir, bu bilgisayarlı görüdeki (computer vision) evrişimli katmanları (convolutional layers) üst üste koymaya benzer. Kod çözücü de kendi kod çözücü katmanları yığınına sahiptir.

Kodlayıcının çıktısı her bir kod çözücü katmanına beslenir ve kod çözücü, dizideki en olası sonraki token için bir tahminde bulunur. Bu adımın çıktısı daha sonra bir sonraki tokeni oluşturmak için kod çözücüye geri beslenir ve bu, özel bir son-of-sequence (EOS) tokeni elde edilene kadar devam eder.

Örneğin, Şekil 3-1'deki örnekte, kod çözücünün zaten "Die" ve "Zeit" kelimelerini tahmin ettiğini varsayalım. Şimdi, kod çözücü bu iki kelimeyi girdi olarak alır ve kodlayıcının tüm çıktılarıyla birlikte bir sonraki tokeni, "fliegt"i tahmin eder. Bir sonraki adımda, kod çözücü "fliegt"i ek girdi olarak alır. EOS tokeni tahmin edilene veya maksimum uzunluğa ulaşılana kadar bu işlemi tekrarlarız.

## Transformer Mimarisi Türleri

Transformer mimarisi başlangıçta makine çevirisi gibi dizi-dizi (sequence-to-sequence) görevleri için tasarlanmıştı, ancak hem kodlayıcı hem de kod çözücü blokları kısa süre sonra bağımsız modeller olarak uyarlandı. Yüzlerce farklı Transformer modeli olmasına rağmen, çoğu üç türden birine aittir:

*   **Kodlayıcı-Tabanlı (Encoder-Only) Modeller**: Giriş metnini zengin bir sayısal gösterime (numerical representation) dönüştürür, bu gösterim metin sınıflandırma (text classification) veya adlandırılmış varlık tanıma (named entity recognition) gibi görevler için uygundur. BERT ve varyantları (RoBERTa, DistilBERT gibi) bu mimari sınıfına aittir. Bu mimaride, belirli bir token için hesaplanan gösterim, hem soldaki (tokenin öncesindeki) hem de sağdaki (tokenin sonrasındaki) bağlama bağlıdır. Buna çift yönlü dikkat (bidirectional attention) denir.
*   **Kod Çözücü-Tabanlı (Decoder-Only) Modeller**: "Thanks for lunch, I had a…" gibi bir metin verildiğinde, bu modeller diziyi otomatik olarak tamamlar ve en olası sonraki kelimeyi yinelemeli olarak tahmin eder. GPT modelleri ailesi bu sınıfa aittir. Bu mimaride, belirli bir token için hesaplanan gösterim yalnızca soldaki bağlama bağlıdır. Buna nedensel (causal) veya otoregresif (autoregressive) dikkat denir.
*   **Kodlayıcı-Kod Çözücü (Encoder-Decoder) Modeller**: Bir metin dizisinden diğerine karmaşık eşlemeler modellemek için kullanılır; makine çevirisi ve özetleme (summarization) görevleri için uygundurlar. BART ve T5 modelleri bu sınıfa aittir.

Uygulamada, kod çözücü-tabanlı ve kodlayıcı-tabanlı mimariler arasındaki ayrım biraz bulanıktır. Örneğin, GPT ailesindeki gibi kod çözücü-tabanlı modeller, geleneksel olarak dizi-dizi görevleri olarak düşünülen çeviri gibi görevler için hazırlanabilir. Benzer şekilde, BERT gibi kodlayıcı-tabanlı modeller, genellikle kodlayıcı-kod çözücü veya kod çözücü-tabanlı modellerle ilişkilendirilen özetleme görevlerine uygulanabilir.

## Kod Örneği ve Açıklaması

Aşağıdaki kod örneği, Transformer mimarisinin temel bileşenlerini göstermektedir:
```python
import torch
import torch.nn as nn
import torch.optim as optim

# Token embedding boyutu
token_embedding_dim = 512

# Pozisyonel embedding boyutu
positional_embedding_dim = 512

# Token embedding katmanı
token_embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=token_embedding_dim)

# Pozisyonel embedding katmanı
positional_embedding_layer = nn.Embedding(num_embeddings=512, embedding_dim=positional_embedding_dim)

# Giriş dizisi
input_sequence = torch.tensor([1, 2, 3, 4, 5])

# Token embedding işlemi
token_embeddings = token_embedding_layer(input_sequence)

# Pozisyonel embedding işlemi
positional_embeddings = positional_embedding_layer(torch.arange(len(input_sequence)))

# Embedding vektörlerinin toplanması
input_embeddings = token_embeddings + positional_embeddings

# Kodlayıcı katmanı
encoder_layer = nn.TransformerEncoderLayer(d_model=token_embedding_dim, nhead=8)

# Kodlayıcı
encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)

# Kodlayıcı işlemi
encoder_output = encoder(input_embeddings)

# Kod çözücü katmanı
decoder_layer = nn.TransformerDecoderLayer(d_model=token_embedding_dim, nhead=8)

# Kod çözücü
decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)

# Kod çözücü işlemi
decoder_output = decoder(encoder_output, input_embeddings)

print(decoder_output.shape)
```
Kodun satır satır açıklaması:

1.  `token_embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=token_embedding_dim)`: Token embedding katmanını tanımlar. `num_embeddings` parametresi, toplam token sayısını; `embedding_dim` parametresi, embedding vektörünün boyutunu belirtir.
2.  `positional_embedding_layer = nn.Embedding(num_embeddings=512, embedding_dim=positional_embedding_dim)`: Pozisyonel embedding katmanını tanımlar. `num_embeddings` parametresi, maksimum dizi uzunluğunu; `embedding_dim` parametresi, embedding vektörünün boyutunu belirtir.
3.  `input_sequence = torch.tensor([1, 2, 3, 4, 5])`: Giriş dizisini tanımlar.
4.  `token_embeddings = token_embedding_layer(input_sequence)`: Token embedding işlemini gerçekleştirir.
5.  `positional_embeddings = positional_embedding_layer(torch.arange(len(input_sequence)))`: Pozisyonel embedding işlemini gerçekleştirir.
6.  `input_embeddings = token_embeddings + positional_embeddings`: Token embedding ve pozisyonel embedding vektörlerini toplar.
7.  `encoder_layer = nn.TransformerEncoderLayer(d_model=token_embedding_dim, nhead=8)`: Kodlayıcı katmanını tanımlar. `d_model` parametresi, embedding vektörünün boyutunu; `nhead` parametresi, dikkat başlığı (attention head) sayısını belirtir.
8.  `encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)`: Kodlayıcıyı tanımlar. `num_layers` parametresi, kodlayıcı katmanlarının sayısını belirtir.
9.  `encoder_output = encoder(input_embeddings)`: Kodlayıcı işlemini gerçekleştirir.
10. `decoder_layer = nn.TransformerDecoderLayer(d_model=token_embedding_dim, nhead=8)`: Kod çözücü katmanını tanımlar.
11. `decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)`: Kod çözücüyü tanımlar.
12. `decoder_output = decoder(encoder_output, input_embeddings)`: Kod çözücü işlemini gerçekleştirir.

Bu kod örneği, Transformer mimarisinin temel bileşenlerini ve işlemlerini göstermektedir.

---

## The Encoder

# Transformer Encoder (Dönüştürücü Kodlayıcı)

Transformer encoder (dönüştürücü kodlayıcı), birçok encoder katmanının (kodlayıcı katmanları) birbirine bağlı olarak sıralanmasıyla oluşur. Her bir encoder katmanı, bir dizi embedding (gömme) alır ve bunları aşağıdaki alt katmanlardan geçirir:

*   Çok başlı öz dikkat katmanı (Multi-head self-attention layer)
*   Her bir girdi embeddingine (gömme) uygulanan tam bağlı bir ileri beslemeli ağ (Fully connected feed-forward layer)

Her bir encoder katmanının çıktı embeddingleri, girdilerle aynı boyuttadır ve encoder yığınının ana rolü, dizideki bazı bağlamsal bilgileri kodlayan temsiller üretmek için girdi embeddinglerini "güncellemek" tir. Örneğin, "elma" kelimesi, yakında "başlangıç" veya "telefon" kelimeleri varsa, daha çok "şirket" gibi ve daha az "meyve" gibi güncellenir.

## Öz Dikkat Katmanı (Self-Attention Layer)

Öz dikkat, sinir ağlarının bir dizideki her bir elemana farklı bir ağırlık veya "dikkat" atamasına izin veren bir mekanizmadır. Metin dizileri için elemanlar, token embeddingleridir (kelime gömmeleri). Öz dikkatteki "öz" kısmı, bu ağırlıkların aynı setteki tüm gizli durumlar için hesaplanmasını ifade eder.

Öz dikkatin arkasındaki ana fikir, her bir token için sabit bir embedding kullanmak yerine, tüm diziyi kullanarak her bir embeddingin ağırlıklı ortalamasını hesaplamaktır. Başka bir deyişle, x1, ..., xn token embeddingleri dizisi verildiğinde, öz dikkat, her bir xi'nin tüm xj'lerin doğrusal bir kombinasyonu olduğu yeni bir x1', ..., xn' embedding dizisi üretir.

Bu işlemin bir diyagramı Şekil 3-3'te gösterilmektedir. Burada, bağlama bağlı olarak "sinekler" için iki farklı temsilin nasıl üretilebildiği gösterilmektedir.

## Ölçekli Nokta Ürün Dikkat (Scaled Dot-Product Attention)

Öz dikkat katmanını uygulamak için birkaç yol vardır, ancak en yaygın olanı, Transformer mimarisini tanıtan makaleden ölçekli nokta ürün dikkatidir (scaled dot-product attention). Bu mekanizmayı uygulamak için dört ana adım vardır:

1.  Her bir token embeddingini query (sorgu), key (anahtar) ve value (değer) vektörlerine yansıtın.
2.  Dikkat puanlarını (attention scores) hesaplayın. Query ve key vektörlerinin birbirleriyle ne kadar ilgili olduğunu belirlemek için bir benzerlik fonksiyonu kullanın. Ölçekli nokta ürün dikkatinde benzerlik fonksiyonu, embeddinglerin matris çarpımı kullanılarak verimli bir şekilde hesaplanan nokta ürünüdür.
3.  Dikkat ağırlıklarını (attention weights) hesaplayın. Nokta ürünleri genel olarak keyfi olarak büyük sayılar üretebilir, bu da eğitim sürecini istikrarsızlaştırabilir. Bunu ele almak için, dikkat puanları önce varyanslarını normalize etmek için bir ölçekleme faktörü ile çarpılır ve ardından tüm sütun değerlerinin 1'e toplanmasını sağlamak için bir softmax ile normalize edilir.
4.  Token embeddinglerini güncelleyin. Dikkat ağırlıkları hesaplandıktan sonra, xi' = ∑j wjivj formülünü kullanarak güncellenmiş bir temsil elde etmek için bunları value vektörleriyle çarparız.

### Kod

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from math import sqrt

def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    weights = F.softmax(scores, dim=-1)
    return torch.bmm(weights, value)

class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim)
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)

    def forward(self, hidden_state):
        attn_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))
        return attn_outputs

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state):
        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x
```

### Kod Açıklaması

*   `scaled_dot_product_attention` fonksiyonu, ölçekli nokta ürün dikkatini hesaplar. Query, key ve value vektörlerini girdi olarak alır ve güncellenmiş token embeddinglerini döndürür.
*   `AttentionHead` sınıfı, tek bir dikkat başını temsil eder. Her bir dikkat başı, query, key ve value vektörlerini hesaplamak için üç ayrı doğrusal katman kullanır.
*   `MultiHeadAttention` sınıfı, çok başlı dikkat katmanını temsil eder. Birden fazla dikkat başının çıktılarını birleştirir ve nihai çıktıyı üretmek için bir doğrusal katman kullanır.

## Konumsal Kodlamalar (Positional Encodings)

Transformer encoder katmanları, tokenlerin sırasını dikkate almaz. Bu sorunu çözmek için, token embeddinglerine konumsal kodlamalar eklenir. Konumsal kodlamalar, her bir tokenin konumunu temsil eden bir vektördür.

### Kod

```python
class Embeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout()

    def forward(self, input_ids):
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embed_embeddings)
        return embeddings
```

### Kod Açıklaması

*   `Embeddings` sınıfı, token embeddingleri ve konumsal kodlamaları birleştirir. Token embeddingleri, girdi token ID'lerini embedding vektörlerine dönüştürür. Konumsal kodlamalar, tokenlerin konumunu temsil eden vektörlerdir.

## Transformer Encoder (Dönüştürücü Kodlayıcı)

Transformer encoder, embedding katmanı ve birden fazla encoder katmanından oluşur.

### Kod

```python
class TransformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = Embeddings(config)
        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])

    def forward(self, x):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x)
        return x
```

### Kod Açıklaması

*   `TransformerEncoder` sınıfı, Transformer encoder mimarisini temsil eder. Embedding katmanı ve birden fazla encoder katmanından oluşur.

## Sınıflandırma Başı (Classification Head)

Sınıflandırma başı, Transformer encoder çıktısını alır ve sınıflandırma sonuçlarını üretir.

### Kod

```python
class TransformerForSequenceClassification(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = TransformerEncoder(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, x):
        x = self.encoder(x)[:, 0, :]
        x = self.dropout(x)
        x = self.classifier(x)
        return x
```

### Kod Açıklaması

*   `TransformerForSequenceClassification` sınıfı, sınıflandırma başını temsil eder. Transformer encoder çıktısını alır, dropout uygular ve sınıflandırma sonuçlarını üretmek için bir doğrusal katman kullanır.

---

## The Decoder

# Decoder (Çözücü) Mimarisi
Decoder (çözücü), encoder (kodlayıcı)dan farklı olarak iki adet dikkat alt katmanına (attention sublayer) sahiptir. Bu alt katmanlar, decoder'ın her bir zaman adımında (timestep) yalnızca geçmiş çıktılara ve tahmin edilen mevcut tokene bağlı olmasını sağlar.

## Maskeli Öz-Dikkat (Masked Self-Attention)
Decoder'ın öz-dikkat katmanında, gelecekteki tokenlere (gelecekteki kelimelere) bakmasını engellemek için bir maske matrisi kullanılır. Bu maske matrisi, alt üçgensel bir matris olup, üst üçgensel kısmı sıfırlarla doldurulur.

```python
seq_len = inputs.input_ids.size(-1)
mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)
```

Bu kodda:
- `seq_len`: Giriş dizisinin uzunluğunu temsil eder.
- `torch.tril()`: Alt üçgensel matrisi oluşturur.
- `unsqueeze(0)`: Matrisi bir boyut ekleyerek genişletir.

Maske matrisi örneği:
```python
mask[0]
tensor([[1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1.]])
```

## Dikkat Skorlarının Maskelenmesi
Maske matrisi kullanılarak, dikkat skorları (attention scores) maskelenir. Bu işlem, gelecekteki tokenlere karşılık gelen skorları negatif sonsuza (-∞) ayarlar.

```python
scores.masked_fill(mask == 0, -float("inf"))
```

Bu kodda:
- `scores`: Dikkat skorlarını temsil eder.
- `masked_fill()`: Belirtilen maske değerlerine göre skorları doldurur.

Maskelenmiş skorlar örneği:
```python
tensor([[[26.8082,    -inf,    -inf,    -inf,    -inf],
         [-0.6981, 26.9043,    -inf,    -inf,    -inf],
         [-2.3190,  1.2928, 27.8710,    -inf,    -inf],
         [-0.5897,  0.3497, -0.3807, 27.5488,    -inf],
         [ 0.5275,  2.0493, -0.4869,  1.6100, 29.0893]]],
       grad_fn=<MaskedFillBackward0>)
```

## Ölçekli Nokta Ürün Dikkat Fonksiyonu (Scaled Dot-Product Attention Function)
Decoder'ın dikkat mekanizması, ölçekli nokta ürün dikkat fonksiyonu kullanılarak uygulanır.

```python
def scaled_dot_product_attention(query, key, value, mask=None):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    weights = F.softmax(scores, dim=-1)
    return weights.bmm(value)
```

Bu kodda:
- `query`, `key`, `value`: Dikkat mekanizmasının girdilerini temsil eder.
- `mask`: Maske matrisini temsil eder.
- `dim_k`: `query` ve `key` vektörlerinin boyutunu temsil eder.
- `torch.bmm()`: Toplu matris çarpımı işlemini gerçekleştirir.
- `F.softmax()`: Softmax aktivasyon fonksiyonunu uygular.

## Encoder-Decoder Dikkat (Encoder-Decoder Attention)
Encoder-decoder dikkat katmanı, decoder'ın ara temsillerini (intermediate representations) sorgu (query) olarak, encoder'ın çıktı anahtar (key) ve değer (value) vektörlerini kullanarak çok başlı dikkat (multi-head attention) uygular.

Bu sayede, encoder-decoder dikkat katmanı, iki farklı dizilimdeki (örneğin, iki farklı dildeki) tokenler arasındaki ilişkiyi öğrenir.

## Özet
Decoder, maskeli öz-dikkat ve encoder-decoder dikkat katmanlarını kullanarak, girdi dizilimini işler ve çıktı dizilimini oluşturur. Bu işlem, Transformer mimarisinin temel bileşenlerinden biridir.

---

## Meet the Transformers

# Transformatör Modelleri (Transformer Models)

Transformatör modelleri, doğal dil işleme (NLP) alanında kullanılan bir tür yapay zeka modelidir. Bu modeller, encoder, decoder ve encoder-decoder olmak üzere üç ana mimariye sahiptir.

## Encoder Modelleri (Encoder Models)

Encoder modelleri, metin sınıflandırma, adlandırılmış varlık tanıma ve soru cevaplama gibi doğal dil anlama (NLU) görevlerinde kullanılır.

*   **BERT (Bidirectional Encoder Representations from Transformers)**: BERT, encoder-only bir modeldir ve iki ön-eğitim hedefi kullanır: maskeli dil modelleme (MLM) ve sonraki cümle tahmini (NSP).
*   **DistilBERT**: DistilBERT, BERT'in daha küçük ve daha hızlı bir versiyonudur. Bilgi damıtma (knowledge distillation) tekniği kullanılarak elde edilmiştir.
*   **RoBERTa**: RoBERTa, BERT'in daha uzun süre eğitilmiş ve daha büyük veri kümeleri kullanılarak elde edilmiş bir versiyonudur.
*   **XLM (Cross-lingual Language Model)**: XLM, çok dilli ön-eğitim için kullanılan bir modeldir. Çeşitli ön-eğitim hedefleri kullanır, including autoregressive language modeling ve MLM.
*   **XLM-RoBERTa (XLM-R)**: XLM-R, XLM ve RoBERTa'nın birleşimi olan bir modeldir. Çok dilli ön-eğitim için kullanılır.
*   **ALBERT**: ALBERT, encoder mimarisini daha verimli hale getirmek için üç değişiklik yapan bir modeldir: token embedding boyutunu hidden boyuttan ayırma, tüm katmanlarda aynı parametreleri paylaşma ve NSP hedefini cümle sırası tahmini ile değiştirme.
*   **ELECTRA**: ELECTRA, iki-model yaklaşımı kullanan bir modeldir. İlk model, maskeli dil modelleme yapar ve ikinci model, ilk modelin çıktısındaki tokenlerin orijinal olarak maskelenip maskelenmediğini tahmin eder.
*   **DeBERTa**: DeBERTa, her tokenin iki vektörle temsil edildiği bir modeldir: biri içerik için, diğeri göreli pozisyon için.

## Decoder Modelleri (Decoder Models)

Decoder modelleri, metin oluşturma görevlerinde kullanılır.

*   **GPT (Generative Pre-trained Transformer)**: GPT, decoder-only bir modeldir ve ön-eğitim için bir sonraki kelimeyi tahmin etme hedefi kullanır.
*   **GPT-2**: GPT-2, GPT'nin daha büyük ve daha güçlü bir versiyonudur.
*   **CTRL (Conditional Transformer Language Model)**: CTRL, GPT-2'ye benzer, ancak "kontrol tokenları" kullanarak oluşturulan metnin stilini kontrol etmeyi sağlar.
*   **GPT-3**: GPT-3, GPT-2'nin daha da büyük ve daha güçlü bir versiyonudur. 175 milyar parametreye sahiptir ve few-shot learning capabilities gösterir.
*   **GPT-Neo ve GPT-J-6B**: GPT-Neo ve GPT-J-6B, EleutherAI tarafından geliştirilen GPT-3 benzeri modellerdir.

## Encoder-Decoder Modelleri (Encoder-Decoder Models)

Encoder-decoder modelleri, hem NLU hem de NLG görevlerinde kullanılır.

*   **T5 (Text-to-Text Transfer Transformer)**: T5, tüm NLU ve NLG görevlerini metin-metine görevi olarak çerçeveleyen bir modeldir. Encoder-decoder mimarisini kullanır.
*   **BART**: BART, BERT ve GPT'nin ön-eğitim prosedürlerini encoder-decoder mimarisinde birleştiren bir modeldir.
*   **M2M-100**: M2M-100, 100 dil arasında çeviri yapabilen bir çeviri modelidir.
*   **BigBird**: BigBird, dikkat mekanizmasının quadratic bellek gereksinimlerini azaltmak için seyrek dikkat kullanan bir modeldir.

### Kodlar ve Açıklamaları

Paragrafta direkt bir kod geçmemektedir ancak transformer modelleri ile alakalı örnek bir kod aşağıdaki gibidir 

```python
from transformers import BertTokenizer, BertModel

# BERT tokenizer ve modelini yükle
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Metni tokenize et
inputs = tokenizer("Merhaba, dünya!", return_tensors="pt")

# Modeli kullanarak çıktı elde et
outputs = model(**inputs)

# Çıktıyı yazdır
print(outputs.last_hidden_state[:, 0, :])
```

*   `BertTokenizer.from_pretrained('bert-base-uncased')`: BERT tokenizerını yükler.
*   `BertModel.from_pretrained('bert-base-uncased')`: BERT modelini yükler.
*   `tokenizer("Merhaba, dünya!", return_tensors="pt")`: Metni tokenize eder ve PyTorch tensorları olarak döndürür.
*   `model(**inputs)`: Tokenize edilmiş metni modele verir ve çıktı elde eder.
*   `outputs.last_hidden_state[:, 0, :]`: Çıktının son hidden katmanının ilk tokeninin (CLS tokeni) temsilini verir.

Bu kod, BERT modelini kullanarak bir metnin temsilini elde etmek için kullanılır.

---

## Conclusion

# Transformer Mimarisi ve Uygulamaları

Bu bölümde, Transformer mimarisinin (Transformer Architecture) kalbinde yer alan öz-dikkat (self-attention) mekanizmasını derinlemesine inceledik ve ardından bir Transformer kodlayıcı (encoder) modeli oluşturmak için gerekli tüm bileşenleri ekledik. 

## Öz-Dikkat ve Transformer Kodlayıcı

Transformer kodlayıcı modeli oluştururken, tokenler (token) ve pozisyonel bilgi (positional information) için gömme katmanları (embedding layers) ekledik. Ayrıca, dikkat başlıklarını (attention heads) tamamlamak için bir ileri beslemeli katman (feed-forward layer) ekledik ve son olarak, tahminlerde bulunmak için model gövdesine bir sınıflandırma başlığı (classification head) ekledik.

## Kodlayıcı ve Çözücü

Transformer mimarisinin kodlayıcı tarafını ayrıntılı olarak inceledik ve daha sonra çözücü (decoder) tarafına da göz attık. Bölümü, en önemli model mimarilerinin (model architectures) bir özeti ile sonlandırdık.

## Önemli Noktalar

*   Öz-dikkat mekanizması (Self-Attention Mechanism)
*   Gömme katmanları (Embedding Layers)
*   İleri beslemeli katman (Feed-Forward Layer)
*   Dikkat başlıkları (Attention Heads)
*   Sınıflandırma başlığı (Classification Head)
*   Kodlayıcı ve Çözücü mimarisi (Encoder-Decoder Architecture)

Artık altta yatan ilkeleri daha iyi anladığımıza göre, basit sınıflandırmadan (simple classification) daha öteye giderek çok dilli adlandırılmış varlık tanıma (multilingual named entity recognition) modeli oluşturalım.

Bu bölümde kullanılan kodlar ve açıklamaları aşağıda verilmiştir:

Bu bölümde spesifik bir kod örneği bulunmamaktadır. Ancak, Transformer modeli oluşturmak için genel olarak kullanılan PyTorch kütüphanesine ait kod yapısı aşağıda verilmiştir:

```python
import torch
import torch.nn as nn
import torch.optim as optim

class TransformerModel(nn.Module):
    def __init__(self):
        super(TransformerModel, self).__init__()
        self.encoder = nn.TransformerEncoderLayer(d_model=512, nhead=8)
        self.decoder = nn.TransformerDecoderLayer(d_model=512, nhead=8)
        self.embedding = nn.Embedding(num_embeddings=10000, embedding_dim=512)
        self.classification_head = nn.Linear(512, 8)

    def forward(self, x):
        x = self.embedding(x)
        x = self.encoder(x)
        x = self.decoder(x)
        x = self.classification_head(x)
        return x

# Model oluşturma
model = TransformerModel()

# Optimizer ve loss fonksiyonu tanımlama
optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

# Eğitim döngüsü
for epoch in range(10):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()
```

Kod Açıklamaları:

1.  `import torch`: PyTorch kütüphanesini içe aktarır.
2.  `import torch.nn as nn`: PyTorch'un sinir ağları modülünü `nn` takma adı ile içe aktarır.
3.  `import torch.optim as optim`: PyTorch'un optimizasyon algoritmaları modülünü `optim` takma adı ile içe aktarır.
4.  `class TransformerModel(nn.Module)`: `nn.Module` sınıfından miras alan `TransformerModel` adlı bir sınıf tanımlar.
5.  `self.encoder = nn.TransformerEncoderLayer(d_model=512, nhead=8)`: Transformer kodlayıcı katmanını tanımlar.
6.  `self.decoder = nn.TransformerDecoderLayer(d_model=512, nhead=8)`: Transformer çözücü katmanını tanımlar.
7.  `self.embedding = nn.Embedding(num_embeddings=10000, embedding_dim=512)`: Gömme katmanını tanımlar.
8.  `self.classification_head = nn.Linear(512, 8)`: Sınıflandırma başlığını tanımlar.
9.  `def forward(self, x)`: Modelin ileri besleme metodunu tanımlar.
10. `x = self.embedding(x)`: Girdi verisini gömme katmanından geçirir.
11. `x = self.encoder(x)`: Gömülü veriyi kodlayıcı katmanından geçirir.
12. `x = self.decoder(x)`: Kodlanmış veriyi çözücü katmanından geçirir.
13. `x = self.classification_head(x)`: Çözülmüş veriyi sınıflandırma başlığından geçirir.
14. `model = TransformerModel()`: Transformer modelini oluşturur.
15. `optimizer = optim.Adam(model.parameters(), lr=0.001)`: Adam optimizasyon algoritmasını tanımlar.
16. `loss_fn = nn.CrossEntropyLoss()`: Çapraz entropi kayıp fonksiyonunu tanımlar.
17. `for epoch in range(10)`: Eğitim döngüsünü 10 epoch boyunca çalıştırır.

Bu kod yapısı, temel bir Transformer modelinin nasıl oluşturulacağını ve eğitileceğini gösterir.

---

