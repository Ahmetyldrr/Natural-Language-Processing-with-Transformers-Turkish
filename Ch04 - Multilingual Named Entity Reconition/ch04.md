## The Dataset

# Veri Kümesi (Dataset)

Bu bölümde, XTREME (Cross-lingual TRansfer Evaluation of Multilingual Encoders) kıyaslamasının bir alt kümesi olan WikiANN veya PAN-X veri kümesi kullanılacaktır. Bu veri kümesi, birçok dilde Wikipedia makalelerini içerir, İsviçre'de en çok konuşulan dört dil de dahil olmak üzere: Almanca (%62.9), Fransızca (%22.9), İtalyanca (%8.4) ve İngilizce (%5.9). Her makale, "inside-outside-beginning" (IOB2) formatında LOC (konum), PER (kişi) ve ORG (örgüt) etiketleri ile açıklanmıştır.

## IOB2 Formatı

IOB2 formatında, bir varlığın başlangıcını belirtmek için B- öneki kullanılır ve aynı varlığa ait ardışık tokenler I- öneki ile etiketlenir. O etiketi, tokenin herhangi bir varlıkla ilişkili olmadığını gösterir. Örneğin, "Jeff Dean is a computer scientist at Google in California" cümlesi IOB2 formatında aşağıdaki gibi etiketlenir:

| Token | Etiket |
| --- | --- |
| Jeff | B-PER |
| Dean | I-PER |
| is | O |
| a | O |
| computer | O |
| scientist | O |
| at | O |
| Google | B-ORG |
| in | O |
| California | B-LOC |

## PAN-X Veri Kümesini Yükleme

PAN-X veri kümesini XTREME'den yüklemek için hangi veri kümesi konfigürasyonunun kullanılacağını bilmemiz gerekir. Birden fazla alan içeren veri kümeleriyle çalışırken, mevcut alt kümeleri bulmak için `get_dataset_config_names()` fonksiyonunu kullanabiliriz:
```python
from datasets import get_dataset_config_names
xtreme_subsets = get_dataset_config_names("xtreme")
print(f"XTREME has {len(xtreme_subsets)} configurations")
```
Bu kod, XTREME'deki konfigürasyon sayısını yazdırır.

## PAN-X Alt Kümelerini Bulma

PAN-X alt kümelerini bulmak için, `startswith()` metodunu kullanarak "PAN" ile başlayan konfigürasyonları arayabiliriz:
```python
panx_subsets = [s for s in xtreme_subsets if s.startswith("PAN")]
panx_subsets[:3]
```
Bu kod, ilk üç PAN-X alt kümesini yazdırır.

## Almanca Veri Kümesini Yükleme

Almanca veri kümesini yüklemek için, `load_dataset()` fonksiyonunu kullanarak "PAN-X.de" konfigürasyonunu belirtebiliriz:
```python
from datasets import load_dataset
load_dataset("xtreme", name="PAN-X.de")
```
Bu kod, Almanca PAN-X veri kümesini yükler.

## İsviçre Veri Kümesini Oluşturma

İsviçre veri kümesini oluşturmak için, Almanca, Fransızca, İtalyanca ve İngilizce PAN-X veri kümelerini konuşma oranlarına göre örnekleyeceğiz:
```python
from collections import defaultdict
from datasets import DatasetDict

langs = ["de", "fr", "it", "en"]
fracs = [0.629, 0.229, 0.084, 0.059]

panx_ch = defaultdict(DatasetDict)

for lang, frac in zip(langs, fracs):
    ds = load_dataset("xtreme", name=f"PAN-X.{lang}")
    for split in ds:
        panx_ch[lang][split] = ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))
```
Bu kod, İsviçre veri kümesini oluşturur.

## Eğitim Örneklerinin Sayısı

Eğitim örneklerinin sayısını görmek için, `num_rows` özelliğini kullanabiliriz:
```python
import pandas as pd

pd.DataFrame({lang: [panx_ch[lang]["train"].num_rows] for lang in langs}, index=["Eğitim Örneklerinin Sayısı"])
```
Bu kod, her dil için eğitim örneklerinin sayısını yazdırır.

## Etiketleri İnsan Okunabilir Hale Getirme

Etiketleri insan okunabilir hale getirmek için, `ClassLabel.int2str()` metodunu kullanarak yeni bir sütun oluşturabiliriz:
```python
def create_tag_names(batch):
    return {"ner_tags_str": [tags.int2str(idx) for idx in batch["ner_tags"]]}

panx_de = panx_ch["de"].map(create_tag_names)
```
Bu kod, Almanca veri kümesi için etiketleri insan okunabilir hale getirir.

## Etiket Frekanslarını Hesaplama

Etiket frekanslarını hesaplamak için, `Counter` sınıfını kullanabiliriz:
```python
from collections import Counter

split2freqs = defaultdict(Counter)

for split, dataset in panx_de.items():
    for row in dataset["ner_tags_str"]:
        for tag in row:
            if tag.startswith("B"):
                tag_type = tag.split("-")[1]
                split2freqs[split][tag_type] += 1

pd.DataFrame.from_dict(split2freqs, orient="index")
```
Bu kod, her bölme için etiket frekanslarını hesaplar ve yazdırır.

Kod açıklamaları:

* `get_dataset_config_names()`: XTREME'deki konfigürasyon adlarını döndürür.
* `load_dataset()`: Belirtilen konfigürasyona göre veri kümesini yükler.
* `shuffle()`: Veri kümesini karıştırır.
* `select()`: Veri kümesinden belirtilen sayıda örnek seçer.
* `num_rows`: Veri kümesindeki örnek sayısını döndürür.
* `ClassLabel.int2str()`: Etiket indeksini etiket adına çevirir.
* `map()`: Veri kümesine belirtilen fonksiyonu uygular.
* `Counter`: Etiket frekanslarını hesaplar.

---

## Multilingual Transformers

# Çokdilli (Multilingual) Transformerlar

Çokdilli transformerlar, tek dilli (monolingual) karşılıkları ile benzer mimari ve eğitim prosedürlerini içerir, ancak ön eğitim (pretraining) için kullanılan corpus birçok dilde belgeler içerir. Bu yaklaşımın dikkat çekici bir özelliği, diller arasında ayrım yapmak için açık bilgi almamalarına rağmen, ortaya çıkan dilbilimsel temsillerin (linguistic representations) çeşitli downstream görevleri için diller arasında iyi genelleme yapabilmesidir.

## Değerlendirme Yöntemleri (Evaluation Methods)

Çokdilli transformer modelleri genellikle üç farklı şekilde değerlendirilir:
1. İngilizce eğitim verileri üzerinde ince ayar (fine-tune) yapın ve ardından her dilin test seti üzerinde değerlendirin.
2. Tek dilli test verileri üzerinde ince ayar yapın ve değerlendirin.
3. Tüm eğitim verileri üzerinde ince ayar yapın ve her dilin test seti üzerinde değerlendirin.

## XLM-RoBERTa (XLM-R)

XLM-R, mBERT'den daha gelişmiş bir çokdilli transformer modelidir. XLM-R, 100 dil için yalnızca MLM (Masked Language Modeling) ön eğitim hedefi olarak kullanır, ancak önceki modellere kıyasla devasa boyuttaki ön eğitim corpusuyla dikkat çeker: her dil için Wikipedia dökümleri ve web'den 2,5 terabaytlık Common Crawl verisi. Bu corpus, önceki modellerde kullanılanlardan çok daha büyüktür ve Burmese ve Swahili gibi düşük kaynaklı diller için sinyalde önemli bir artış sağlar.

### Kod Örneği

XLM-R modeli aşağıdaki gibi kullanılabilir:
```python
from transformers import XLMRobertaTokenizer, XLMRobertaModel

tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')
model = XLMRobertaModel.from_pretrained('xlm-roberta-base')

input_ids = tokenizer.encode("Merhaba, nasılsınız?", return_tensors='pt')
outputs = model(input_ids)

print(outputs.last_hidden_state[:, 0, :])
```
Kod Açıklaması:
- `XLMRobertaTokenizer` ve `XLMRobertaModel`, XLM-R modelini ve tokenizerını yüklemek için kullanılır.
- `tokenizer.encode`, girdi metnini modele uygun token ID'lerine dönüştürür.
- `model`, girdi ID'lerini alır ve çıktı olarak son hidden state'i verir.
- `outputs.last_hidden_state[:, 0, :]`, ilk tokenin (genellikle [CLS] tokeni) son hidden state vektörünü verir.

## Neden XLM-R?

XLM-R, çokdilli NLU görevleri için mükemmel bir seçimdir. Büyük ön eğitim corpusu ve RoBERTa'nın gelişmiş ön eğitim yaklaşımı sayesinde, XLM-R, diller arasında iyi genelleme yapabilir ve çeşitli downstream görevlerinde yüksek performans sergiler.

### XLM-R'ın Özellikleri

- **Büyük ön eğitim corpusu**: Wikipedia dökümleri ve Common Crawl verileriyle eğitilmiştir.
- **Gelişmiş ön eğitim yaklaşımı**: RoBERTa'nın MLM ön eğitim hedefi ve SentencePiece tokenization kullanılır.
- **Çokdilli destek**: 100'den fazla dili destekler.

XLM-R, çokdilli NLU görevlerinde yüksek performans sergileyen güçlü bir modeldir.

---

## A Closer Look at Tokenization

# Tokenizasyon'a (Tokenization) Daha Yakından Bakış

XLM-R, bir WordPiece tokenizörü yerine, ham metin üzerinde eğitilen SentencePiece adlı bir tokenizör kullanır. SentencePiece'ın WordPiece ile nasıl karşılaştırıldığını görmek için, BERT ve XLM-R tokenizörlerini Transformers kütüphanesini kullanarak yükleyelim:

```python
from transformers import AutoTokenizer

bert_model_name = "bert-base-cased"
xlmr_model_name = "xlm-roberta-base"

bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)
```

Kod Açıklaması:
- `from transformers import AutoTokenizer`: Transformers kütüphanesinden `AutoTokenizer` sınıfını içe aktarır. Bu sınıf, önceden eğitilmiş tokenizörleri otomatik olarak yüklemeye yarar.
- `bert_model_name` ve `xlmr_model_name`: BERT ve XLM-R modellerinin isimlerini tanımlar.
- `bert_tokenizer` ve `xlmr_tokenizer`: `AutoTokenizer.from_pretrained()` methodu kullanılarak BERT ve XLM-R tokenizörleri yüklenir.

## Tokenizasyon İşlemi

Tokenizasyon, metni modele verebileceğimiz integer değerlerine dönüştürme işlemidir. Aslında tokenizasyon, dört adımdan oluşan bir işlem hattıdır:
1. **Normalizasyon (Normalization)**: Ham metni "temizlemek" için uygulanan işlemlerdir. Boşlukların temizlenmesi, aksanlı karakterlerin kaldırılması ve Unicode normalizasyonu gibi işlemler içerir. Örneğin, "Jack Sparrow loves New York!" cümlesi "jack sparrow loves new york!" haline gelir.
2. **Ön Tokenizasyon (Pretokenization)**: Metni daha küçük objelere ayırma işlemidir. Bu adım, metni kelimelere ayırma işlemidir. Örneğin, ["jack", "sparrow", "loves", "new", "york", "!"].
3. **Model Tokenizasyonu**: Önceden tokenize edilmiş kelimeleri alt kelimelere ayırma işlemidir. Bu adımda, Byte-Pair Encoding (BPE) veya Unigram algoritmaları kullanılır. Örneğin, [jack, spa, rrow, loves, new, york, !].
4. **Son İşlem (Postprocessing)**: Token listesine ek işlemler uygulanmasıdır. Örneğin, BERT-style tokenizörler sınıflandırma ve ayırıcı tokenler ekler: [CLS, jack, spa, rrow, loves, new, york, !, SEP].

## SentencePiece Tokenizörü

SentencePiece tokenizörü, Unigram tabanlı bir alt kelime segmentasyonuna dayanır ve her girdi metnini Unicode karakterlerinin bir dizisi olarak kodlar. Bu özellik, çok dilli corpuslar için özellikle yararlıdır. SentencePiece'ın bir diğer özel özelliği, boşluk karakterini Unicode sembolü U+2581 ile temsil etmesidir. Bu, SentencePiece'ın dil-specifik ön tokenizörlere güvenmeden bir diziyi detokenize etmesini sağlar.

```python
text = "Jack Sparrow loves New York!"
bert_tokens = bert_tokenizer(text).tokens()
xlmr_tokens = xlmr_tokenizer(text).tokens()

print("".join(xlmr_tokens).replace(u"\u2581", " "))
# Çıktı: '<s> Jack Sparrow loves New York!</s>'
```

Kod Açıklaması:
- `text`: İşlenecek metni tanımlar.
- `bert_tokens` ve `xlmr_tokens`: BERT ve XLM-R tokenizörleri kullanılarak metnin tokenlerine ayrılmış halini elde eder.
- `"".join(xlmr_tokens).replace(u"\u2581", " ")`: XLM-R tokenizörünün çıktısını detokenize eder ve boşluk karakterlerini geri yükler.

## NER için Tokenizasyon

NER (Named Entity Recognition) görevi için metni uygun forma encode etmek için SentencePiece tokenizörünü kullanabiliriz. Öncelikle, önceden eğitilmiş modeli bir token sınıflandırma başlığı ile yüklemeliyiz. Transformers API'sini kullanarak bunu birkaç adımda yapabiliriz.

---

## Transformers for Named Entity Recognition

# Adlandırılmış Varlık Tanıma (Named Entity Recognition) için Dönüştürücüler (Transformers)

Adlandırılmış varlık tanıma (Named Entity Recognition, NER), metin sınıflandırma (text classification) gibi, özel bir token olan [CLS] kullanılarak tüm metin dizisini temsil eden bir yaklaşım kullanır. Ancak, NER'de her bir girdi tokeninin (input token) temsili, aynı tam bağlı katmana (fully connected layer) beslenir ve tokenin varlık türünü (entity type) çıktı olarak verir. Bu nedenle, NER genellikle bir token sınıflandırma görevi (token classification task) olarak çerçevelenir.

## Token Sınıflandırma Görevi

Token sınıflandırma görevinde, her bir tokenin temsili, aynı tam bağlı katmana beslenir ve tokenin varlık türünü çıktı olarak verir. Bu işlem Şekil 4-3'te gösterilmektedir.

## Alt Kelimelerin (Subwords) İşlenmesi

Token sınıflandırma görevinde alt kelimelerin (subwords) nasıl işleneceği önemli bir sorudur. Örneğin, "Christa" ismi "Chr" ve "##ista" alt kelimelerine tokenize edilebilir. Bu durumda, hangi alt kelimeye B-PER etiketi atanmalıdır? BERT makalesinde, yazarlar bu etiketi ilk alt kelimeye ("Chr" örneğimizde) atamış ve sonraki alt kelimeyi ("##ista") yok saymışlardır. Bu konvansiyonu burada da benimseyeceğiz ve yok sayılan alt kelimeleri IGN ile göstereceğiz.

Kod örneği:
```python
import torch
from transformers import BertTokenizer, BertModel

# BERT tokenleştiricisini ve modelini yükle
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# "Christa" kelimesini tokenize et
inputs = tokenizer("Christa", return_tensors="pt")

# Tokenize edilmiş girdileri modele besle
outputs = model(**inputs)

# İlk alt kelimenin temsilini al
first_subword_representation = outputs.last_hidden_state[:, 0, :]

# Tam bağlı katmana besle
classification_layer = torch.nn.Linear(first_subword_representation.shape[-1], 8)  # 8 sınıf için
logits = classification_layer(first_subword_representation)
```
Kod açıklaması:

*   `BertTokenizer` ve `BertModel` sınıflarını `transformers` kütüphanesinden içe aktardık.
*   BERT tokenleştiricisini ve modelini `bert-base-uncased` önceden eğitilmiş modeli kullanarak yükledik.
*   "Christa" kelimesini tokenize ettik ve girdileri modele besledik.
*   Modelin çıktılarını kullanarak ilk alt kelimenin temsilini elde ettik.
*   Tam bağlı katmana besleyerek sınıflandırma yaptık.

## XLM-R ve BERT

XLM-R'ın mimarisi RoBERTa'ya dayandığı için, BERT'te gördüğümüz mimari yönler XLM-R için de geçerlidir. Dolayısıyla, XLM-R'ı NER görevleri için de kullanabiliriz.

## Sonuç

Dönüştürücüler (Transformers), adlandırılmış varlık tanıma (Named Entity Recognition, NER) görevleri için de kullanılabilir. Token sınıflandırma görevi olarak çerçevelenen NER'de, her bir tokenin temsili aynı tam bağlı katmana beslenir ve tokenin varlık türünü çıktı olarak verir. Alt kelimelerin işlenmesi önemli bir sorudur ve BERT'te olduğu gibi ilk alt kelimeye etiket atanabilir. XLM-R da BERT'e benzer bir mimariye sahip olduğu için NER görevleri için kullanılabilir.

---

## The Anatomy of the Transformers Model Class

# Transformers Model Sınıfının Anatomisi

Transformers kütüphanesi, her bir mimari ve görev için özel sınıflar etrafında düzenlenmiştir. Farklı görevlerle ilişkili model sınıfları, `<ModelName>For<Task>` konvansiyonuna göre adlandırılır veya `AutoModelFor<Task>` kullanıldığında `AutoModel` sınıfları kullanılır.

## Özel Model Oluşturma

Transformers kütüphanesinin temel kavramlarından biri, mimariyi gövde ve kafa olarak ayırmaktır. Gövde, token embedding'leri ve transformer katmanlarını içerir ve görevden bağımsızdır. Kafa ise görev-specifik'tir ve son katmandır.

Özel bir model oluşturmak için, `RobertaPreTrainedModel` sınıfını miras alıyoruz ve `XLMRobertaForTokenClassification` adlı özel bir sınıf oluşturuyoruz.

```python
import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.modeling_outputs import TokenClassifierOutput
from transformers.models.roberta.modeling_roberta import RobertaModel
from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel

class XLMRobertaForTokenClassification(RobertaPreTrainedModel):
    config_class = XLMRobertaConfig

    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        # Model gövdesini yükle
        self.roberta = RobertaModel(config, add_pooling_layer=False)
        # Token sınıflandırma kafasını kur
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
        # Ağırlıkları yükle ve başlat
        self.init_weights()

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):
        # Model gövdesini kullanarak encoder representasyonlarını al
        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)
        # Encoder representasyonlarına sınıflandırıcı uygula
        sequence_output = self.dropout(outputs[0])
        logits = self.classifier(sequence_output)
        # Kaybı hesapla
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        # Model çıktı objesini döndür
        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)
```

### Kod Açıklaması

*   `XLMRobertaForTokenClassification` sınıfı, `RobertaPreTrainedModel` sınıfını miras alır ve `config_class` özelliğini `XLMRobertaConfig` olarak tanımlar.
*   `__init__` methodu, model gövdesini (`RobertaModel`) ve token sınıflandırma kafasını (`dropout` ve `classifier`) başlatır.
*   `forward` methodu, model gövdesini kullanarak encoder representasyonlarını alır, sınıflandırıcı uygular ve kaybı hesaplar.

## Özel Modeli Eğitme

Özel modelimizi eğitmek için, önce etiketleri ve etiket indekslerini tanımlarız.

```python
index2tag = {idx: tag for idx, tag in enumerate(tags.names)}
tag2index = {tag: idx for idx, tag in enumerate(tags.names)}
```

Ardından, `AutoConfig` sınıfını kullanarak model konfigürasyonunu yükleriz ve özel modelimizi oluştururuz.

```python
from transformers import AutoConfig

xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)
xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)
```

### Kod Açıklaması

*   `index2tag` ve `tag2index` sözlükleri, etiketleri ve etiket indekslerini tanımlar.
*   `AutoConfig` sınıfı, model konfigürasyonunu yükler ve özel model konfigürasyonunu tanımlar.
*   `XLMRobertaForTokenClassification` sınıfı, özel modelimizi oluşturur ve `from_pretrained` methodu kullanarak önceden eğitilmiş ağırlıkları yükler.

## Modeli Kullanma

Özel modelimizi kullanarak, girdi metnini etiketleyebiliriz.

```python
def tag_text(text, tags, model, tokenizer):
    # Tokenleri özel karakterlerle al
    tokens = tokenizer(text).tokens()
    # Sırayı ID'lere kodla
    input_ids = xlmr_tokenizer(text, return_tensors="pt").input_ids.to(device)
    # Tahminleri 7 olası sınıf üzerinden dağıt
    outputs = model(input_ids)[0]
    # En olası sınıfı almak için argmax uygula
    predictions = torch.argmax(outputs, dim=2)
    # DataFrame'e dönüştür
    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]
    return pd.DataFrame([tokens, preds], index=["Tokens", "Tags"])
```

### Kod Açıklaması

*   `tag_text` fonksiyonu, girdi metnini etiketler ve tokenleri, etiketleri ve tahminleri içeren bir DataFrame döndürür.
*   `tokenizer` fonksiyonu, girdi metnini tokenlere böler.
*   `model` fonksiyonu, girdi ID'lerini alır ve tahminleri döndürür.

---

## Tokenizing Texts for NER

# NER (Named Entity Recognition) için Metinlerin Tokenleştirilmesi

XLM-R modeli ile ince ayar yapabilmek için tüm veri kümesini tokenleştirmemiz gerekir. Datasets kütüphanesi, `map()` işlemi ile bir Dataset nesnesini hızlı bir şekilde tokenleştirmemizi sağlar.

## Tokenleştirme İşlemi

Öncelikle, minimal imzaya sahip bir fonksiyon tanımlamamız gerekir: `function(examples: Dict[str, List]) -> Dict[str, List]`. Burada `examples`, bir Dataset'in bir dilimini temsil eder.

### Örnek Kod
```python
words, labels = de_example["tokens"], de_example["ner_tags"]
tokenized_input = xlmr_tokenizer(de_example["tokens"], is_split_into_words=True)
tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
pd.DataFrame([tokens], index=["Tokens"])
```

### Kod Açıklaması

1. `de_example["tokens"]` ve `de_example["ner_tags"]` sırasıyla kelimeleri ve etiketleri liste olarak alır.
2. `xlmr_tokenizer` ile kelimeleri tokenleştirir ve `is_split_into_words=True` parametresi ile girdinin zaten kelimelere ayrıldığını belirtir.
3. `convert_ids_to_tokens` ile token ID'lerini tokenlere çevirir.

## Tokenleştirme Sonrası İşlemler

Tokenleştirme sonrası, her bir tokenin hangi kelimeye karşılık geldiğini belirlemek için `word_ids()` fonksiyonu kullanılır.

### Örnek Kod
```python
word_ids = tokenized_input.word_ids()
pd.DataFrame([tokens, word_ids], index=["Tokens", "Word IDs"])
```

### Kod Açıklaması

1. `word_ids()` her bir tokenin kelime indeksini döndürür.
2. Özel tokenler gibi `<s>` ve `</s>` `None` değerini alır.

## Etiketlerin Hizalanması

Etiketleri tokenlerle hizalamak için, `label_ids` listesi oluşturulur.

### Örnek Kod
```python
previous_word_idx = None
label_ids = []
for word_idx in word_ids:
    if word_idx is None or word_idx == previous_word_idx:
        label_ids.append(-100)
    elif word_idx != previous_word_idx:
        label_ids.append(labels[word_idx])
    previous_word_idx = word_idx
labels = [index2tag[l] if l != -100 else "IGN" for l in label_ids]
index = ["Tokens", "Word IDs", "Label IDs", "Labels"]
pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)
```

### Kod Açıklaması

1. `-100` değeri, PyTorch'un `CrossEntropyLoss` sınıfında `ignore_index` özelliği olarak kullanılır ve eğitim sırasında göz ardı edilir.
2. `label_ids` listesi, her bir tokenin etiketini içerir.

## Veri Kümesinin Kodlanması

Tüm veri kümesini kodlamak için, `tokenize_and_align_labels` fonksiyonu tanımlanır.

### Örnek Kod
```python
def tokenize_and_align_labels(examples):
    tokenized_inputs = xlmr_tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for idx, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=idx)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None or word_idx == previous_word_idx:
                label_ids.append(-100)
            else:
                label_ids.append(label[word_idx])
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs
```

### Kod Açıklaması

1. `tokenize_and_align_labels` fonksiyonu, `examples` veri kümesini tokenleştirir ve etiketleri hizalar.

## Veri Kümesinin Kodlanması ve Modelin Hazırlanması

Veri kümesini kodlamak için, `encode_panx_dataset` fonksiyonu tanımlanır.

### Örnek Kod
```python
def encode_panx_dataset(corpus):
    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])
panx_de_encoded = encode_panx_dataset(panx_ch["de"])
```

### Kod Açıklaması

1. `encode_panx_dataset` fonksiyonu, `corpus` veri kümesini kodlar.
2. `panx_de_encoded` değişkeni, kodlanmış Alman veri kümesini içerir.

Artık bir model ve kodlanmış veri kümesi mevcuttur. Bir sonraki adım, bir performans metriği tanımlamaktır.

---

## Performance Measures

# Performans Ölçütleri (Performance Measures)

Adlandırılmış Varlık Tanıma (Named Entity Recognition - NER) modelinin değerlendirilmesi, metin sınıflandırma (text classification) modelinin değerlendirilmesine benzer ve genellikle doğruluk (precision), geri çağırma (recall) ve F1-skoru (F1-score) sonuçları raporlanır. 

## Değerlendirme İncelikleri (Evaluation Subtleties)

Tahminin doğru sayılabilmesi için bir varlığın tüm kelimelerinin doğru tahmin edilmesi gerekir. Neyse ki, seqeval adlı kullanışlı bir kütüphane bu tür görevler için tasarlanmıştır.

## Seqeval Kullanımı (Using Seqeval)

Örneğin, bazı yer tutucu NER etiketleri (placeholder NER tags) ve model tahminleri verildiğinde, seqeval'ın classification_report() fonksiyonu aracılığıyla ölçütleri hesaplayabiliriz:

```python
from seqeval.metrics import classification_report

y_true = [["O", "O", "O", "B-MISC", "I-MISC", "I-MISC", "O"], ["B-PER", "I-PER", "O"]]
y_pred = [["O", "O", "B-MISC", "I-MISC", "I-MISC", "I-MISC", "O"], ["B-PER", "I-PER", "O"]]

print(classification_report(y_true, y_pred))
```

Çıktı:
```
              precision    recall  f1-score   support

        MISC       0.00      0.00      0.00         1
         PER       1.00      1.00      1.00         1

   micro avg       0.50      0.50      0.50         2
   macro avg       0.50      0.50      0.50         2
weighted avg       0.50      0.50      0.50         2
```

## Kod Açıklamaları (Code Explanations)

* `y_true` ve `y_pred`: Gerçek etiketler ve model tahminleri listeleri. Her liste bir örnekteki etiketleri veya tahminleri temsil eder.
* `classification_report()`: Seqeval'ın sağladığı bir fonksiyon, gerçek etiketler ve tahminler arasındaki doğruluk, geri çağırma ve F1-skorunu hesaplar.

## Tahminleri Hizalama (Aligning Predictions)

Eğitim sırasında bu ölçütleri entegre etmek için, model çıktılarını seqeval'ın beklediği listelere dönüştüren bir fonksiyona ihtiyaç vardır. Aşağıdaki fonksiyon, ardışık alt kelimelerle ilişkili etiket ID'lerini yok sayar:

```python
import numpy as np

def align_predictions(predictions, label_ids):
    preds = np.argmax(predictions, axis=2)
    batch_size, seq_len = preds.shape
    labels_list, preds_list = [], []
    
    for batch_idx in range(batch_size):
        example_labels, example_preds = [], []
        for seq_idx in range(seq_len):
            # Ignore label IDs = -100
            if label_ids[batch_idx, seq_idx] != -100:
                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])
                example_preds.append(index2tag[preds[batch_idx][seq_idx]])
        labels_list.append(example_labels)
        preds_list.append(example_preds)
    
    return preds_list, labels_list
```

## Kod Açıklamaları (Code Explanations)

* `np.argmax()`: Numpy kütüphanesinin sağladığı bir fonksiyon, en yüksek olasılıklı sınıfın indeksini döndürür.
* `batch_size` ve `seq_len`: Tahminlerin şeklini temsil eden değişkenler.
* `label_ids[batch_idx, seq_idx] != -100`: -100 etiket ID'sine sahip olanları yok sayar. Bu, ardışık alt kelimelerle ilişkili etiketleri yok saymak için kullanılır.
* `index2tag`: Etiket indekslerini etiket isimlerine çeviren bir sözlük.

Bu fonksiyon, model çıktılarını seqeval'ın beklediği formata dönüştürür ve böylece eğitim sırasında ölçütleri hesaplamak mümkün olur.

---

## Fine-Tuning XLM-RoBERTa

# XLM-RoBERTa Modelini İnce Ayara Alma (Fine-Tuning)

XLM-RoBERTa modelini PAN-X veri kümesinin Almanca alt kümesinde ince ayara almak ve daha sonra sıfır-shot (zero-shot) çapraz-dil performansını Fransızca, İtalyanca ve İngilizce dillerinde değerlendirmek için bir strateji uygulanmaktadır.

## Eğitim Özelliklerini Tanımlama (Defining Training Attributes)

Eğitim özellikleri `TrainingArguments` sınıfı kullanılarak tanımlanır:
```python
from transformers import TrainingArguments

num_epochs = 3
batch_size = 24
logging_steps = len(panx_de_encoded["train"]) // batch_size
model_name = f"{xlmr_model_name}-finetuned-panx-de"

training_args = TrainingArguments(
    output_dir=model_name,
    log_level="error",
    num_train_epochs=num_epochs,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    evaluation_strategy="epoch",
    save_steps=1e6,
    weight_decay=0.01,
    disable_tqdm=False,
    logging_steps=logging_steps,
    push_to_hub=True
)
```
Kod açıklamaları:

* `num_epochs`: Eğitim dönemi sayısı (3)
* `batch_size`: Toplu işlem boyutu (24)
* `logging_steps`: Günlük kaydı adımları (toplu işlem boyutuna göre hesaplanır)
* `model_name`: Model adı (xlmr_model_name'den türetilir)
* `output_dir`: Çıktı dizini (model adı)
* `log_level`: Günlük kaydı seviyesi (hata)
* `evaluation_strategy`: Değerlendirme stratejisi (her dönem sonunda)
* `save_steps`: Kaydetme adımları (1e6, yani devre dışı)
* `weight_decay`: Ağırlık azaltma (0.01)
* `disable_tqdm`: tqdm devre dışı bırakma (False)
* `push_to_hub`: Modeli Hugging Face Hub'a gönderme (True)

## Hugging Face Hub'a Giriş Yapma

Hugging Face Hub'a giriş yapmak için:
```python
from huggingface_hub import notebook_login
notebook_login()
```
## Değerlendirme Metriği Tanımlama (Defining Evaluation Metric)

Değerlendirme metriği `compute_metrics` fonksiyonu ile tanımlanır:
```python
from seqeval.metrics import f1_score

def compute_metrics(eval_pred):
    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)
    return {"f1": f1_score(y_true, y_pred)}
```
Kod açıklamaları:

* `align_predictions`: Tahminleri ve gerçek etiketleri hizalar
* `f1_score`: F1 skoru hesaplar

## Veri Collator Tanımlama (Defining Data Collator)

Veri collator `DataCollatorForTokenClassification` sınıfı ile tanımlanır:
```python
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)
```
Kod açıklamaları:

* `DataCollatorForTokenClassification`: Token sınıflandırma görevi için veri collator

## Model Başlatma (Initializing Model)

Model `model_init` fonksiyonu ile başlatılır:
```python
def model_init():
    return XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)
```
Kod açıklamaları:

* `XLMRobertaForTokenClassification`: XLM-RoBERTa token sınıflandırma modeli
* `from_pretrained`: Önceden eğitilmiş modelden başlatma

## Eğitim ve Değerlendirme

Eğitim ve değerlendirme `Trainer` sınıfı ile yapılır:
```python
from transformers import Trainer

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    train_dataset=panx_de_encoded["train"],
    eval_dataset=panx_de_encoded["validation"],
    tokenizer=xlmr_tokenizer
)

trainer.train()
trainer.push_to_hub(commit_message="Training completed!")
```
Kod açıklamaları:

* `Trainer`: Eğitim ve değerlendirme sınıfı
* `model_init`: Model başlatma fonksiyonu
* `args`: Eğitim özellikleri
* `data_collator`: Veri collator
* `compute_metrics`: Değerlendirme metriği fonksiyonu
* `train_dataset`: Eğitim veri kümesi
* `eval_dataset`: Değerlendirme veri kümesi
* `tokenizer`: Tokenizer

Eğitim tamamlandıktan sonra model Hugging Face Hub'a gönderilir.

---

## Error Analysis

# Hata Analizi (Error Analysis)

XLM-R'nin çok dilli yönlerini daha derinlemesine incelemeden önce, modelimizin hatalarını incelemeye bir dakika ayıralım. Bir modelin hata analizi, transformer'ları (ve genel olarak makine öğrenimi modellerini) eğitirken ve hata ayıklarken en önemli yönlerden biridir. Modelin iyi performans gösterdiği halde ciddi kusurları olabileceği birkaç başarısızlık modu vardır.

## Hata Analizi Neden Önemlidir? (Why is Error Analysis Important?)

- Modelin gerçekten ne kadar iyi performans gösterdiğini anlamak için.
- Modelin zayıf yönlerini ve iyileştirme alanlarını belirlemek için.
- Üretim ortamında bir model dağıtıldığında her zaman dikkate alınması gereken yönlerdir.

## Hata Analizi Nasıl Yapılır? (How to Perform Error Analysis?)

1. **Doğrulama Kümesindeki Yüksek Kayıplı Örnekleri İncelemek (Examining High Loss Examples in the Validation Set):** 
   - Modelin performansını değerlendirmek için doğrulama kümesindeki örneklerin kayıplarını hesaplayabiliriz.
   - Bunun için `forward_pass_with_label` adlı bir fonksiyon tanımladık.

```python
from torch.nn.functional import cross_entropy

def forward_pass_with_label(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    batch = data_collator(features)
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)
    with torch.no_grad():
        output = trainer.model(input_ids, attention_mask)
    predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()
    loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction="none")
    loss = loss.view(len(input_ids), -1).cpu().numpy()
    return {"loss": loss, "predicted_label": predicted_label}
```

**Kod Açıklaması (Code Explanation):**

- `features = [dict(zip(batch, t)) for t in zip(*batch.values())]`: Bu satır, batch içindeki verileri yeniden düzenler.
- `batch = data_collator(features)`: Verileri collator ile işler.
- `input_ids`, `attention_mask`, `labels` tensörlerini cihaza taşır.
- `output = trainer.model(input_ids, attention_mask)`: Model üzerinden veri geçişini sağlar.
- `predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()`: Tahmin edilen etiketleri hesaplar.
- `loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction="none")`: Her token için kayıp hesaplar.

2. **Doğrulama Kümesini İşlemek ve DataFrame'e Yüklemek (Processing the Validation Set and Loading into DataFrame):**

```python
valid_set = panx_de_encoded["validation"]
valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)
df = valid_set.to_pandas()
```

3. **Token ve Etiketleri Dizgi (String) Haline Getirmek (Converting Tokens and Labels to Strings):**

```python
index2tag[-100] = "IGN"
df["input_tokens"] = df["input_ids"].apply(lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))
df["predicted_label"] = df["predicted_label"].apply(lambda x: [index2tag[i] for i in x])
df["labels"] = df["labels"].apply(lambda x: [index2tag[i] for i in x])
```

**Kod Açıklaması:**

- `index2tag[-100] = "IGN"`: -100 etiketini "IGN" olarak tanımlar, bu genellikle padding tokenları için kullanılır.
- `df["input_tokens"]`, `df["predicted_label"]`, `df["labels"]` sütunlarını işler.

4. **Token Seviyesinde Hata Analizi (Error Analysis at Token Level):**

```python
df_tokens = df.apply(pd.Series.explode)
df_tokens = df_tokens.query("labels != 'IGN'")
df_tokens["loss"] = df_tokens["loss"].astype(float).round(2)
```

**Kod Açıklaması:**

- `df.apply(pd.Series.explode)`: Liste içindeki elemanları patlatır.
- `df_tokens.query("labels != 'IGN'")`: "IGN" etiketli satırları filtreler.

5. **Tokenlere Göre Kayıpları Gruplamak ve Analiz Etmek (Grouping Losses by Tokens and Analyzing):**

```python
df_tokens.groupby("input_tokens")[["loss"]].agg(["count", "mean", "sum"]).sort_values(by="sum", ascending=False)
```

**Kod Açıklaması:**

- Tokenlere göre kayıpları gruplar ve her bir token için count, mean, sum hesaplar.

## Karışıklık Matrisi ile Modelin Performansını Değerlendirmek (Evaluating Model Performance with Confusion Matrix)

```python
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

def plot_confusion_matrix(y_preds, y_true, labels):
    cm = confusion_matrix(y_true, y_preds, normalize="true")
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap="Blues", values_format=".2f")
    plt.title("Normalized confusion matrix")
    plt.show()

plot_confusion_matrix(df_tokens["predicted_label"], df_tokens["labels"], tags.names)
```

**Kod Açıklaması:**

- Gerçek etiketler (`y_true`) ve tahmin edilen etiketler (`y_preds`) arasındaki karışıklık matrisini hesaplar ve çizer.

## Yüksek Kayıplı Dizileri İncelemek (Examining Sequences with High Losses)

```python
df["total_loss"] = df["loss"].apply(sum)
df_tmp = df.sort_values(by="total_loss", ascending=False).head(3)
for sample in get_samples(df_tmp):
    display(sample)
```

**Kod Açıklaması:**

- Her örnek için toplam kayıp hesaplar ve en yüksek kayba sahip örnekleri gösterir.

Bu analizler sonucunda modelin ve veri kümesinin zayıf yönlerini belirledik. Gerçek kullanım senaryolarında, veri kümesini temizleyerek, modeli yeniden eğiterek ve yeni hataları analiz ederek bu süreci yinelemeli olarak gerçekleştirmek gerekir.

---

## Cross-Lingual Transfer

# Cross-Lingual Transfer (Çapraz Dil Aktarımı)

XLM-R modelini Alman dilinde (Deutsch) ince ayarladıktan (fine-tune) sonra, diğer dillere aktarımını değerlendirmek için `Trainer` sınıfının `predict()` metodu kullanılır. Birden fazla dili değerlendirmek için basit bir fonksiyon oluşturulur:

```python
def get_f1_score(trainer, dataset):
    return trainer.predict(dataset).metrics["test_f1"]
```

Bu fonksiyon, test seti üzerindeki performansı inceler ve skorları bir sözlükte (`dict`) takip eder:

```python
f1_scores = defaultdict(dict)
f1_scores["de"]["de"] = get_f1_score(trainer, panx_de_encoded["test"])
print(f"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}")
```

F1-score, Alman dilinde eğitilen modelin Alman test setindeki performansıdır: 0.868.

## Diğer Dillerde Performans

Modelin diğer dillerdeki performansı değerlendirilir. Örneğin, Fransızca (French) için:

```python
text_fr = "Jeff Dean est informaticien chez Google en Californie"
tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)
```

Model, Fransızca metni doğru bir şekilde etiketler.

## Sıfır-Gözlem (Zero-Shot) Aktarımı

Modelin Alman dilinde eğitildikten sonra diğer dillere sıfır-gözlem aktarımı yapılır. Örneğin, Fransızca test seti için:

```python
f1_scores["de"]["fr"] = evaluate_lang_performance("fr", trainer)
print(f"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}")
```

F1-score, Alman modelinin Fransızca test setindeki performansıdır: 0.714.

## Dil Aileleri ve Performans

Modelin performansı, dil aileleri arasındaki mesafeye bağlıdır. Alman ve Fransızca, Hint-Avrupa dil ailesine ait olmalarına rağmen, farklı alt ailelere (Germanik ve Roman) aittir.

## İtalyanca ve İngilizce Performansı

Modelin İtalyanca ve İngilizce performansı da değerlendirilir:

```python
f1_scores["de"]["it"] = evaluate_lang_performance("it", trainer)
f1_scores["de"]["en"] = evaluate_lang_performance("en", trainer)
```

F1-score, Alman modelinin İtalyanca test setindeki performansıdır: 0.692, İngilizce test setindeki performansıdır: 0.589.

## Hedef Dilde İnce Ayar (Fine-Tuning)

Modelin hedef dilde ince ayar yapıldığında performansı artar. Fransızca için:

```python
def train_on_subset(dataset, num_samples):
    # ...
```

Bu fonksiyon, Fransızca veri setini kullanarak modelin performansını değerlendirir.

## Çok-Dilli Öğrenme (Multilingual Learning)

Modelin birden fazla dilde eğitilmesi, performansı artırır. Alman ve Fransızca veri setleri birleştirilir:

```python
panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])
```

Model, birleştirilmiş veri seti üzerinde eğitilir ve performansı değerlendirilir.

## Sonuçlar

Çok-dilli öğrenme, performansı önemli ölçüde artırır, özellikle düşük kaynaklı diller için. Alman, Fransızca ve İtalyanca benzer performans gösterir, İngilizce ise daha düşük performans gösterir.

### Kod Açıklamaları

*   `get_f1_score` fonksiyonu, `Trainer` sınıfının `predict()` metodunu kullanarak F1-score'u hesaplar.
*   `evaluate_lang_performance` fonksiyonu, bir dildeki test seti üzerindeki performansı değerlendirir.
*   `train_on_subset` fonksiyonu, bir veri setini kullanarak modelin performansını değerlendirir.
*   `concatenate_splits` fonksiyonu, birden fazla veri setini birleştirir.

### Kullanılan Kodlar

```python
def get_f1_score(trainer, dataset):
    return trainer.predict(dataset).metrics["test_f1"]

def evaluate_lang_performance(lang, trainer):
    panx_ds = encode_panx_dataset(panx_ch[lang])
    return get_f1_score(trainer, panx_ds["test"])

def train_on_subset(dataset, num_samples):
    train_ds = dataset["train"].shuffle(seed=42).select(range(num_samples))
    valid_ds = dataset["validation"]
    test_ds = dataset["test"]
    # ...

def concatenate_splits(corpora):
    multi_corpus = DatasetDict()
    for split in corpora[0].keys():
        multi_corpus[split] = concatenate_datasets([corpus[split] for corpus in corpora]).shuffle(seed=42)
    return multi_corpus
```

### Neden Kullanıldıkları

*   `get_f1_score`: F1-score'u hesaplamak için.
*   `evaluate_lang_performance`: Bir dildeki test seti üzerindeki performansı değerlendirmek için.
*   `train_on_subset`: Bir veri setini kullanarak modelin performansını değerlendirmek için.
*   `concatenate_splits`: Birden fazla veri setini birleştirmek için.

---

## Interacting with Model Widgets

# Model Widget'ları ile Etkileşim (Interacting with Model Widgets)

Bu bölümde, ince ayar yapılmış (fine-tuned) birçok modeli Hub'a gönderdik. Bu modellerle yerel makinemizde `pipeline()` fonksiyonunu kullanarak etkileşime girebilsek de, Hub, bu tür iş akışları için harika olan widget'lar sağlar. Örneğin, transformersbook/xlm-roberta-base-finetuned-panx-all kontrol noktasımız (checkpoint) için Şekil 4-5'te gösterilen bir örnek vardır. Görüldüğü gibi, bu model bir Almanca metindeki tüm varlıkları (entities) tanımlamada iyi bir iş çıkarmıştır.

## Model Widget'ları Kullanma (Using Model Widgets)

Model widget'ları, Hub'da barındırılan (hosted) modellerle etkileşime girmenin kolay bir yolunu sağlar. Bu widget'lar, modelin girdilere (inputs) nasıl yanıt verdiğini görselleştirmek (visualize) için kullanılabilir.

Örneğin, `pipeline()` fonksiyonunu kullanarak bir model yüklemek (load) ve bir metni işlemek (process) için aşağıdaki kodu kullanabilirsiniz:
```python
from transformers import pipeline

# Modeli yükle (Load the model)
model_name = "transformersbook/xlm-roberta-base-finetuned-panx-all"
ner_pipeline = pipeline("ner", model=model_name)

# Metni işle (Process the text)
text = "Der Präsident der USA ist Joe Biden."
output = ner_pipeline(text)

# Çıktıyı yazdır (Print the output)
print(output)
```
Kodun açıklaması:

* `from transformers import pipeline`: Transformers kütüphanesinden `pipeline()` fonksiyonunu içe aktarır (import).
* `model_name = "transformersbook/xlm-roberta-base-finetuned-panx-all"`: Yüklemek istediğimiz modelin adını belirler.
* `ner_pipeline = pipeline("ner", model=model_name)`: `pipeline()` fonksiyonunu kullanarak bir NER (Named Entity Recognition) modeli yükler.
* `text = "Der Präsident der USA ist Joe Biden."`: İşlemek istediğimiz metni tanımlar.
* `output = ner_pipeline(text)`: Metni NER modeli ile işler ve çıktıyı `output` değişkenine atar.
* `print(output)`: Çıktıyı yazdırır.

Bu kod, Hub'da barındırılan bir model ile etkileşime girmenin bir yolunu gösterir. Model widget'ları, benzer bir işlevi daha görselleştirilmiş (visualized) bir şekilde sağlar.

---

## Conclusion

# Doğal Dil İşleme (NLP) Görevlerinde Çok Dilli (Multilingual) Korpus Kullanma

Bu bölümde, 100 dilde önceden eğitilmiş (pretrained) tek bir transformer modeli olan XLM-R kullanarak çok dilli bir korpus üzerinde NLP görevini nasıl ele alacağımızı gördük. Alman dilinden Fransızca'ya çapraz dil transferinin (cross-lingual transfer) iyi bir performans sergilediğini, ancak hedef dil (target language) temel modelin ince ayar (fine-tuning) yapıldığı dilden önemli ölçüde farklı olduğunda veya ön eğitim sırasında kullanılan 100 dilden biri olmadığında bu iyi performansın genellikle gerçekleşmediğini gösterdik.

# Önemli Noktalar
- XLM-R modeli 100 dilde önceden eğitilmiştir (pretrained on 100 languages).
- Çapraz dil transferi (cross-lingual transfer) küçük miktarda etiketli örnek (labeled examples) olduğunda rekabetçi bir performans sergiler.
- Hedef dil temel modelin ince ayar yapıldığı dilden veya ön eğitimde kullanılan dillerden farklıysa performans düşer.
- MAD-X gibi yeni öneriler (recent proposals) düşük kaynaklı senaryolar (low-resource scenarios) için tasarlanmıştır.
- MAD-X, Transformers üzerine inşa edilmiştir (built on top of Transformers).

# Kod Kullanımı ve Açıklamalar
Bu bölümde spesifik bir kod örneği verilmemiştir, ancak MAD-X ile çalışmak için bu bölümdeki kodun kolayca uyarlanabileceği belirtilmiştir. 
Örneğin, bir transformer modelini fine-tuning yapmak için aşağıdaki gibi bir kod kullanılabilir:
```python
from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer
import torch

# Model ve tokenizer yükleme
model_name = "xlm-r-base"
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
model = XLMRobertaForSequenceClassification.from_pretrained(model_name)

# Örnek girdi
input_ids = tokenizer("Bu bir örnek cümledir.", return_tensors="pt").input_ids

# Model çıktısı
outputs = model(input_ids)

# Çıktıyı işleme
logits = outputs.logits
print(logits)
```
Kod Açıklaması:
1. `from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer`: Gerekli sınıfları import eder. `XLMRobertaForSequenceClassification` dizilim sınıflandırma (sequence classification) için kullanılan bir model sınıfıdır. `XLMRobertaTokenizer` ise XLM-R modelinin tokenizer'ını temsil eder.
2. `model_name = "xlm-r-base"`: Kullanılacak XLM-R modelinin adını belirler.
3. `tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)`: Belirtilen model adına göre tokenizer'ı yükler.
4. `model = XLMRobertaForSequenceClassification.from_pretrained(model_name)`: Dizilim sınıflandırma için XLM-R modelini yükler.
5. `input_ids = tokenizer("Bu bir örnek cümledir.", return_tensors="pt").input_ids`: Örnek bir cümleyi tokenize eder ve input_ids'i elde eder. `return_tensors="pt"` parametresi çıktının PyTorch tensörü olarak döndürülmesini sağlar.
6. `outputs = model(input_ids)`: Yüklenen modele input_ids'i verir ve çıktıyı alır.
7. `logits = outputs.logits`: Modelin çıktısından logits'i elde eder.

Bu kod, XLM-R modelini kullanarak bir dizilim sınıflandırma görevi için nasıl ince ayar yapılabileceğini gösterir. MAD-X gibi daha spesifik yaklaşımlar için kodun uyarlanması gerekebilir.

---

