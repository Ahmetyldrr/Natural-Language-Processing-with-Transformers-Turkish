## The Dataset

# Duygu Analizi Veri Seti (Emotion Dataset)

Bu bölümde, İngilizce Twitter mesajlarında duyguların nasıl temsil edildiğini inceleyen bir makaleden alınan veri setini kullanarak bir duygu dedektörü (emotion detector) oluşturacağız. Çoğu duygu analizi veri setinin aksine, bu veri seti altı temel duygu içerir: kızgınlık (anger), iğrenme (disgust), korku (fear), sevinç (joy), üzüntü (sadness) ve sürpriz (surprise).

## Veri Setini İndirme ve İnceleme

Veri setini Hugging Face Hub'dan indireceğiz. `list_datasets()` fonksiyonunu kullanarak Hub'daki mevcut veri setlerini listeleyebiliriz:
```python
from datasets import list_datasets
all_datasets = list_datasets()
print(f"There are {len(all_datasets)} datasets currently available on the Hub")
print(f"The first 10 are: {all_datasets[:10]}")
```
Çıktı:
```
There are 1753 datasets currently available on the Hub
The first 10 are: ['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']
```
Her veri setinin bir adı vardır, bu nedenle `load_dataset()` fonksiyonunu kullanarak duygu veri setini indirebiliriz:
```python
from datasets import load_dataset
emotions = load_dataset("emotion")
```
`emotions` nesnesini incelediğimizde, bir Python sözlüğüne (dictionary) benzer olduğunu görürüz:
```python
emotions
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 16000
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 2000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 2000
    })
})
```
Her bir anahtar (key), farklı bir bölüme (split) karşılık gelir. `train` bölümünü erişmek için:
```python
train_ds = emotions["train"]
train_ds
Dataset({
    features: ['text', 'label'],
    num_rows: 16000
})
```
`Dataset` nesnesi, Datasets kütüphanesinin temel veri yapılarından biridir.

## Veri Setini İnceleme

`train_ds` nesnesini incelediğimizde, her bir satırın bir sözlük olarak temsil edildiğini görürüz:
```python
train_ds[0]
{'label': 0, 'text': 'i didnt feel humiliated'}
```
Sütun adlarına erişmek için:
```python
train_ds.column_names
['text', 'label']
```
Veri tiplerini incelemek için:
```python
print(train_ds.features)
{'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}
```
`text` sütunu `string` tipinde, `label` sütunu ise `ClassLabel` nesnesi olarak temsil edilir.

## Veri Setini Pandas DataFrame'e Dönüştürme

Datasets kütüphanesi, `set_format()` metodunu kullanarak `Dataset` nesnesini Pandas DataFrame'e dönüştürmemize olanak tanır:
```python
import pandas as pd
emotions.set_format(type="pandas")
df = emotions["train"][:]
df.head()
```
Çıktı:
```
                  text  label
0      i didnt feel humiliated      0
1  i can go from feeling so hopeless...      0
2       im grabbing a minute to post...      3
3  i am ever feeling nostalgic about...      2
4         i am feeling grouchy      3
```
`label` sütununu `int2str()` metodunu kullanarak label isimlerine dönüştürebiliriz:
```python
def label_int2str(row):
    return emotions["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
df.head()
```
Çıktı:
```
                  text  label label_name
0      i didnt feel humiliated      0     sadness
1  i can go from feeling so hopeless...      0     sadness
2       im grabbing a minute to post...      3       anger
3  i am ever feeling nostalgic about...      2        love
4         i am feeling grouchy      3       anger
```
## Sınıf Dağılımını İnceleme

Sınıf dağılımını incelemek için:
```python
import matplotlib.pyplot as plt
df["label_name"].value_counts(ascending=True).plot.barh()
plt.title("Frequency of Classes")
plt.show()
```
Çıktı:

Grafiğe göre, veri seti dengesizdir; `joy` ve `sadness` sınıfları sıkça görülürken, `love` ve `surprise` sınıfları daha az görülmektedir.

## Metin Uzunluğu İnceleme

Metin uzunluğunu incelemek için:
```python
df["Words Per Tweet"] = df["text"].str.split().apply(len)
df.boxplot("Words Per Tweet", by="label_name", grid=False, showfliers=False, color="black")
plt.suptitle("")
plt.xlabel("")
plt.show()
```
Çıktı:

Grafiğe göre, her bir duygu için tweet uzunluğu benzerdir ve maksimum bağlam boyutu olan 512'den küçüktür.

## Sonuç

Veri setini inceledik ve dengesiz olduğunu gördük. Ayrıca, metin uzunluğunun maksimum bağlam boyutundan küçük olduğunu gördük. Şimdi, bu veri setini kullanarak bir duygu analizi modeli oluşturmaya hazırız.

Kodların açıklamaları:

* `list_datasets()`: Hugging Face Hub'daki mevcut veri setlerini listeler.
* `load_dataset()`: Belirtilen veri setini indirir.
* `set_format()`: `Dataset` nesnesini Pandas DataFrame'e dönüştürür.
* `int2str()`: Label isimlerini integer değerlerden string değerlere dönüştürür.
* `value_counts()`: Sınıf dağılımını hesaplar.
* `boxplot()`: Metin uzunluğunu görselleştirir.

Kod satırlarının açıklamaları:

1. `from datasets import list_datasets`: Datasets kütüphanesinden `list_datasets` fonksiyonunu içe aktarır.
2. `all_datasets = list_datasets()`: Hugging Face Hub'daki mevcut veri setlerini listeler.
3. `emotions = load_dataset("emotion")`: Duygu veri setini indirir.
4. `train_ds = emotions["train"]`: `train` bölümünü erişir.
5. `df = emotions["train"][:]`: `train` bölümünü Pandas DataFrame'e dönüştürür.
6. `df["label_name"] = df["label"].apply(label_int2str)`: Label isimlerini integer değerlerden string değerlere dönüştürür.
7. `df["Words Per Tweet"] = df["text"].str.split().apply(len)`: Metin uzunluğunu hesaplar.

---

## From Text to Tokens

# Metinden Tokenlere (From Text to Tokens)

## Tokenization Nedir? (What is Tokenization?)

Tokenization, bir metni modelde kullanılan atomik birimlere ayırma işlemidir. Bu işlem, metni daha küçük parçalara bölerek modelin işleyebileceği hale getirmeyi sağlar.

## Karakter Tokenization (Character Tokenization)

En basit tokenization şeması, her karakteri ayrı ayrı modele beslemektir. Python'da `str` nesneleri aslında dizilerdir, bu nedenle karakter düzeyinde tokenization'ı tek satırda gerçekleştirebiliriz:

```python
text = "Tokenizing text is a core task of NLP."
tokenized_text = list(text)
print(tokenized_text)
```

Çıktı:
```python
['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']
```

Bu kod, metni karakterlere ayırır.

## Sayısallaştırma (Numericalization)

Model, her karakteri bir tamsayıya dönüştürmeyi bekler. Bunu yapmak için, her benzersiz token (karakter) için bir tamsayı atayabiliriz:

```python
token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}
print(token2idx)
```

Çıktı:
```python
{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}
```

Bu kod, her karakteri bir tamsayıya eşler.

## One-Hot Encoding (One-Hot Kodlama)

One-hot encoding, kategorik verileri kodlamak için kullanılır. Her kategoriye karşılık bir vektör oluşturulur ve ilgili kategori için 1, diğerleri için 0 atanır.

```python
import torch
import torch.nn.functional as F

input_ids = torch.tensor([token2idx[token] for token in tokenized_text])
one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))
print(one_hot_encodings.shape)
```

Çıktı:
```python
torch.Size([38, 20])
```

Bu kod, her token için one-hot encoding oluşturur.

## Kelime Tokenization (Word Tokenization)

Kelime tokenization, metni kelimelere ayırır. Bu, modelin kelimeleri öğrenmesini kolaylaştırır.

```python
tokenized_text = text.split()
print(tokenized_text)
```

Çıktı:
```python
['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']
```

Bu kod, metni kelimelere ayırır.

## Alt Kelime Tokenization (Subword Tokenization)

Alt kelime tokenization, kelimeleri alt birimlere ayırır. Bu, modelin nadir kelimeleri öğrenmesini sağlar.

## DistilBERT Tokenizer

DistilBERT tokenizer, metni tokenize etmek için kullanılır.

```python
from transformers import AutoTokenizer

model_ckpt = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

encoded_text = tokenizer(text)
print(encoded_text)
```

Çıktı:
```python
{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Bu kod, metni tokenize eder ve input_ids ve attention_mask oluşturur.

## Tokenize İşlemini Uygulama (Applying Tokenization)

Tokenize işlemini tüm veri kümesine uygulamak için `map()` fonksiyonu kullanılır.

```python
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)
```

Bu kod, tokenize işlemini tüm veri kümesine uygular.

### Kodların Açıklaması

*   `list(text)`: Metni karakterlere ayırır.
*   `{ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}`: Her karakteri bir tamsayıya eşler.
*   `F.one_hot(input_ids, num_classes=len(token2idx))`: One-hot encoding oluşturur.
*   `text.split()`: Metni kelimelere ayırır.
*   `AutoTokenizer.from_pretrained(model_ckpt)`: DistilBERT tokenizer'ı yükler.
*   `tokenizer(text)`: Metni tokenize eder.
*   `emotions.map(tokenize, batched=True, batch_size=None)`: Tokenize işlemini tüm veri kümesine uygular.

---

## Training a Text Classifier

# Metin Sınıflandırma için Transformer Modelinin Eğitimi

Bu bölümde, DistilBERT gibi önceden eğitilmiş transformer tabanlı modellerin metin sınıflandırma görevleri için nasıl kullanılacağı ve eğitileceği anlatılmaktadır.

## Önceden Eğitilmiş Modellerin Kullanılması

Önceden eğitilmiş transformer modelleri, metin sınıflandırma görevlerinde kullanılabilir. Ancak, bu modellerin sınıflandırma katmanları ile birlikte kullanılması gerekir.

### Token Encodings ve Embeddings

*   Metin verileri tokenize edilir ve one-hot vektörler olarak temsil edilir (Token Encodings).
*   Token encodings daha sonra daha düşük boyutlu bir uzayda temsil edilen token embeddings'e dönüştürülür.

### Sınıflandırma Katmanının Eklenmesi

*   Önceden eğitilmiş modelin çıktıları, bir sınıflandırma katmanına girdi olarak verilir.
*   PyTorch'ta `nn.Embedding` sınıfı kullanılarak one-hot vektörlerin oluşturulması atlanabilir.

## Özellik Tabanlı Yaklaşım

*   Önceden eğitilmiş modelin ağırlıkları dondurulur ve gizli durumlar (hidden states) özellik olarak kullanılır.
*   Bu özellikler daha sonra bir sınıflandırıcıda kullanılır.

### Kod Örneği

```python
from transformers import AutoModel

model_ckpt = "distilbert-base-uncased"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModel.from_pretrained(model_ckpt).to(device)
```

*   `AutoModel.from_pretrained()` methodu kullanılarak önceden eğitilmiş model yüklenir.
*   `to(device)` methodu kullanılarak model, GPU'ya (varsa) taşınır.

### Gizli Durumların Çıkarılması

```python
def extract_hidden_states(batch):
    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}
    with torch.no_grad():
        last_hidden_state = model(**inputs).last_hidden_state
    return {"hidden_state": last_hidden_state[:, 0].cpu().numpy()}
```

*   `extract_hidden_states()` fonksiyonu, bir batch'in gizli durumlarını çıkarır.
*   `torch.no_grad()` context manager'ı kullanılarak gradient hesaplamaları devre dışı bırakılır.

## İnce Ayar (Fine-Tuning) Yaklaşımı

*   Önceden eğitilmiş modelin ağırlıkları güncellenerek sınıflandırma görevi için optimize edilir.
*   Bu yaklaşım, daha iyi performans sağlar ancak daha fazla hesaplama kaynağı gerektirir.

### Kod Örneği

```python
from transformers import AutoModelForSequenceClassification

num_labels = 6
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)
```

*   `AutoModelForSequenceClassification.from_pretrained()` methodu kullanılarak sınıflandırma katmanı olan bir model yüklenir.

### Eğitim ve Değerlendirme

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=model_name,
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    disable_tqdm=False,
    logging_steps=logging_steps,
    push_to_hub=True,
    log_level="error"
)

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=emotions_encoded["train"],
    eval_dataset=emotions_encoded["validation"],
    tokenizer=tokenizer
)

trainer.train()
```

*   `Trainer` sınıfı kullanılarak model eğitilir ve değerlendirilir.
*   `compute_metrics()` fonksiyonu kullanılarak modelin performansı değerlendirilir.

### Sonuçların Değerlendirilmesi

*   Modelin performansı, doğruluk ve F1 skoru gibi metrikler kullanılarak değerlendirilir.
*   Karışıklık matrisi (confusion matrix) kullanılarak modelin hata yapısı analiz edilir.

## Modelin Kaydedilmesi ve Paylaşılması

*   Eğitilen model, Hugging Face Hub'a kaydedilir ve paylaşılır.
*   `trainer.push_to_hub()` methodu kullanılarak model Hub'a yüklenir.

### Kod Örneği

```python
from transformers import pipeline

model_id = "transformersbook/distilbert-base-uncased-finetuned-emotion"
classifier = pipeline("text-classification", model=model_id)

custom_tweet = "I saw a movie today and it was really good."
preds = classifier(custom_tweet, return_all_scores=True)
```

*   `pipeline()` fonksiyonu kullanılarak model, metin sınıflandırma görevi için kullanılır.
*   `return_all_scores=True` parametresi kullanılarak tüm sınıflar için olasılık değerleri döndürülür.

---

## Conclusion

# Sonuç (Conclusion)

Tebrikler, artık tweet'lerdeki duyguları sınıflandırmak için bir transformer modeli (Transformer Model) eğitmenin nasıl yapılacağını biliyorsunuz! Özelliklere (Features) ve ince ayarlara (Fine-Tuning) dayanan iki tamamlayıcı yaklaşımı gördük ve bunların güçlü ve zayıf yönlerini inceledik. Ancak, bu, transformer modelleriyle gerçek dünya uygulamaları oluşturmada sadece ilk adımdır ve daha çok yol kat etmemiz gerekir.

## Karşılaşılabilecek Zorluklar (Challenges)

NLP (Doğal Dil İşleme) yolculuğunuzda karşılaşmanız muhtemel zorlukların bir listesi:
- Çoğu uygulamada, modeliniz sadece bir yerde toz toplamamakta, tahminler sunmaktadır! (Serving Predictions)
- Bir model Hub'a gönderildiğinde (Pushed to the Hub), otomatik olarak HTTP istekleriyle çağrılabilen bir çıkarım uç noktası (Inference Endpoint) oluşturulur.
- Çıkarım API'sinin (Inference API) belgelerine göz atmanızı öneririz.

## Modeli Hızlandırma (Speeding Up the Model)

- DistilBERT gibi teknikler kullanarak modeli hızlandırabilirsiniz. 
- Bölüm 8'de bilgi damıtma (Knowledge Distillation) ve transformer modellerinizi hızlandırmak için diğer püf noktalarını ele alacağız.

## Transformer Modellerinin Çok Yönlülüğü (Versatility of Transformers)

- Transformer'lar son derece çok yönlüdür (Extremely Versatile).
- Kitabın geri kalanında, aynı temel mimariyi (Basic Architecture) kullanarak soru cevaplama (Question Answering) ve adlandırılmış varlık tanıma (Named Entity Recognition) gibi bir dizi görevi keşfedeceğiz.
- Transformer'lar çok dilli (Multilingual) bir çeşitlilikte gelir ve Bölüm 4'te bunları birden fazla dili aynı anda ele almak için kullanacağız.

## Az Veri ile Çalışma (Dealing with Limited Labeled Data)

- Etiketli veri (Labeled Data) çok az olduğunda, ince ayar yapmak (Fine-Tuning) mümkün olmayabilir.
- Bölüm 9'da bu durumla başa çıkmak için bazı teknikleri keşfedeceğiz.

## Kendi Transformer Modelinizi Oluşturma (Implementing Your Own Transformer Model)

- Artık bir transformer'ı eğitmek ve paylaşmak için neler gerektiğini gördüğümüze göre, bir sonraki bölümde kendi transformer modelimizi sıfırdan nasıl oluşturacağımızı keşfedeceğiz.

Kod örneği bulunmamaktadır, ancak metinde geçen teknik terimlerin açıklamaları aşağıda verilmiştir:

- **Transformer Model**: Metin verilerini işlemek için kullanılan bir derin öğrenme modelidir.
- **Features**: Modelin girdi olarak kullandığı özellikler.
- **Fine-Tuning**: Önceden eğitilmiş bir modelin belirli bir görev için yeniden eğitilmesi.
- **Inference Endpoint**: Modelin tahminler sunduğu uç nokta.
- **HTTP Requests**: Modeli çağırmak için kullanılan HTTP istekleri.
- **Inference API**: Modelin çıkarım yapması için kullanılan API.
- **DistilBERT**: BERT modelinin daha küçük ve hızlı bir versiyonudur.
- **Knowledge Distillation**: Bir modelin başka bir modele bilgi aktarması süreci.
- **Basic Architecture**: Modelin temel mimarisi.
- **Question Answering**: Modelin soruları cevaplaması görevi.
- **Named Entity Recognition**: Modelin metindeki adlandırılmış varlıkları tanıması görevi.
- **Multilingual**: Birden fazla dili destekleyen modeller.
- **Labeled Data**: Etiketlenmiş veri, yani doğru cevapları bilinen veri.
- **Fine-Tuning**: Modelin belirli bir görev için yeniden eğitilmesi.

---

