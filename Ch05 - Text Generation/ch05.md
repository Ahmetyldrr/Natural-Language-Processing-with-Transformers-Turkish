## The Challenge with Generating Coherent Text

# TutarlÄ± Metin Ãœretme ZorluÄŸu (The Challenge with Generating Coherent Text)

Bu kitapta ÅŸimdiye kadar, Ã¶n eÄŸitim (pretraining) ve denetimli ince ayar (supervised fine-tuning) kombinasyonu yoluyla DoÄŸal Dil Ä°ÅŸleme (NLP) gÃ¶revlerini ele aldÄ±k. GÃ¶rev spesifik baÅŸlÄ±klar (task-specific heads) iÃ§in tahminler Ã¼retmek oldukÃ§a basit; model bazÄ± logitler (logits) Ã¼retir ve biz ya tahmin edilen sÄ±nÄ±fÄ± elde etmek iÃ§in maksimum deÄŸeri alÄ±rÄ±z ya da her sÄ±nÄ±f iÃ§in tahmin edilen olasÄ±lÄ±klarÄ± elde etmek iÃ§in bir softmax fonksiyonu uygularÄ±z. Buna karÅŸÄ±lÄ±k, modelin olasÄ±lÄ±kÃ§Ä± Ã§Ä±ktÄ±sÄ±nÄ± (probabilistic output) metne dÃ¶nÃ¼ÅŸtÃ¼rmek bir kod Ã§Ã¶zme yÃ¶ntemi (decoding method) gerektirir ve bu, metin Ã¼retimine Ã¶zgÃ¼ birkaÃ§ zorluk ortaya Ã§Ä±karÄ±r: Kod Ã§Ã¶zme iÅŸlemi yinelemeli (iterative) olarak yapÄ±lÄ±r ve bu nedenle bir modelin ileri geÃ§iÅŸi (forward pass) yoluyla girdileri bir kez geÃ§irmekten Ã§ok daha fazla hesaplama gerektirir. Ãœretilen metnin kalitesi ve Ã§eÅŸitliliÄŸi, kod Ã§Ã¶zme yÃ¶nteminin ve iliÅŸkili hiperparametrelerin (hyperparameters) seÃ§imine baÄŸlÄ±dÄ±r.

## Kod Ã‡Ã¶zme SÃ¼reci (Decoding Process)

Bu kod Ã§Ã¶zme sÃ¼recinin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamak iÃ§in, GPT-2'nin nasÄ±l Ã¶n eÄŸitim gÃ¶rdÃ¼ÄŸÃ¼nÃ¼ ve daha sonra metin Ã¼retmek iÃ§in nasÄ±l uygulandÄ±ÄŸÄ±nÄ± inceleyelim. DiÄŸer otoregresif (autoregressive) veya nedensel dil modelleri (causal language models) gibi, GPT-2 de bir dizi tokenin (ğ² = y1, y2, ... yt) metinde ortaya Ã§Ä±kma olasÄ±lÄ±ÄŸÄ±nÄ± P(ğ²|ğ±), bazÄ± baÅŸlangÄ±Ã§ istemleri veya baÄŸlam dizileri (ğ± = x1, x2, ... xk) verildiÄŸinde tahmin etmek iÃ§in Ã¶n eÄŸitim gÃ¶rÃ¼r. P(ğ²|ğ±)'i doÄŸrudan tahmin etmek iÃ§in yeterli eÄŸitim verisi elde etmek pratik olmadÄ±ÄŸÄ±ndan, olasÄ±lÄ±k zinciri kuralÄ±nÄ± (chain rule of probability) kullanarak bunu koÅŸullu olasÄ±lÄ±klarÄ±n bir Ã¼rÃ¼nÃ¼ olarak faktÃ¶rize etmek yaygÄ±ndÄ±r:
```
P(ğ²|ğ±) = P(y1|ğ±) * P(y2|y1, ğ±) * ... * P(yt|y<t, ğ±)
```
Burada y<t, y1, ..., yt-1 dizisini temsil eder.

## Metin Ãœretimi (Text Generation)

Bu sonraki token tahmini gÃ¶revini (next token prediction task) keyfi uzunlukta metin dizileri Ã¼retmek iÃ§in nasÄ±l uyarlayabileceÄŸimizi tahmin etmiÅŸ olabilirsiniz. Åekil 5-3'te gÃ¶sterildiÄŸi gibi, "Transformers are the" gibi bir istemle baÅŸlarÄ±z ve modeli bir sonraki tokeni tahmin etmek iÃ§in kullanÄ±rÄ±z. Bir sonraki tokeni belirledikten sonra, onu isteme ekleriz ve yeni girdi dizisini baÅŸka bir token Ã¼retmek iÃ§in kullanÄ±rÄ±z. Bu iÅŸlemi Ã¶zel bir son-of-sequence tokenine veya Ã¶nceden tanÄ±mlanmÄ±ÅŸ bir maksimum uzunluÄŸa ulaÅŸana kadar yaparÄ±z. Ã‡Ä±ktÄ± dizisi girdi isteminin seÃ§imine baÄŸlÄ± olduÄŸundan, bu tÃ¼r metin Ã¼retimi genellikle koÅŸullu metin Ã¼retimi (conditional text generation) olarak adlandÄ±rÄ±lÄ±r.

## Kod Ã‡Ã¶zme YÃ¶ntemleri (Decoding Methods)

Bu sÃ¼recin merkezinde, her zaman adÄ±mÄ±nda hangi tokenin seÃ§ileceÄŸini belirleyen bir kod Ã§Ã¶zme yÃ¶ntemi bulunur. Dil modeli baÅŸlÄ±ÄŸÄ± (language model head) her adÄ±mda sÃ¶zlÃ¼kteki her token iÃ§in bir logit zt,i Ã¼rettiÄŸinden, bir sonraki olasÄ± token wi iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± softmax kullanarak elde edebiliriz:
```python
P(wi|ğ±, y<t) = softmax(zt,i)
```
Ã‡oÄŸu kod Ã§Ã¶zme yÃ¶nteminin amacÄ±, bir ğ²^ seÃ§erek en olasÄ± genel diziyi aramaktÄ±r:
```
ğ²^ = argmax P(ğ²|ğ±)
```
ğ²^'i doÄŸrudan bulmak, dil modeliyle her olasÄ± diziyi deÄŸerlendirmek anlamÄ±na gelir. Bunu makul bir sÃ¼rede yapabilecek bir algoritma bulunmadÄ±ÄŸÄ±ndan, bunun yerine yaklaÅŸÄ±k yÃ¶ntemlere gÃ¼veniyoruz.

### Kod AÃ§Ä±klamasÄ±

YukarÄ±daki kod parÃ§acÄ±ÄŸÄ±nda kullanÄ±lan softmax fonksiyonu:
```python
import torch.nn.functional as F

# zt_i: logit deÄŸerleri
zt_i = torch.tensor([2.0, 1.0, 0.5])

# Softmax fonksiyonu uygulama
probabilities = F.softmax(zt_i, dim=0)

print(probabilities)
```
Bu kod, `zt_i` logit deÄŸerlerine softmax fonksiyonunu uygular ve olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± hesaplar.

1. `torch.tensor([2.0, 1.0, 0.5])`: Logit deÄŸerlerini temsil eden bir tensor oluÅŸturur.
2. `F.softmax(zt_i, dim=0)`: Softmax fonksiyonunu uygular. `dim=0` parametresi, softmax'Ä±n tensorÃ¼n ilk boyutuna uygulanacaÄŸÄ±nÄ± belirtir.
3. `print(probabilities)`: OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± yazdÄ±rÄ±r.

Bu kod, olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± hesaplamak iÃ§in kullanÄ±lÄ±r ve kod Ã§Ã¶zme yÃ¶ntemlerinde Ã¶nemli bir adÄ±mdÄ±r.

---

## Greedy Search Decoding

# Greedy Search Decoding (AÃ§gÃ¶zlÃ¼ Arama Kod Ã‡Ã¶zme)

AÃ§gÃ¶zlÃ¼ arama, bir modelin sÃ¼rekli Ã§Ä±ktÄ±sÄ±ndan ayrÄ±k tokenler elde etmek iÃ§in en basit kod Ã§Ã¶zme yÃ¶ntemidir. Her bir zaman adÄ±mÄ±nda en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip tokeni seÃ§er.

## Kod

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

input_txt = "Transformers are the"
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)

iterations = []
n_steps = 8
choices_per_step = 5

with torch.no_grad():
    for _ in range(n_steps):
        iteration = dict()
        iteration["Input"] = tokenizer.decode(input_ids[0])
        output = model(input_ids=input_ids)
        
        # Son tokenin logitsini seÃ§ ve softmax uygula
        next_token_logits = output.logits[0, -1, :]
        next_token_probs = torch.softmax(next_token_logits, dim=-1)
        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
        
        # En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip tokenleri sakla
        for choice_idx in range(choices_per_step):
            token_id = sorted_ids[choice_idx]
            token_prob = next_token_probs[token_id].cpu().numpy()
            token_choice = f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"
            iteration[f"Choice {choice_idx+1}"] = token_choice
        
        # Tahmin edilen sonraki tokeni inputa ekle
        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
        iterations.append(iteration)

pd.DataFrame(iterations)
```

### Kod AÃ§Ä±klamasÄ±

1. `import torch`: PyTorch kÃ¼tÃ¼phanesini iÃ§e aktarÄ±r.
2. `from transformers import AutoTokenizer, AutoModelForCausalLM`: Transformers kÃ¼tÃ¼phanesinden AutoTokenizer ve AutoModelForCausalLM sÄ±nÄ±flarÄ±nÄ± iÃ§e aktarÄ±r.
3. `device = "cuda" if torch.cuda.is_available() else "cpu"`: EÄŸer CUDA destekli bir GPU varsa "cuda" olarak, yoksa "cpu" olarak ayarlar.
4. `model_name = "gpt2-xl"`: KullanÄ±lacak modelin adÄ±nÄ± belirler.
5. `tokenizer = AutoTokenizer.from_pretrained(model_name)`: Belirtilen model iÃ§in tokenizer'Ä± yÃ¼kler.
6. `model = AutoModelForCausalLM.from_pretrained(model_name).to(device)`: Belirtilen model iÃ§in Causal LM modelini yÃ¼kler ve belirtilen cihaza taÅŸÄ±r.
7. `input_txt = "Transformers are the"`: GiriÅŸ metnini belirler.
8. `input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)`: GiriÅŸ metnini tokenlara Ã§evirir ve belirtilen cihaza taÅŸÄ±r.
9. `iterations = []`: Her bir zaman adÄ±mÄ±nda elde edilen sonuÃ§larÄ± saklamak iÃ§in bir liste oluÅŸturur.
10. `n_steps = 8`: Kod Ã§Ã¶zme iÅŸleminin kaÃ§ adÄ±m devam edeceÄŸini belirler.
11. `choices_per_step = 5`: Her bir adÄ±mda kaÃ§ tane alternatif token gÃ¶sterileceÄŸini belirler.
12. `with torch.no_grad():`: Gradyan hesaplamalarÄ±nÄ± devre dÄ±ÅŸÄ± bÄ±rakÄ±r.
13. `for _ in range(n_steps):`: Belirtilen adÄ±m sayÄ±sÄ± kadar dÃ¶ngÃ¼ oluÅŸturur.
14. `iteration = dict()`: Her bir adÄ±mda elde edilen sonuÃ§larÄ± saklamak iÃ§in bir sÃ¶zlÃ¼k oluÅŸturur.
15. `output = model(input_ids=input_ids)`: Modeli belirtilen giriÅŸ tokenlarÄ± ile Ã§alÄ±ÅŸtÄ±rÄ±r.
16. `next_token_logits = output.logits[0, -1, :]`: Son tokenin logitsini seÃ§er.
17. `next_token_probs = torch.softmax(next_token_logits, dim=-1)`: Logits deÄŸerlerini softmax fonksiyonu ile olasÄ±lÄ±klara Ã§evirir.
18. `sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)`: OlasÄ±lÄ±klarÄ± sÄ±ralar ve en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip tokenin indeksini bulur.
19. `token_choice = f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"`: SeÃ§ilen tokeni ve olasÄ±lÄ±ÄŸÄ±nÄ± bir string olarak biÃ§imlendirir.
20. `input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)`: Tahmin edilen sonraki tokeni inputa ekler.

## Greedy Search Decoding ile Metin Ãœretme

AÃ§gÃ¶zlÃ¼ arama ile metin Ã¼retmek iÃ§in Transformers kÃ¼tÃ¼phanesinin `generate()` fonksiyonunu kullanabiliriz.

```python
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)
print(tokenizer.decode(output[0]))
```

### Kod AÃ§Ä±klamasÄ±

1. `input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)`: GiriÅŸ metnini tokenlara Ã§evirir ve belirtilen cihaza taÅŸÄ±r.
2. `output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)`: Modeli belirtilen giriÅŸ tokenlarÄ± ile Ã§alÄ±ÅŸtÄ±rÄ±r ve `max_new_tokens` kadar yeni token Ã¼retir. `do_sample=False` parametresi aÃ§gÃ¶zlÃ¼ arama yapÄ±lacaÄŸÄ±nÄ± belirtir.
3. `print(tokenizer.decode(output[0]))`: Ãœretilen metni yazdÄ±rÄ±r.

## SonuÃ§

AÃ§gÃ¶zlÃ¼ arama, basit ve hÄ±zlÄ± bir metin Ã¼retme yÃ¶ntemidir. Ancak, tekrarlayan metinler Ã¼retebilir ve optimal sonuÃ§lar vermeyebilir. Daha geliÅŸmiÅŸ metin Ã¼retme yÃ¶ntemleri iÃ§in beam search decoding gibi yÃ¶ntemler kullanÄ±labilir.

---

## Beam Search Decoding

# Beam Search Decoding (Beam Arama Kod Ã‡Ã¶zÃ¼mÃ¼)

Beam search decoding, her adÄ±mda en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip tokeni Ã§Ã¶zmek yerine, en olasÄ± b sonraki tokeni takip eden bir yÃ¶ntemdir. Burada b, beam sayÄ±sÄ± veya kÄ±smi hipotez sayÄ±sÄ± olarak adlandÄ±rÄ±lÄ±r.

## NasÄ±l Ã‡alÄ±ÅŸÄ±r?

1. Mevcut token dizisine gÃ¶re en olasÄ± b sonraki tokeni seÃ§er.
2. SeÃ§ilen tokenlerin olasÄ±lÄ±klarÄ±nÄ± hesaplar ve en olasÄ± b diziyi seÃ§er.
3. Bu iÅŸlem maksimum uzunluÄŸa veya EOS (End Of Sequence) tokenÄ±na ulaÅŸana kadar devam eder.
4. En olasÄ± dizi, beam'lerin log olasÄ±lÄ±klarÄ±na gÃ¶re sÄ±ralanarak seÃ§ilir.

## Log OlasÄ±lÄ±klarÄ±nÄ±n KullanÄ±lmasÄ±

Dizi olasÄ±lÄ±klarÄ±nÄ± hesaplamak iÃ§in log olasÄ±lÄ±klarÄ± kullanÄ±lÄ±r. Bunun nedeni, olasÄ±lÄ±klarÄ±n Ã§arpÄ±mÄ±nÄ±n sayÄ±sal kararsÄ±zlÄ±klara yol aÃ§abilmesidir. Log olasÄ±lÄ±klarÄ± kullanmak, bu sorunu Ã§Ã¶zer ve daha kararlÄ± sonuÃ§lar verir.

Ã–rneÄŸin, bir dizi tokenin olasÄ±lÄ±ÄŸÄ± 0.5 ** 1024 = 5.562684646268003e-309 gibi Ã§ok kÃ¼Ã§Ã¼k bir sayÄ± olabilir. Bu, sayÄ±sal kararsÄ±zlÄ±klara yol aÃ§abilir.

```python
import numpy as np
sum([np.log(0.5)] * 1024)
```

Bu kod, log olasÄ±lÄ±klarÄ±nÄ±n toplamÄ±nÄ± hesaplar ve -709.7827128933695 sonucunu verir.

## Kod AÃ§Ä±klamalarÄ±

### `log_probs_from_logits` Fonksiyonu

Bu fonksiyon, logits deÄŸerlerinden log olasÄ±lÄ±klarÄ±nÄ± hesaplar.

```python
import torch.nn.functional as F
def log_probs_from_logits(logits, labels):
    logp = F.log_softmax(logits, dim=-1)
    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
    return logp_label
```

*   `F.log_softmax(logits, dim=-1)`: Logits deÄŸerlerini log softmax'a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
*   `torch.gather(logp, 2, labels.unsqueeze(2))`: Log olasÄ±lÄ±klarÄ±nÄ± etiketlere gÃ¶re toplar.

### `sequence_logprob` Fonksiyonu

Bu fonksiyon, bir dizi tokenin log olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar.

```python
def sequence_logprob(model, labels, input_len=0):
    with torch.no_grad():
        output = model(labels)
        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])
        seq_log_prob = torch.sum(log_probs[:, input_len:])
    return seq_log_prob.cpu().numpy()
```

*   `model(labels)`: Modelin Ã§Ä±ktÄ±sÄ±nÄ± hesaplar.
*   `log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])`: Log olasÄ±lÄ±klarÄ±nÄ± hesaplar.
*   `torch.sum(log_probs[:, input_len:])`: Log olasÄ±lÄ±klarÄ±nÄ±n toplamÄ±nÄ± hesaplar.

## Ã–rnek KullanÄ±mlar

### Greedy Decoding

```python
logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))
print(tokenizer.decode(output_greedy[0]))
print(f"\nlog-prob: {logp:.2f}")
```

Bu kod, greedy decoding ile oluÅŸturulan dizinin log olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar ve yazdÄ±rÄ±r.

### Beam Search Decoding

```python
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")
```

Bu kod, beam search decoding ile oluÅŸturulan dizinin log olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar ve yazdÄ±rÄ±r.

### n-gram Penalty ile Beam Search Decoding

```python
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")
```

Bu kod, n-gram penalty ile beam search decoding ile oluÅŸturulan dizinin log olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar ve yazdÄ±rÄ±r.

Beam search decoding, greedy decoding'e gÃ¶re daha iyi sonuÃ§lar verir, ancak daha yavaÅŸ Ã§alÄ±ÅŸÄ±r. n-gram penalty ile birlikte kullanÄ±ldÄ±ÄŸÄ±nda, tekrarlarÄ± azaltmaya yardÄ±mcÄ± olur.

---

## Sampling Methods

# Ã–rnekleme YÃ¶ntemleri (Sampling Methods)

En basit Ã¶rnekleme yÃ¶ntemi, her bir zaman adÄ±mÄ±nda modelin Ã§Ä±ktÄ± olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±ndan (probability distribution) rastgele Ã¶rneklemektir. Burada |V|, kelime haznesinin (vocabulary) kardinalitesini (cardinality) temsil eder.

## SÄ±caklÄ±k Parametresi (Temperature Parameter)

Ã‡Ä±ktÄ±nÄ±n Ã§eÅŸitliliÄŸini (diversity), sÄ±caklÄ±k parametresi T ile kontrol edebiliriz. SÄ±caklÄ±k parametresi, softmax fonksiyonundan Ã¶nce logitleri yeniden Ã¶lÃ§eklendirir (rescale). T deÄŸerini ayarlayarak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±n ÅŸeklini kontrol edebiliriz. 
T â‰ª 1 olduÄŸunda, daÄŸÄ±lÄ±m orijin etrafÄ±nda yoÄŸunlaÅŸÄ±r (peaked) ve nadir tokenler (rare tokens) baskÄ±lanÄ±r (suppressed). 
T â‰« 1 olduÄŸunda, daÄŸÄ±lÄ±m dÃ¼zleÅŸir (flatten) ve her bir token eÅŸit olasÄ±lÄ±ÄŸa sahip olur.

### Kod Ã–rneÄŸi
```python
output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)
print(tokenizer.decode(output_temp[0]))
```
**Kod AÃ§Ä±klamasÄ±**

- `model.generate()`: Modelin metin oluÅŸturma fonksiyonu.
- `input_ids`: GiriÅŸ metninin token ID'leri.
- `max_length`: OluÅŸturulacak metnin maksimum uzunluÄŸu.
- `do_sample=True`: Ã–rnekleme yapar.
- `temperature=2.0`: SÄ±caklÄ±k parametresi.
- `top_k=0`: Top-k Ã¶rneklemesi iÃ§in parametre (sonraki bÃ¶lÃ¼mde aÃ§Ä±klanacaktÄ±r).

**Ã‡Ä±ktÄ±**
In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

While the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational ï¿½rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter

YÃ¼ksek sÄ±caklÄ±k deÄŸerinin nadir tokenleri Ã¶ne Ã§Ä±kardÄ±ÄŸÄ± ve garip gramer ve uydurma kelimeler Ã¼rettiÄŸi gÃ¶rÃ¼lmektedir.

## SÄ±caklÄ±k DeÄŸerini DÃ¼ÅŸÃ¼rme (Cooling Down the Temperature)

SÄ±caklÄ±k deÄŸerini dÃ¼ÅŸÃ¼rerek daha tutarlÄ± metinler Ã¼retebiliriz.
```python
output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)
print(tokenizer.decode(output_temp[0]))
```
**Ã‡Ä±ktÄ±**
In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

The scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.

The unicorns were living in a remote valley in the Andes mountains

'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.

'But when 

DÃ¼ÅŸÃ¼k sÄ±caklÄ±k deÄŸeri daha tutarlÄ± ve anlamlÄ± metinler Ã¼retir.

## Top-k ve Nucleus (Top-p) Ã–rneklemesi

SÄ±caklÄ±k parametresi ile birlikte, daÄŸÄ±lÄ±mÄ±n kesilmesi (truncation) ile de Ã§eÅŸitlilik ve tutarlÄ±lÄ±k arasÄ±nda denge kurulabilir. Top-k ve nucleus (top-p) Ã¶rneklemesi bu amaÃ§la kullanÄ±lÄ±r.

Bu yÃ¶ntemler, olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± belirli bir aralÄ±ÄŸa sÄ±nÄ±rlar ve bÃ¶ylece daha az olasÄ±lÄ±klÄ± kelimelerin seÃ§ilmesini engeller.

---

## Top-k and Nucleus Sampling

# Top-k ve Nucleus Sampling (Top-p Sampling)

Top-k ve nucleus sampling, dil modellemesinde kullanÄ±lan iki popÃ¼ler alternatif veya geniÅŸletmedir. Her iki durumda da, temel fikir her bir zaman adÄ±mÄ±nda Ã¶rnekleyebileceÄŸimiz olasÄ± token sayÄ±sÄ±nÄ± sÄ±nÄ±rlamaktÄ±r.

## Top-k Sampling

Top-k sampling'in arkasÄ±ndaki fikir, dÃ¼ÅŸÃ¼k olasÄ±lÄ±klÄ± seÃ§imlerden kaÃ§Ä±narak yalnÄ±zca en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip k token arasÄ±ndan Ã¶rnekleme yapmaktÄ±r. Bu, daÄŸÄ±lÄ±mÄ±n uzun kuyruÄŸuna sabit bir kesinti koyar ve yalnÄ±zca olasÄ± seÃ§imlerden Ã¶rnekleme yapmamÄ±zÄ± saÄŸlar.

### Kod Ã–rneÄŸi
```python
output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)
print(tokenizer.decode(output_topk[0]))
```
### Kod AÃ§Ä±klamasÄ±

* `model.generate()`: Dil modeli kullanarak metin Ã¼retir.
* `input_ids`: GiriÅŸ token IDs.
* `max_length`: Ãœretilecek metnin maksimum uzunluÄŸu.
* `do_sample=True`: Ã–rnekleme yapar.
* `top_k=50`: En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip 50 token arasÄ±ndan Ã¶rnekleme yapar.

## Nucleus Sampling (Top-p Sampling)

Nucleus sampling, sabit bir kesinti deÄŸeri yerine, seÃ§imde belirli bir olasÄ±lÄ±k kÃ¼tlesine ulaÅŸÄ±ldÄ±ÄŸÄ±nda kesinti yapar. Ã–rneÄŸin, %95 olasÄ±lÄ±k kÃ¼tlesine ulaÅŸana kadar token eklemeye devam eder.

### Kod Ã–rneÄŸi
```python
output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)
print(tokenizer.decode(output_topp[0]))
```
### Kod AÃ§Ä±klamasÄ±

* `model.generate()`: Dil modeli kullanarak metin Ã¼retir.
* `input_ids`: GiriÅŸ token IDs.
* `max_length`: Ãœretilecek metnin maksimum uzunluÄŸu.
* `do_sample=True`: Ã–rnekleme yapar.
* `top_p=0.90`: %90 olasÄ±lÄ±k kÃ¼tlesine ulaÅŸana kadar token ekler.

## Top-k ve Nucleus Sampling'in Birlikte KullanÄ±lmasÄ±

Ä°ki Ã¶rnekleme yaklaÅŸÄ±mÄ±nÄ± birleÅŸtirerek her iki dÃ¼nyanÄ±n en iyisini elde edebilirsiniz. `top_k=50` ve `top_p=0.9` ayarlamak, en fazla 50 token arasÄ±ndan %90 olasÄ±lÄ±k kÃ¼tlesine sahip tokenleri seÃ§me kuralÄ±na karÅŸÄ±lÄ±k gelir.

## Beam Search ile Ã–rnekleme

Ã–rnekleme yaptÄ±ÄŸÄ±mÄ±zda beam search'i de uygulayabiliriz. Bir sonraki aday token grubunu aÃ§ gÃ¶zlÃ¼lÃ¼kle seÃ§mek yerine, Ã¶rnekleyerek ve aynÄ± ÅŸekilde Ä±ÅŸÄ±nlarÄ± oluÅŸturarak yapabiliriz.

### Ã–nemli Noktalar

* Top-k sampling, dÃ¼ÅŸÃ¼k olasÄ±lÄ±klÄ± seÃ§imlerden kaÃ§Ä±narak yalnÄ±zca en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip k token arasÄ±ndan Ã¶rnekleme yapar.
* Nucleus sampling, sabit bir kesinti deÄŸeri yerine, seÃ§imde belirli bir olasÄ±lÄ±k kÃ¼tlesine ulaÅŸÄ±ldÄ±ÄŸÄ±nda kesinti yapar.
* Ä°ki Ã¶rnekleme yaklaÅŸÄ±mÄ±nÄ± birleÅŸtirerek her iki dÃ¼nyanÄ±n en iyisini elde edebilirsiniz.
* Beam search ile Ã¶rnekleme yapÄ±ldÄ±ÄŸÄ±nda, bir sonraki aday token grubunu Ã¶rnekleyerek ve aynÄ± ÅŸekilde Ä±ÅŸÄ±nlarÄ± oluÅŸturarak yapabiliriz.

---

## Which Decoding Method Is Best?

# En Ä°yi Kod Ã‡Ã¶zme YÃ¶ntemi Hangisidir? (Which Decoding Method Is Best?)

Ne yazÄ±k ki, evrensel olarak "en iyi" bir kod Ã§Ã¶zme yÃ¶ntemi (decoding method) yoktur. En iyi yaklaÅŸÄ±m, metin oluÅŸturduÄŸunuz gÃ¶revin doÄŸasÄ±na (task nature) baÄŸlÄ±dÄ±r. 

## Hassas GÃ¶revler Ä°Ã§in (For Precise Tasks)
EÄŸer modelinizin aritmetik iÅŸlemler yapmasÄ± veya belirli bir soruya cevap vermesi gibi hassas bir gÃ¶rev yapmasÄ±nÄ± istiyorsanÄ±z, sÄ±caklÄ±ÄŸÄ± (temperature) dÃ¼ÅŸÃ¼rmeli veya greedy arama (greedy search) gibi deterministik yÃ¶ntemleri (deterministic methods) Ä±ÅŸÄ±n aramasÄ± (beam search) ile birlikte kullanarak en olasÄ± cevabÄ± garanti altÄ±na almalÄ±sÄ±nÄ±z.

## YaratÄ±cÄ± GÃ¶revler Ä°Ã§in (For Creative Tasks)
EÄŸer modelin daha uzun metinler oluÅŸturmasÄ±nÄ± ve yaratÄ±cÄ± olmasÄ±nÄ± istiyorsanÄ±z, Ã¶rnekleme yÃ¶ntemlerine (sampling methods) geÃ§meli ve sÄ±caklÄ±ÄŸÄ± artÄ±rmalÄ±sÄ±nÄ±z veya Ã¼st-k (top-k) ve nucleus Ã¶rneklemeyi (nucleus sampling) karÄ±ÅŸÄ±k olarak kullanmalÄ±sÄ±nÄ±z.

### Ã–rnek Kod ParÃ§acÄ±klarÄ±
AÅŸaÄŸÄ±daki kod parÃ§acÄ±klarÄ±, farklÄ± kod Ã§Ã¶zme yÃ¶ntemlerini gÃ¶stermektedir:
```python
# Greedy Arama ile IÅŸÄ±n AramasÄ± (Greedy Search with Beam Search)
output = model.generate(input_ids, 
                        max_length=50, 
                        num_beams=5, 
                        no_repeat_ngram_size=2, 
                        early_stopping=True)

# Ã–rnekleme YÃ¶ntemi ile Metin OluÅŸturma (Text Generation with Sampling Method)
output = model.generate(input_ids, 
                        max_length=50, 
                        do_sample=True, 
                        top_k=50, 
                        top_p=0.95, 
                        temperature=1.0)
```

### Kod AÃ§Ä±klamalarÄ±
1. `model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)`:
   - `input_ids`: GiriÅŸ metninin kimliklerini (input IDs) temsil eder.
   - `max_length=50`: OluÅŸturulacak metnin maksimum uzunluÄŸunu belirler.
   - `num_beams=5`: IÅŸÄ±n aramasÄ± iÃ§in Ä±ÅŸÄ±n sayÄ±sÄ±nÄ± (number of beams) belirler.
   - `no_repeat_ngram_size=2`: Tekrar eden n-gramlarÄ±n boyutunu belirler ve bu boyuttaki n-gramlarÄ±n tekrar etmesini engeller.
   - `early_stopping=True`: IÅŸÄ±n aramasÄ± sÄ±rasÄ±nda erken durdurmayÄ± etkinleÅŸtirir.

2. `model.generate(input_ids, max_length=50, do_sample=True, top_k=50, top_p=0.95, temperature=1.0)`:
   - `do_sample=True`: Ã–rnekleme yÃ¶ntemini etkinleÅŸtirir.
   - `top_k=50`: Ãœst-k Ã¶rneklemeyi etkinleÅŸtirir ve en olasÄ± k tane tokeni dikkate alÄ±r.
   - `top_p=0.95`: Nucleus Ã¶rneklemeyi etkinleÅŸtirir ve kÃ¼mÃ¼latif olasÄ±lÄ±ÄŸÄ± p kadar olan tokenleri dikkate alÄ±r.
   - `temperature=1.0`: SÄ±caklÄ±ÄŸÄ± belirler; yÃ¼ksek sÄ±caklÄ±k daha rastgele sonuÃ§lar Ã¼retir.

Bu kod parÃ§acÄ±klarÄ±, metin oluÅŸturma gÃ¶revlerinde farklÄ± kod Ã§Ã¶zme yÃ¶ntemlerinin nasÄ±l kullanÄ±labileceÄŸini gÃ¶stermektedir.

---

## Conclusion

# Metin Ãœretimi (Text Generation) ve ZorluklarÄ±

Bu bÃ¶lÃ¼mde, daha Ã¶nce karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±z DoÄŸal Dil Anlama (NLU - Natural Language Understanding) gÃ¶revlerinden oldukÃ§a farklÄ± olan metin Ã¼retimi gÃ¶revini inceledik. Metin Ã¼retimi, en az bir ileri geÃ§iÅŸ (forward pass) gerektirir ve Ä±ÅŸÄ±n aramasÄ± (beam search) kullanÄ±ldÄ±ÄŸÄ±nda bu sayÄ± daha da artar. Bu, metin Ã¼retimini hesaplama aÃ§Ä±sÄ±ndan zorlu bir gÃ¶rev haline getirir ve bÃ¼yÃ¼k Ã¶lÃ§ekli metin Ã¼retimi modellerini Ã§alÄ±ÅŸtÄ±rmak iÃ§in uygun altyapÄ±ya ihtiyaÃ§ duyulur.

## Metin Ãœretiminde Kodlama Stratejisi (Decoding Strategy)

Ä°yi bir kodlama stratejisi, modelin Ã§Ä±ktÄ± olasÄ±lÄ±klarÄ±nÄ± ayrÄ±k tokenlere (discrete tokens) dÃ¶nÃ¼ÅŸtÃ¼rerek metin kalitesini artÄ±rabilir. En iyi kodlama stratejisini bulmak, bazÄ± deneyler ve Ã¼retilen metinlerin Ã¶znel deÄŸerlendirmesini gerektirir. Ancak pratikte, bu kararlarÄ± yalnÄ±zca sezgilere dayanarak vermek istemeyiz! DiÄŸer NLP gÃ¶revlerinde olduÄŸu gibi, Ã§Ã¶zmek istediÄŸimiz problemi yansÄ±tan bir model performans metriÄŸi (model performance metric) seÃ§meliyiz.

## Metin Ãœretimi iÃ§in Performans Metrikleri

Beklenmedik olmayan bir ÅŸekilde, Ã§ok Ã§eÅŸitli seÃ§enekler vardÄ±r ve bir sonraki bÃ¶lÃ¼mde en yaygÄ±n olanlarÄ± ele alacaÄŸÄ±z. Burada, bir metin Ã¶zetleme (text summarization) modeli iÃ§in nasÄ±l eÄŸitileceÄŸini ve deÄŸerlendirileceÄŸini inceleyeceÄŸiz. Veya bir GPT-tipi modeli sÄ±fÄ±rdan nasÄ±l eÄŸiteceÄŸinizi Ã¶ÄŸrenmek iÃ§in sabÄ±rsÄ±zlanÄ±yorsanÄ±z, 10. BÃ¶lÃ¼me geÃ§ebilirsiniz, burada bÃ¼yÃ¼k bir kod veri kÃ¼mesi topluyor ve ardÄ±ndan bu veri kÃ¼mesi Ã¼zerinde bir otoregresif dil modeli (autoregressive language model) eÄŸitiyoruz.

### Ã–nemli Noktalar:
- Metin Ã¼retimi, NLU gÃ¶revlerinden farklÄ±dÄ±r.
- Metin Ã¼retimi hesaplama aÃ§Ä±sÄ±ndan zorlu bir gÃ¶revdir.
- Ä°yi bir kodlama stratejisi metin kalitesini artÄ±rabilir.
- Model performans metriÄŸi seÃ§imi Ã¶nemlidir.

### KullanÄ±lan Teknik Terimler:
- DoÄŸal Dil Anlama (NLU - Natural Language Understanding)
- Ä°leri GeÃ§iÅŸ (Forward Pass)
- IÅŸÄ±n AramasÄ± (Beam Search)
- Kodlama Stratejisi (Decoding Strategy)
- AyrÄ±k Tokenler (Discrete Tokens)
- Model Performans Metrikleri (Model Performance Metrics)
- Metin Ã–zetleme (Text Summarization)
- Otoregresif Dil Modeli (Autoregressive Language Model)

Bu bÃ¶lÃ¼mde kod Ã¶rneÄŸi bulunmamaktadÄ±r. Ancak, ileriki bÃ¶lÃ¼mlerde GPT-tipi model eÄŸitimi iÃ§in kod Ã¶rnekleri verilecektir. Ã–rneÄŸin, bir otoregresif dil modelinin eÄŸitimi iÃ§in aÅŸaÄŸÄ±daki gibi bir kod bloÄŸu kullanÄ±labilir:

```python
# Ã–rnek kod bloÄŸu
import torch
import torch.nn as nn
import torch.optim as optim

# Model tanÄ±mlama
class AutoregressiveLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(AutoregressiveLanguageModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.rnn(embedded)
        output = self.fc(output[:, -1, :])
        return output

# Model, kayÄ±p fonksiyonu ve optimizasyon algoritmasÄ± tanÄ±mlama
model = AutoregressiveLanguageModel(vocab_size=1000, embedding_dim=128, hidden_dim=256)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# EÄŸitim dÃ¶ngÃ¼sÃ¼
for epoch in range(10):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
```

### Kod AÃ§Ä±klamasÄ±:
1. `import torch`: PyTorch kÃ¼tÃ¼phanesini iÃ§e aktarÄ±r.
2. `import torch.nn as nn`: PyTorch'un sinir aÄŸlarÄ± modÃ¼lÃ¼nÃ¼ iÃ§e aktarÄ±r.
3. `import torch.optim as optim`: PyTorch'un optimizasyon algoritmalarÄ± modÃ¼lÃ¼nÃ¼ iÃ§e aktarÄ±r.
4. `AutoregressiveLanguageModel` sÄ±nÄ±fÄ±: Otoregresif dil modelini tanÄ±mlar. Bu model, bir girdi dizisini alÄ±r ve bir Ã§Ä±ktÄ± dizisi Ã¼retir.
5. `forward` metodu: Modelin ileri geÃ§iÅŸini tanÄ±mlar. Girdi dizisini gÃ¶mÃ¼lÃ¼ temsiline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, RNN katmanÄ±ndan geÃ§irir ve son olarak doÄŸrusal bir katmandan geÃ§irerek Ã§Ä±ktÄ± Ã¼retir.
6. `model = AutoregressiveLanguageModel(...)`: Otoregresif dil modelini Ã¶rnekler.
7. `criterion = nn.CrossEntropyLoss()`: KayÄ±p fonksiyonunu tanÄ±mlar. Ã‡apraz entropi kaybÄ±, sÄ±nÄ±flandÄ±rma problemlerinde yaygÄ±n olarak kullanÄ±lÄ±r.
8. `optimizer = optim.Adam(model.parameters(), lr=0.001)`: Optimizasyon algoritmasÄ±nÄ± tanÄ±mlar. Adam optimizasyonu, deÄŸiÅŸken Ã¶ÄŸrenme oranlarÄ± ile gradient descent algoritmasÄ±nÄ±n bir varyantÄ±dÄ±r.
9. EÄŸitim dÃ¶ngÃ¼sÃ¼: Modeli eÄŸitmek iÃ§in kullanÄ±lan dÃ¶ngÃ¼. Her bir epoch'ta, modelin Ã§Ä±ktÄ±larÄ± hesaplanÄ±r, kayÄ±p hesaplanÄ±r, gradientler hesaplanÄ±r ve model parametreleri gÃ¼ncellenir.

---

