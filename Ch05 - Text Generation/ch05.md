## The Challenge with Generating Coherent Text

# Tutarlı Metin Üretme Zorluğu (The Challenge with Generating Coherent Text)

Bu kitapta şimdiye kadar, ön eğitim (pretraining) ve denetimli ince ayar (supervised fine-tuning) kombinasyonu yoluyla Doğal Dil İşleme (NLP) görevlerini ele aldık. Görev spesifik başlıklar (task-specific heads) için tahminler üretmek oldukça basit; model bazı logitler (logits) üretir ve biz ya tahmin edilen sınıfı elde etmek için maksimum değeri alırız ya da her sınıf için tahmin edilen olasılıkları elde etmek için bir softmax fonksiyonu uygularız. Buna karşılık, modelin olasılıkçı çıktısını (probabilistic output) metne dönüştürmek bir kod çözme yöntemi (decoding method) gerektirir ve bu, metin üretimine özgü birkaç zorluk ortaya çıkarır: Kod çözme işlemi yinelemeli (iterative) olarak yapılır ve bu nedenle bir modelin ileri geçişi (forward pass) yoluyla girdileri bir kez geçirmekten çok daha fazla hesaplama gerektirir. Üretilen metnin kalitesi ve çeşitliliği, kod çözme yönteminin ve ilişkili hiperparametrelerin (hyperparameters) seçimine bağlıdır.

## Kod Çözme Süreci (Decoding Process)

Bu kod çözme sürecinin nasıl çalıştığını anlamak için, GPT-2'nin nasıl ön eğitim gördüğünü ve daha sonra metin üretmek için nasıl uygulandığını inceleyelim. Diğer otoregresif (autoregressive) veya nedensel dil modelleri (causal language models) gibi, GPT-2 de bir dizi tokenin (𝐲 = y1, y2, ... yt) metinde ortaya çıkma olasılığını P(𝐲|𝐱), bazı başlangıç istemleri veya bağlam dizileri (𝐱 = x1, x2, ... xk) verildiğinde tahmin etmek için ön eğitim görür. P(𝐲|𝐱)'i doğrudan tahmin etmek için yeterli eğitim verisi elde etmek pratik olmadığından, olasılık zinciri kuralını (chain rule of probability) kullanarak bunu koşullu olasılıkların bir ürünü olarak faktörize etmek yaygındır:
```
P(𝐲|𝐱) = P(y1|𝐱) * P(y2|y1, 𝐱) * ... * P(yt|y<t, 𝐱)
```
Burada y<t, y1, ..., yt-1 dizisini temsil eder.

## Metin Üretimi (Text Generation)

Bu sonraki token tahmini görevini (next token prediction task) keyfi uzunlukta metin dizileri üretmek için nasıl uyarlayabileceğimizi tahmin etmiş olabilirsiniz. Şekil 5-3'te gösterildiği gibi, "Transformers are the" gibi bir istemle başlarız ve modeli bir sonraki tokeni tahmin etmek için kullanırız. Bir sonraki tokeni belirledikten sonra, onu isteme ekleriz ve yeni girdi dizisini başka bir token üretmek için kullanırız. Bu işlemi özel bir son-of-sequence tokenine veya önceden tanımlanmış bir maksimum uzunluğa ulaşana kadar yaparız. Çıktı dizisi girdi isteminin seçimine bağlı olduğundan, bu tür metin üretimi genellikle koşullu metin üretimi (conditional text generation) olarak adlandırılır.

## Kod Çözme Yöntemleri (Decoding Methods)

Bu sürecin merkezinde, her zaman adımında hangi tokenin seçileceğini belirleyen bir kod çözme yöntemi bulunur. Dil modeli başlığı (language model head) her adımda sözlükteki her token için bir logit zt,i ürettiğinden, bir sonraki olası token wi için olasılık dağılımını softmax kullanarak elde edebiliriz:
```python
P(wi|𝐱, y<t) = softmax(zt,i)
```
Çoğu kod çözme yönteminin amacı, bir 𝐲^ seçerek en olası genel diziyi aramaktır:
```
𝐲^ = argmax P(𝐲|𝐱)
```
𝐲^'i doğrudan bulmak, dil modeliyle her olası diziyi değerlendirmek anlamına gelir. Bunu makul bir sürede yapabilecek bir algoritma bulunmadığından, bunun yerine yaklaşık yöntemlere güveniyoruz.

### Kod Açıklaması

Yukarıdaki kod parçacığında kullanılan softmax fonksiyonu:
```python
import torch.nn.functional as F

# zt_i: logit değerleri
zt_i = torch.tensor([2.0, 1.0, 0.5])

# Softmax fonksiyonu uygulama
probabilities = F.softmax(zt_i, dim=0)

print(probabilities)
```
Bu kod, `zt_i` logit değerlerine softmax fonksiyonunu uygular ve olasılık dağılımını hesaplar.

1. `torch.tensor([2.0, 1.0, 0.5])`: Logit değerlerini temsil eden bir tensor oluşturur.
2. `F.softmax(zt_i, dim=0)`: Softmax fonksiyonunu uygular. `dim=0` parametresi, softmax'ın tensorün ilk boyutuna uygulanacağını belirtir.
3. `print(probabilities)`: Olasılık dağılımını yazdırır.

Bu kod, olasılık dağılımını hesaplamak için kullanılır ve kod çözme yöntemlerinde önemli bir adımdır.

---

## Greedy Search Decoding

# Greedy Search Decoding (Açgözlü Arama Kod Çözme)

Açgözlü arama, bir modelin sürekli çıktısından ayrık tokenler elde etmek için en basit kod çözme yöntemidir. Her bir zaman adımında en yüksek olasılığa sahip tokeni seçer.

## Kod

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

input_txt = "Transformers are the"
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)

iterations = []
n_steps = 8
choices_per_step = 5

with torch.no_grad():
    for _ in range(n_steps):
        iteration = dict()
        iteration["Input"] = tokenizer.decode(input_ids[0])
        output = model(input_ids=input_ids)
        
        # Son tokenin logitsini seç ve softmax uygula
        next_token_logits = output.logits[0, -1, :]
        next_token_probs = torch.softmax(next_token_logits, dim=-1)
        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
        
        # En yüksek olasılığa sahip tokenleri sakla
        for choice_idx in range(choices_per_step):
            token_id = sorted_ids[choice_idx]
            token_prob = next_token_probs[token_id].cpu().numpy()
            token_choice = f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"
            iteration[f"Choice {choice_idx+1}"] = token_choice
        
        # Tahmin edilen sonraki tokeni inputa ekle
        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
        iterations.append(iteration)

pd.DataFrame(iterations)
```

### Kod Açıklaması

1. `import torch`: PyTorch kütüphanesini içe aktarır.
2. `from transformers import AutoTokenizer, AutoModelForCausalLM`: Transformers kütüphanesinden AutoTokenizer ve AutoModelForCausalLM sınıflarını içe aktarır.
3. `device = "cuda" if torch.cuda.is_available() else "cpu"`: Eğer CUDA destekli bir GPU varsa "cuda" olarak, yoksa "cpu" olarak ayarlar.
4. `model_name = "gpt2-xl"`: Kullanılacak modelin adını belirler.
5. `tokenizer = AutoTokenizer.from_pretrained(model_name)`: Belirtilen model için tokenizer'ı yükler.
6. `model = AutoModelForCausalLM.from_pretrained(model_name).to(device)`: Belirtilen model için Causal LM modelini yükler ve belirtilen cihaza taşır.
7. `input_txt = "Transformers are the"`: Giriş metnini belirler.
8. `input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)`: Giriş metnini tokenlara çevirir ve belirtilen cihaza taşır.
9. `iterations = []`: Her bir zaman adımında elde edilen sonuçları saklamak için bir liste oluşturur.
10. `n_steps = 8`: Kod çözme işleminin kaç adım devam edeceğini belirler.
11. `choices_per_step = 5`: Her bir adımda kaç tane alternatif token gösterileceğini belirler.
12. `with torch.no_grad():`: Gradyan hesaplamalarını devre dışı bırakır.
13. `for _ in range(n_steps):`: Belirtilen adım sayısı kadar döngü oluşturur.
14. `iteration = dict()`: Her bir adımda elde edilen sonuçları saklamak için bir sözlük oluşturur.
15. `output = model(input_ids=input_ids)`: Modeli belirtilen giriş tokenları ile çalıştırır.
16. `next_token_logits = output.logits[0, -1, :]`: Son tokenin logitsini seçer.
17. `next_token_probs = torch.softmax(next_token_logits, dim=-1)`: Logits değerlerini softmax fonksiyonu ile olasılıklara çevirir.
18. `sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)`: Olasılıkları sıralar ve en yüksek olasılığa sahip tokenin indeksini bulur.
19. `token_choice = f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"`: Seçilen tokeni ve olasılığını bir string olarak biçimlendirir.
20. `input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)`: Tahmin edilen sonraki tokeni inputa ekler.

## Greedy Search Decoding ile Metin Üretme

Açgözlü arama ile metin üretmek için Transformers kütüphanesinin `generate()` fonksiyonunu kullanabiliriz.

```python
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)
print(tokenizer.decode(output[0]))
```

### Kod Açıklaması

1. `input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)`: Giriş metnini tokenlara çevirir ve belirtilen cihaza taşır.
2. `output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)`: Modeli belirtilen giriş tokenları ile çalıştırır ve `max_new_tokens` kadar yeni token üretir. `do_sample=False` parametresi açgözlü arama yapılacağını belirtir.
3. `print(tokenizer.decode(output[0]))`: Üretilen metni yazdırır.

## Sonuç

Açgözlü arama, basit ve hızlı bir metin üretme yöntemidir. Ancak, tekrarlayan metinler üretebilir ve optimal sonuçlar vermeyebilir. Daha gelişmiş metin üretme yöntemleri için beam search decoding gibi yöntemler kullanılabilir.

---

## Beam Search Decoding

# Beam Search Decoding (Beam Arama Kod Çözümü)

Beam search decoding, her adımda en yüksek olasılığa sahip tokeni çözmek yerine, en olası b sonraki tokeni takip eden bir yöntemdir. Burada b, beam sayısı veya kısmi hipotez sayısı olarak adlandırılır.

## Nasıl Çalışır?

1. Mevcut token dizisine göre en olası b sonraki tokeni seçer.
2. Seçilen tokenlerin olasılıklarını hesaplar ve en olası b diziyi seçer.
3. Bu işlem maksimum uzunluğa veya EOS (End Of Sequence) tokenına ulaşana kadar devam eder.
4. En olası dizi, beam'lerin log olasılıklarına göre sıralanarak seçilir.

## Log Olasılıklarının Kullanılması

Dizi olasılıklarını hesaplamak için log olasılıkları kullanılır. Bunun nedeni, olasılıkların çarpımının sayısal kararsızlıklara yol açabilmesidir. Log olasılıkları kullanmak, bu sorunu çözer ve daha kararlı sonuçlar verir.

Örneğin, bir dizi tokenin olasılığı 0.5 ** 1024 = 5.562684646268003e-309 gibi çok küçük bir sayı olabilir. Bu, sayısal kararsızlıklara yol açabilir.

```python
import numpy as np
sum([np.log(0.5)] * 1024)
```

Bu kod, log olasılıklarının toplamını hesaplar ve -709.7827128933695 sonucunu verir.

## Kod Açıklamaları

### `log_probs_from_logits` Fonksiyonu

Bu fonksiyon, logits değerlerinden log olasılıklarını hesaplar.

```python
import torch.nn.functional as F
def log_probs_from_logits(logits, labels):
    logp = F.log_softmax(logits, dim=-1)
    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
    return logp_label
```

*   `F.log_softmax(logits, dim=-1)`: Logits değerlerini log softmax'a dönüştürür.
*   `torch.gather(logp, 2, labels.unsqueeze(2))`: Log olasılıklarını etiketlere göre toplar.

### `sequence_logprob` Fonksiyonu

Bu fonksiyon, bir dizi tokenin log olasılığını hesaplar.

```python
def sequence_logprob(model, labels, input_len=0):
    with torch.no_grad():
        output = model(labels)
        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])
        seq_log_prob = torch.sum(log_probs[:, input_len:])
    return seq_log_prob.cpu().numpy()
```

*   `model(labels)`: Modelin çıktısını hesaplar.
*   `log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])`: Log olasılıklarını hesaplar.
*   `torch.sum(log_probs[:, input_len:])`: Log olasılıklarının toplamını hesaplar.

## Örnek Kullanımlar

### Greedy Decoding

```python
logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))
print(tokenizer.decode(output_greedy[0]))
print(f"\nlog-prob: {logp:.2f}")
```

Bu kod, greedy decoding ile oluşturulan dizinin log olasılığını hesaplar ve yazdırır.

### Beam Search Decoding

```python
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")
```

Bu kod, beam search decoding ile oluşturulan dizinin log olasılığını hesaplar ve yazdırır.

### n-gram Penalty ile Beam Search Decoding

```python
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")
```

Bu kod, n-gram penalty ile beam search decoding ile oluşturulan dizinin log olasılığını hesaplar ve yazdırır.

Beam search decoding, greedy decoding'e göre daha iyi sonuçlar verir, ancak daha yavaş çalışır. n-gram penalty ile birlikte kullanıldığında, tekrarları azaltmaya yardımcı olur.

---

## Sampling Methods

# Örnekleme Yöntemleri (Sampling Methods)

En basit örnekleme yöntemi, her bir zaman adımında modelin çıktı olasılık dağılımından (probability distribution) rastgele örneklemektir. Burada |V|, kelime haznesinin (vocabulary) kardinalitesini (cardinality) temsil eder.

## Sıcaklık Parametresi (Temperature Parameter)

Çıktının çeşitliliğini (diversity), sıcaklık parametresi T ile kontrol edebiliriz. Sıcaklık parametresi, softmax fonksiyonundan önce logitleri yeniden ölçeklendirir (rescale). T değerini ayarlayarak olasılık dağılımının şeklini kontrol edebiliriz. 
T ≪ 1 olduğunda, dağılım orijin etrafında yoğunlaşır (peaked) ve nadir tokenler (rare tokens) baskılanır (suppressed). 
T ≫ 1 olduğunda, dağılım düzleşir (flatten) ve her bir token eşit olasılığa sahip olur.

### Kod Örneği
```python
output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)
print(tokenizer.decode(output_temp[0]))
```
**Kod Açıklaması**

- `model.generate()`: Modelin metin oluşturma fonksiyonu.
- `input_ids`: Giriş metninin token ID'leri.
- `max_length`: Oluşturulacak metnin maksimum uzunluğu.
- `do_sample=True`: Örnekleme yapar.
- `temperature=2.0`: Sıcaklık parametresi.
- `top_k=0`: Top-k örneklemesi için parametre (sonraki bölümde açıklanacaktır).

**Çıktı**
In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

While the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter

Yüksek sıcaklık değerinin nadir tokenleri öne çıkardığı ve garip gramer ve uydurma kelimeler ürettiği görülmektedir.

## Sıcaklık Değerini Düşürme (Cooling Down the Temperature)

Sıcaklık değerini düşürerek daha tutarlı metinler üretebiliriz.
```python
output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)
print(tokenizer.decode(output_temp[0]))
```
**Çıktı**
In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

The scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.

The unicorns were living in a remote valley in the Andes mountains

'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.

'But when 

Düşük sıcaklık değeri daha tutarlı ve anlamlı metinler üretir.

## Top-k ve Nucleus (Top-p) Örneklemesi

Sıcaklık parametresi ile birlikte, dağılımın kesilmesi (truncation) ile de çeşitlilik ve tutarlılık arasında denge kurulabilir. Top-k ve nucleus (top-p) örneklemesi bu amaçla kullanılır.

Bu yöntemler, olasılık dağılımını belirli bir aralığa sınırlar ve böylece daha az olasılıklı kelimelerin seçilmesini engeller.

---

## Top-k and Nucleus Sampling

# Top-k ve Nucleus Sampling (Top-p Sampling)

Top-k ve nucleus sampling, dil modellemesinde kullanılan iki popüler alternatif veya genişletmedir. Her iki durumda da, temel fikir her bir zaman adımında örnekleyebileceğimiz olası token sayısını sınırlamaktır.

## Top-k Sampling

Top-k sampling'in arkasındaki fikir, düşük olasılıklı seçimlerden kaçınarak yalnızca en yüksek olasılığa sahip k token arasından örnekleme yapmaktır. Bu, dağılımın uzun kuyruğuna sabit bir kesinti koyar ve yalnızca olası seçimlerden örnekleme yapmamızı sağlar.

### Kod Örneği
```python
output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)
print(tokenizer.decode(output_topk[0]))
```
### Kod Açıklaması

* `model.generate()`: Dil modeli kullanarak metin üretir.
* `input_ids`: Giriş token IDs.
* `max_length`: Üretilecek metnin maksimum uzunluğu.
* `do_sample=True`: Örnekleme yapar.
* `top_k=50`: En yüksek olasılığa sahip 50 token arasından örnekleme yapar.

## Nucleus Sampling (Top-p Sampling)

Nucleus sampling, sabit bir kesinti değeri yerine, seçimde belirli bir olasılık kütlesine ulaşıldığında kesinti yapar. Örneğin, %95 olasılık kütlesine ulaşana kadar token eklemeye devam eder.

### Kod Örneği
```python
output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)
print(tokenizer.decode(output_topp[0]))
```
### Kod Açıklaması

* `model.generate()`: Dil modeli kullanarak metin üretir.
* `input_ids`: Giriş token IDs.
* `max_length`: Üretilecek metnin maksimum uzunluğu.
* `do_sample=True`: Örnekleme yapar.
* `top_p=0.90`: %90 olasılık kütlesine ulaşana kadar token ekler.

## Top-k ve Nucleus Sampling'in Birlikte Kullanılması

İki örnekleme yaklaşımını birleştirerek her iki dünyanın en iyisini elde edebilirsiniz. `top_k=50` ve `top_p=0.9` ayarlamak, en fazla 50 token arasından %90 olasılık kütlesine sahip tokenleri seçme kuralına karşılık gelir.

## Beam Search ile Örnekleme

Örnekleme yaptığımızda beam search'i de uygulayabiliriz. Bir sonraki aday token grubunu aç gözlülükle seçmek yerine, örnekleyerek ve aynı şekilde ışınları oluşturarak yapabiliriz.

### Önemli Noktalar

* Top-k sampling, düşük olasılıklı seçimlerden kaçınarak yalnızca en yüksek olasılığa sahip k token arasından örnekleme yapar.
* Nucleus sampling, sabit bir kesinti değeri yerine, seçimde belirli bir olasılık kütlesine ulaşıldığında kesinti yapar.
* İki örnekleme yaklaşımını birleştirerek her iki dünyanın en iyisini elde edebilirsiniz.
* Beam search ile örnekleme yapıldığında, bir sonraki aday token grubunu örnekleyerek ve aynı şekilde ışınları oluşturarak yapabiliriz.

---

## Which Decoding Method Is Best?

# En İyi Kod Çözme Yöntemi Hangisidir? (Which Decoding Method Is Best?)

Ne yazık ki, evrensel olarak "en iyi" bir kod çözme yöntemi (decoding method) yoktur. En iyi yaklaşım, metin oluşturduğunuz görevin doğasına (task nature) bağlıdır. 

## Hassas Görevler İçin (For Precise Tasks)
Eğer modelinizin aritmetik işlemler yapması veya belirli bir soruya cevap vermesi gibi hassas bir görev yapmasını istiyorsanız, sıcaklığı (temperature) düşürmeli veya greedy arama (greedy search) gibi deterministik yöntemleri (deterministic methods) ışın araması (beam search) ile birlikte kullanarak en olası cevabı garanti altına almalısınız.

## Yaratıcı Görevler İçin (For Creative Tasks)
Eğer modelin daha uzun metinler oluşturmasını ve yaratıcı olmasını istiyorsanız, örnekleme yöntemlerine (sampling methods) geçmeli ve sıcaklığı artırmalısınız veya üst-k (top-k) ve nucleus örneklemeyi (nucleus sampling) karışık olarak kullanmalısınız.

### Örnek Kod Parçacıkları
Aşağıdaki kod parçacıkları, farklı kod çözme yöntemlerini göstermektedir:
```python
# Greedy Arama ile Işın Araması (Greedy Search with Beam Search)
output = model.generate(input_ids, 
                        max_length=50, 
                        num_beams=5, 
                        no_repeat_ngram_size=2, 
                        early_stopping=True)

# Örnekleme Yöntemi ile Metin Oluşturma (Text Generation with Sampling Method)
output = model.generate(input_ids, 
                        max_length=50, 
                        do_sample=True, 
                        top_k=50, 
                        top_p=0.95, 
                        temperature=1.0)
```

### Kod Açıklamaları
1. `model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)`:
   - `input_ids`: Giriş metninin kimliklerini (input IDs) temsil eder.
   - `max_length=50`: Oluşturulacak metnin maksimum uzunluğunu belirler.
   - `num_beams=5`: Işın araması için ışın sayısını (number of beams) belirler.
   - `no_repeat_ngram_size=2`: Tekrar eden n-gramların boyutunu belirler ve bu boyuttaki n-gramların tekrar etmesini engeller.
   - `early_stopping=True`: Işın araması sırasında erken durdurmayı etkinleştirir.

2. `model.generate(input_ids, max_length=50, do_sample=True, top_k=50, top_p=0.95, temperature=1.0)`:
   - `do_sample=True`: Örnekleme yöntemini etkinleştirir.
   - `top_k=50`: Üst-k örneklemeyi etkinleştirir ve en olası k tane tokeni dikkate alır.
   - `top_p=0.95`: Nucleus örneklemeyi etkinleştirir ve kümülatif olasılığı p kadar olan tokenleri dikkate alır.
   - `temperature=1.0`: Sıcaklığı belirler; yüksek sıcaklık daha rastgele sonuçlar üretir.

Bu kod parçacıkları, metin oluşturma görevlerinde farklı kod çözme yöntemlerinin nasıl kullanılabileceğini göstermektedir.

---

## Conclusion

# Metin Üretimi (Text Generation) ve Zorlukları

Bu bölümde, daha önce karşılaştığımız Doğal Dil Anlama (NLU - Natural Language Understanding) görevlerinden oldukça farklı olan metin üretimi görevini inceledik. Metin üretimi, en az bir ileri geçiş (forward pass) gerektirir ve ışın araması (beam search) kullanıldığında bu sayı daha da artar. Bu, metin üretimini hesaplama açısından zorlu bir görev haline getirir ve büyük ölçekli metin üretimi modellerini çalıştırmak için uygun altyapıya ihtiyaç duyulur.

## Metin Üretiminde Kodlama Stratejisi (Decoding Strategy)

İyi bir kodlama stratejisi, modelin çıktı olasılıklarını ayrık tokenlere (discrete tokens) dönüştürerek metin kalitesini artırabilir. En iyi kodlama stratejisini bulmak, bazı deneyler ve üretilen metinlerin öznel değerlendirmesini gerektirir. Ancak pratikte, bu kararları yalnızca sezgilere dayanarak vermek istemeyiz! Diğer NLP görevlerinde olduğu gibi, çözmek istediğimiz problemi yansıtan bir model performans metriği (model performance metric) seçmeliyiz.

## Metin Üretimi için Performans Metrikleri

Beklenmedik olmayan bir şekilde, çok çeşitli seçenekler vardır ve bir sonraki bölümde en yaygın olanları ele alacağız. Burada, bir metin özetleme (text summarization) modeli için nasıl eğitileceğini ve değerlendirileceğini inceleyeceğiz. Veya bir GPT-tipi modeli sıfırdan nasıl eğiteceğinizi öğrenmek için sabırsızlanıyorsanız, 10. Bölüme geçebilirsiniz, burada büyük bir kod veri kümesi topluyor ve ardından bu veri kümesi üzerinde bir otoregresif dil modeli (autoregressive language model) eğitiyoruz.

### Önemli Noktalar:
- Metin üretimi, NLU görevlerinden farklıdır.
- Metin üretimi hesaplama açısından zorlu bir görevdir.
- İyi bir kodlama stratejisi metin kalitesini artırabilir.
- Model performans metriği seçimi önemlidir.

### Kullanılan Teknik Terimler:
- Doğal Dil Anlama (NLU - Natural Language Understanding)
- İleri Geçiş (Forward Pass)
- Işın Araması (Beam Search)
- Kodlama Stratejisi (Decoding Strategy)
- Ayrık Tokenler (Discrete Tokens)
- Model Performans Metrikleri (Model Performance Metrics)
- Metin Özetleme (Text Summarization)
- Otoregresif Dil Modeli (Autoregressive Language Model)

Bu bölümde kod örneği bulunmamaktadır. Ancak, ileriki bölümlerde GPT-tipi model eğitimi için kod örnekleri verilecektir. Örneğin, bir otoregresif dil modelinin eğitimi için aşağıdaki gibi bir kod bloğu kullanılabilir:

```python
# Örnek kod bloğu
import torch
import torch.nn as nn
import torch.optim as optim

# Model tanımlama
class AutoregressiveLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(AutoregressiveLanguageModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.rnn(embedded)
        output = self.fc(output[:, -1, :])
        return output

# Model, kayıp fonksiyonu ve optimizasyon algoritması tanımlama
model = AutoregressiveLanguageModel(vocab_size=1000, embedding_dim=128, hidden_dim=256)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Eğitim döngüsü
for epoch in range(10):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
```

### Kod Açıklaması:
1. `import torch`: PyTorch kütüphanesini içe aktarır.
2. `import torch.nn as nn`: PyTorch'un sinir ağları modülünü içe aktarır.
3. `import torch.optim as optim`: PyTorch'un optimizasyon algoritmaları modülünü içe aktarır.
4. `AutoregressiveLanguageModel` sınıfı: Otoregresif dil modelini tanımlar. Bu model, bir girdi dizisini alır ve bir çıktı dizisi üretir.
5. `forward` metodu: Modelin ileri geçişini tanımlar. Girdi dizisini gömülü temsiline dönüştürür, RNN katmanından geçirir ve son olarak doğrusal bir katmandan geçirerek çıktı üretir.
6. `model = AutoregressiveLanguageModel(...)`: Otoregresif dil modelini örnekler.
7. `criterion = nn.CrossEntropyLoss()`: Kayıp fonksiyonunu tanımlar. Çapraz entropi kaybı, sınıflandırma problemlerinde yaygın olarak kullanılır.
8. `optimizer = optim.Adam(model.parameters(), lr=0.001)`: Optimizasyon algoritmasını tanımlar. Adam optimizasyonu, değişken öğrenme oranları ile gradient descent algoritmasının bir varyantıdır.
9. Eğitim döngüsü: Modeli eğitmek için kullanılan döngü. Her bir epoch'ta, modelin çıktıları hesaplanır, kayıp hesaplanır, gradientler hesaplanır ve model parametreleri güncellenir.

---

