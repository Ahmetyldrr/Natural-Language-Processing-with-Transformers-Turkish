## Large Datasets and Where to Find Them

# Büyük Veri Kümeleri ve Nerede Bulunurlar

Büyük veri kümeleri (Large Datasets), çeşitli alanlarda, hukuki belgelerden biyomedikal veri kümelerine ve programlama kod depolarına kadar birçok alanda bulunabilir. Çoğu durumda, bu veri kümeleri etiketlenmemiştir ve büyük boyutları nedeniyle genellikle yalnızca sezgisel yöntemler (heuristics) kullanılarak veya toplanma işlemi sırasında depolanan ilgili meta veriler kullanılarak etiketlenebilirler.

## Büyük Veri Kümelerinin Önemi

Çok büyük bir veri kümesi, etiketlenmemiş veya yalnızca sezgisel olarak etiketlenmiş olsa bile yararlı olabilir. Bölüm 9'da, bir veri kümesinin etiketlenmemiş kısmını alan adaptasyonu için bir dil modelini fine-tune etmek için kullandığımız bir örnek gördük. Bu yaklaşım, sınırlı veri mevcut olduğunda tipik olarak bir performans kazancı sağlar.

## Büyük Veri Kümelerinin Oluşturulması

Büyük bir ön eğitim veri kümesi oluşturmak, kendi zorluklarını beraberinde getirir. Ön eğitimden sonra bir modelin kalitesi, büyük ölçüde ön eğitim veri kümesinin kalitesini yansıtır. Model, ön eğitim veri kümesindeki herhangi bir kusuru devralacaktır. Bu nedenle, kendi veri kümemizi oluşturmaya çalışmadan önce, büyük veri kümeleri oluşturmayla ilgili bazı yaygın sorunların ve zorlukların farkında olmak iyidir.

### Büyük Veri Kümelerindeki Sorunlar

Büyük ölçekli veri kümeleri çoğunlukla yüksek derecede otomasyonla oluşturulur. Bu, içerikleri ve oluşturulma şekilleri üzerinde sınırlı kontrol olduğu anlamına gelir ve bu nedenle düşük kaliteli ve önyargılı veriler üzerinde model eğitimi riski artar. BookCorpus ve C4 gibi ünlü büyük ölçekli veri kümelerinin incelenmesi, çeşitli sorunları ortaya çıkarmıştır:
1. C4 veri kümesinin önemli bir kısmı insan tarafından değil, makine tarafından çevrilmiştir.
2. C4'te stopword filtrelemesi nedeniyle Afrika-Amerikan İngilizcesinin eşitsiz bir şekilde silinmesi, bu tür içeriklerin az temsil edilmesine yol açmıştır.
3. BookCorpus'ta telif hakkı ihlali örnekleri bulunmaktadır.

Bu keşifler, bu veri kümeleri üzerinde eğitilen modellerin downstream kullanımıyla uyumlu olmayabilir. Örneğin, BookCorpus'taki romantik romanların aşırı temsil edilmesi, modelin romantik roman yazma aracı olarak kullanılması durumunda kabul edilebilir olabilir.

## Model Eğitimi ve Veri Kümesi Kayması

Herhangi bir model, eğitim verilerindeki dil önyargısını ve popülasyonların ve olayların aşırı veya az temsil edilmesini yansıtacaktır. Modelin davranışındaki bu önyargılar, modelle etkileşimde bulunan hedef kitle açısından dikkate alınması önemlidir.

### GPT ve GPT-2 Karşılaştırması

GPT ve GPT-2 modellerini aynı girdiyle karşılaştırdığımızda, GPT'nin romantik içeriklere yönelik bir eğilim gösterdiği, GPT-2'nin ise daha nötr ve blog benzeri veya macera içerikli metinler ürettiği görülmektedir.

```python
from transformers import pipeline, set_seed

generation_gpt = pipeline("text-generation", model="openai-gpt")
generation_gpt2 = pipeline("text-generation", model="gpt2")

def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):
    out = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)
    return "\n".join(f"{i+1}." + s["generated_text"] for i, s in enumerate(out))

prompt = "\nWhen they came back"
print("GPT completions:\n" + enum_pipeline_ouputs(generation_gpt, prompt, 3))
print("")
print("GPT-2 completions:\n" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))
```

### Kodların Açıklaması

1. `from transformers import pipeline, set_seed`: Transformers kütüphanesinden `pipeline` ve `set_seed` fonksiyonlarını içe aktarır. `pipeline` fonksiyonu, belirli bir görev için önceden eğitilmiş bir model yüklemeye yarar.
2. `generation_gpt = pipeline("text-generation", model="openai-gpt")`: GPT modeli için bir metin oluşturma pipeline'ı oluşturur.
3. `def enum_pipeline_ouputs(pipe, prompt, num_return_sequences)`: Belirli bir prompt için bir pipeline'dan birden fazla çıktı üretmeye ve bunları numaralandırmaya yarar.
4. `prompt = "\nWhen they came back"`: Modelin metin oluşturmaya başlayacağı girdi cümlesini tanımlar.

## Büyük Veri Kümeleri Oluşturma: GitHub ve Google BigQuery

Python programlama dili için bir kod oluşturma modeli eğitmek amacıyla büyük bir ön eğitim veri kümesi oluşturmak istiyoruz. GitHub, açıkça erişilebilen terabaytlık kod depoları barındıran doğal bir kaynaktır. Google BigQuery, GitHub depolarını içeren halka açık veri kümeleri sağlar.

### Google BigQuery ile Veri Kümesi Oluşturma

1. Google Cloud hesabı oluşturun.
2. Google BigQuery projesinde bir veri kümesi ve tablo oluşturun.
3. Aşağıdaki SQL sorgusunu çalıştırın:
```sql
SELECT f.repo_name, f.path, c.copies, c.size, c.content, l.license 
FROM `bigquery-public-data.github_repos.files` AS f 
JOIN `bigquery-public-data.github_repos.contents` AS c ON f.id = c.id 
JOIN `bigquery-public-data.github_repos.licenses` AS l ON f.repo_name = l.repo_name 
WHERE NOT c.binary AND ((f.path LIKE '%.py') AND (c.size BETWEEN 1024 AND 1048575))
```
Bu sorgu, yaklaşık 2.6 TB veri işler ve 26.8 milyon Python dosyası içeren bir veri kümesi oluşturur.

### Kodların Açıklaması

1. `SELECT`: Seçilen sütunları belirtir.
2. `FROM` ve `JOIN`: İlgili tabloları birleştirir.
3. `WHERE`: Koşulları belirtir; burada ikili olmayan, `.py` uzantılı ve belirli bir boyut aralığındaki dosyaları seçer.

## Veri Kümesini İndirme ve Hazırlama

1. Sonuçları Google Cloud Storage'a (GCS) aktarın.
2. `gsutil` kütüphanesini kullanarak GCS'deki verileri yerel makineye indirin.

```bash
$ gsutil -m -o "GSUtil:parallel_process_count=1" cp -r gs://<name_of_bucket>
```

Alternatif olarak, veri kümesini Hugging Face Hub'dan doğrudan indirebilirsiniz:
```bash
$ git clone https://huggingface.co/datasets/transformersbook/codeparrot
```

## Büyük Veri Kümeleriyle Çalışma: Datasets Kütüphanesi

Datasets kütüphanesi, büyük veri kümeleriyle çalışmayı kolaylaştırır. Bellek haritalama (memory mapping) ve akış (streaming) gibi özellikler sunar.

### Bellek Haritalama

Datasets, varsayılan olarak sıfır kopyalı ve sıfır ek yükü bellek haritalama mekanizması kullanır. Bu, veri kümesini RAM yerine sabit diskte saklar ve bir işaretçi kullanarak erişir.

```python
from datasets import load_dataset, DownloadConfig

download_config = DownloadConfig(delete_extracted=True)
dataset = load_dataset("./codeparrot", split="train", download_config=download_config)
```

### Kodların Açıklaması

1. `load_dataset`: Belirtilen veri kümesini yükler.
2. `DownloadConfig(delete_extracted=True)`: İndirilen dosyaları çıkardıktan sonra silmeyi sağlar.

## Akış (Streaming)

Datasets, büyük veri kümelerini akış modunda yüklemeyi sağlar. Bu, veri kümesinin tamamını yerel diske indirmeden örnekleri doğrudan indirmeyi mümkün kılar.

```python
streamed_dataset = load_dataset('./codeparrot', split="train", streaming=True)
```

### Kodların Açıklaması

1. `load_dataset`: `streaming=True` parametresi ile akış modunda veri kümesini yükler.
2. `streamed_dataset`: IterableDataset nesnesi olarak döner, bu nedenle örnekleri sırayla okunmalıdır.

## Veri Kümesini Hugging Face Hub'a Yükleme

1. Hugging Face hesabınıza giriş yapın: `$ huggingface-cli login`
2. Veri kümesi için bir depo oluşturun: `$ huggingface-cli repo create --type dataset --organization transformersbook codeparrot-train`
3. Veri kümesini yükleyin ve itin.

```bash
$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-train
$ cd codeparrot-train
$ cp ../codeparrot/*.json.gz .
$ git add .
$ git commit -m "Adding dataset files"
$ git push
```

Bu adımları takip ederek, büyük bir Python kod veri kümesini oluşturabilir, işleyebilir ve Hugging Face Hub'a yükleyebilirsiniz.

---

## Building a Tokenizer

# Tokenizer Oluşturma (Building a Tokenizer)

## Giriş
Derin öğrenme modelleri için veri hazırlamada önemli bir adım olan tokenization (tokenleştirme) işlemini gerçekleştirmek için bir tokenizer oluşturma sürecini anlatan bu bölümde, özel bir kullanım durumu için tokenizer oluşturma adımları ele alınacaktır.

## Mevcut Tokenizerların Sınırlamaları
Mevcut tokenizeler, belirli bir veri kümesi üzerinde eğitildikleri için, farklı veri kümeleri üzerinde kullanıldıklarında sorunlar ortaya çıkabilir. Örneğin, T5 tokenizesi, C4 corpus üzerinde eğitilmiştir ve İngilizcedeki yaygın kelimeleri tanımayabilir. Benzer şekilde, CamemBERT tokenizesi, Fransızca metinler üzerinde eğitilmiştir ve İngilizce kelimeleri tanımayabilir.

```python
from transformers import AutoTokenizer

def tok_list(tokenizer, string):
    input_ids = tokenizer(string, add_special_tokens=False)["input_ids"]
    return [tokenizer.decode(tok) for tok in input_ids]

tokenizer_T5 = AutoTokenizer.from_pretrained("t5-base")
tokenizer_camembert = AutoTokenizer.from_pretrained("camembert-base")

print(f'T5 tokens for "sex": {tok_list(tokenizer_T5, "sex")}')
print(f'CamemBERT tokens for "being": {tok_list(tokenizer_camembert, "being")}')
```

Kod Açıklaması:
- `AutoTokenizer.from_pretrained()` methodu, önceden eğitilmiş bir tokenizer'ı yüklemek için kullanılır.
- `tok_list()` fonksiyonu, bir tokenizer ve bir string alır ve string'i tokenleştirerek her bir token'ı decode eder.

## Tokenizer Eğitimi
Tokenizer eğitimi, derin öğrenme modellerinin eğitilmesinden farklıdır. Tokenizer eğitimi, bir metin dizisini modele beslenebilecek bir tamsayı dizisine çevirmek için optimal bir eşleme oluşturma işlemidir.

### Tokenizer Pipeline
Tokenizer pipeline'ı dört adımdan oluşur: normalizasyon, pretokenization, tokenizer modeli ve postprocessing. Tokenizer modeli, veri üzerinde eğitilebilen pipeline'ın bir parçasıdır.

### Tokenization Algoritmaları
Farklı tokenization algoritmaları vardır, örneğin BPE (Byte-Pair Encoding), WordPiece ve Unigram. BPE, temel birimler listesinden başlayarak yeni token'lar oluşturur ve vocabulary'e ekler.

## Özel Tokenizer Oluşturma
Python kodu için özel bir tokenizer oluşturmak için, GPT-2 tokenizer'ı kullanılabilir. GPT-2 tokenizer'ı, byte-level BPE tokenization algoritmasını kullanır.

```python
python_code = r"""def say_hello():
    print("Hello, World!")
    # Print it
    say_hello()
"""

tokenizer = AutoTokenizer.from_pretrained("gpt2")
print(tokenizer(python_code).tokens())
```

Kod Açıklaması:
- `AutoTokenizer.from_pretrained("gpt2")`, GPT-2 tokenizer'ını yükler.
- `tokenizer(python_code).tokens()`, Python kodunu tokenleştirir.

## Byte-Level BPE Tokenization
Byte-level BPE tokenization, Unicode karakterlerini byte'lara çevirir ve daha sonra BPE algoritmasını uygular.

```python
a, e = u"a", u"€"
byte = ord(a.encode("utf-8"))
print(f'`{a}` is encoded as `{a.encode("utf-8")}` with a single byte: {byte}')

byte = [ord(chr(i)) for i in e.encode("utf-8")]
print(f'`{e}` is encoded as `{e.encode("utf-8")}` with three bytes: {byte}')
```

Kod Açıklaması:
- `ord(a.encode("utf-8"))`, 'a' karakterinin Unicode kod noktasını verir.
- `[ord(chr(i)) for i in e.encode("utf-8")]`, '€' karakterinin Unicode kod noktalarını verir.

## Özel Tokenizer Eğitimi
Özel tokenizer eğitimi için, `train_new_from_iterator()` methodu kullanılır.

```python
from tqdm.auto import tqdm

length = 100000
dataset_name = 'transformersbook/codeparrot-train'
dataset = load_dataset(dataset_name, split="train", streaming=True)
iter_dataset = iter(dataset)

def batch_iterator(batch_size=10):
    for _ in tqdm(range(0, length, batch_size)):
        yield [next(iter_dataset)['content'] for _ in range(batch_size)]

new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=12500, initial_alphabet=base_vocab)
```

Kod Açıklaması:
- `load_dataset()`, veri kümesini yükler.
- `batch_iterator()`, veri kümesinden örnekler alır ve tokenizer'ı eğitmek için kullanılır.
- `train_new_from_iterator()`, tokenizer'ı eğitir.

## Tokenizer Değerlendirmesi
Tokenizer'ın performansı, çeşitli metrikler kullanılarak değerlendirilebilir. Örneğin, subword fertility, continued words oranı ve coverage metrikleri kullanılabilir.

Tokenizer'ı değerlendirmek için, Python reserved keyword'lerinin tokenizer vocabulary'sinde olup olmadığı kontrol edilebilir.

```python
import keyword

for keyw in keyword.kwlist:
    if keyw not in new_tokenizer_larger.vocab:
        print(f'No, keyword `{keyw}` is not in the vocabulary')
```

Kod Açıklaması:
- `keyword.kwlist`, Python reserved keyword'lerini verir.
- `new_tokenizer_larger.vocab`, tokenizer vocabulary'sini verir.

## Tokenizer'ı Kaydetme
Tokenizer'ı kaydetmek için, `push_to_hub()` methodu kullanılır.

```python
model_ckpt = "codeparrot"
org = "transformersbook"
new_token.new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)
```

Kod Açıklaması:
- `push_to_hub()`, tokenizer'ı Hugging Face Hub'a yükler.

---

## Training a Model from Scratch

# Modeli Sıfırdan Eğitmek (Training a Model from Scratch)

Bu bölümde, bir modelin sıfırdan nasıl eğitileceğini anlatacağız. 
## Mimari Seçimi (Choosing the Architecture)
İlk adım, görev için en uygun mimariyi seçmektir. Kod tamamlama görevi için GPT-2 modelini kullanacağız.

## Modeli Başlatma (Initializing the Model)
Modeli önceden eğitilmiş ağırlıklar olmadan başlatacağız. 
```python
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
config = AutoConfig.from_pretrained("gpt2-xl", vocab_size=len(tokenizer))
model = AutoModelForCausalLM.from_config(config)
```
*   Yukarıdaki kodda, `AutoConfig` ve `AutoModelForCausalLM` sınıflarını kullanarak GPT-2 modelini başlatıyoruz.
*   `vocab_size` parametresi, tokenleştiricinin sözlüğünün boyutuna ayarlanır.

## Model Boyutu (Model Size)
Modelin boyutunu kontrol edelim:
```python
print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')
```
*   Bu kod, modelin parametre sayısını hesaplar ve ekrana yazdırır.

## Veri Yükleme (Data Loading)
Verileri modele beslemek için özel bir veri yükleme sınıfı oluşturacağız.
```python
class ConstantLengthDataset(IterableDataset):
    def __init__(self, tokenizer, dataset, seq_length=1024, num_of_sequences=1024, chars_per_token=3.6):
        # ...
```
*   Bu sınıf, `IterableDataset` sınıfından türetilmiştir ve `__iter__` metodu, veri kümesinden örnekler döndürür.

## Eğitim Döngüsü (Training Loop)
Eğitim döngüsünü oluşturmak için `Accelerate` kütüphanesini kullanacağız.
```python
accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)
```
*   `Accelerator` sınıfı, dağıtık eğitim için kullanılır.
*   `prepare` metodu, modeli, optimizasyon algoritmasını ve veri yükleyicilerini hazırlar.

## Eğitim Scripti (Training Script)
Tüm parçaları bir araya getirerek eğitim scriptini oluşturacağız.
```python
# ...
for step, batch in enumerate(train_dataloader, start=1):
    loss = model(batch, labels=batch).loss
    # ...
```
*   Bu script, modeli eğitmek için kullanılır.

## Eğitim Scriptinin Çalıştırılması (Running the Training Script)
Eğitim scriptini çalıştırmak için aşağıdaki komutları kullanacağız:
```bash
$ git clone https://huggingface.co/transformersbook/codeparrot
$ cd codeparrot
$ pip install -r requirements.txt
$ wandb login
$ accelerate config
$ accelerate launch codeparrot_training.py
```
*   Bu komutlar, eğitim scriptini çalıştırmak için gerekli adımları gerçekleştirir.

Kodların satır satır açıklamaları:

1.  `from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer`: Transformers kütüphanesinden gerekli sınıfları içe aktarır.
2.  `tokenizer = AutoTokenizer.from_pretrained(model_ckpt)`: Önceden eğitilmiş tokenleştiriciyi yükler.
3.  `config = AutoConfig.from_pretrained("gpt2-xl", vocab_size=len(tokenizer))`: GPT-2 modelinin konfigürasyonunu yükler ve sözlük boyutunu tokenleştiricinin sözlüğünün boyutuna ayarlar.
4.  `model = AutoModelForCausalLM.from_config(config)`: GPT-2 modelini konfigürasyona göre başlatır.
5.  `print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')`: Modelin parametre sayısını hesaplar ve ekrana yazdırır.
6.  `class ConstantLengthDataset(IterableDataset)`: Özel bir veri yükleme sınıfı tanımlar.
7.  `accelerator = Accelerator()`: Dağıtık eğitim için `Accelerator` sınıfını başlatır.
8.  `model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)`: Modeli, optimizasyon algoritmasını ve veri yükleyicilerini hazırlar.
9.  `for step, batch in enumerate(train_dataloader, start=1)`: Eğitim döngüsünü başlatır.
10. `$ git clone https://huggingface.co/transformersbook/codeparrot`: Eğitim scriptini içeren depoyu klonlar.
11. `$ cd codeparrot`: Depoya girer.
12. `$ pip install -r requirements.txt`: Gerekli Python bağımlılıklarını yükler.
13. `$ wandb login`: Weights & Biases'e giriş yapar.
14. `$ accelerate config`: Dağıtık eğitim için konfigürasyonu ayarlar.
15. `$ accelerate launch codeparrot_training.py`: Eğitim scriptini çalıştırır.

---

## Results and Analysis

# Sonuçlar ve Analiz (Results and Analysis)

Bir hafta boyunca logları endişeyle izledikten sonra, muhtemelen Şekil 10-7'de gösterilenlere benzer kayıp (loss) ve perplexity eğrileri göreceksiniz. Eğitim kaybı (training loss) ve doğrulama perplexity'si (validation perplexity) sürekli olarak azalır ve kayıp eğrisi log-log ölçeğinde neredeyse doğrusal görünür. Ayrıca, büyük modelin işlenen tokenler (processed tokens) açısından daha hızlı yakınsadığı (converge) görülür, ancak genel eğitim süresi daha uzundur.

## Nitel ve Nicel Analiz (Qualitative and Quantitative Analysis)

Taze pişmiş dil modelimizle (freshly baked language model) neler yapabiliriz? İki tür analiz yapabiliriz: nitel (qualitative) ve nicel (quantitative). Nitel analizde, somut örnekler (concrete examples) inceler ve modelin hangi durumlarda başarılı, hangi durumlarda başarısız olduğunu anlamaya çalışırız. Nicel analizde ise, modelin performansını istatistiksel olarak büyük bir test veri seti (large set of test cases) üzerinde değerlendiririz.

### Kod Tamamlama (Code Completion)

İlk olarak, küçük modeli bir pipeline içinde saralım (wrap) ve bazı kod girdilerini (code inputs) tamamlamak için kullanalım:
```python
from transformers import pipeline, set_seed

model_ckpt = 'transformersbook/codeparrot-small'
generation = pipeline('text-generation', model=model_ckpt, device=0)
```
Şimdi, `generation` pipeline'ını kullanarak belirli bir prompt'tan (prompt) aday tamamlamalar (candidate completions) üretebiliriz.

### Kod Tamamlama Fonksiyonu (Complete Code Function)

`complete_code()` fonksiyonu, CodeParrot tarafından üretilen tamamlamaları yazdırmak için kullanılır:
```python
import re

def first_block(string):
    return re.split('\n class|\n def|\n #|\n @|\n print|\n if', string)[0].rstrip()

def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):
    set_seed(seed)
    gen_kwargs = {"temperature": 0.4, "top_p": 0.95, "top_k": 0, "num_beams": 1, "do_sample": True}
    code_gens = generation(prompt, num_return_sequences=num_completions, max_length=max_length, **gen_kwargs)
    code_strings = []
    for code_gen in code_gens:
        generated_code = first_block(code_gen['generated_text'][len(prompt):])
        code_strings.append(generated_code)
    print(('\n' + '=' * 80 + '\n').join(code_strings))
```
### Örnekler (Examples)

Modelin bir dikdörtgenin alanını hesaplayan bir fonksiyon yazmasını isteyelim:
```python
prompt = '''def area_of_rectangle(a: float, b: float): """Return the area of the rectangle."""'''
complete_code(generation, prompt)
```
Çıktı:
```
return a * b
================================================================================
return a * b / 2.0
================================================================================
return math.sqrt(a * b)
================================================================================
return a * b / a
```
Modelin bir HTML dizesinden URL'leri çıkarmasını isteyelim:
```python
prompt = '''def get_urls_from_html(html): """Get all embedded URLs in a HTML string."""'''
complete_code(generation, prompt)
```
Çıktı:
```python
return [url for url in re.findall(r'<a href="(.*?)"', html) if url]
================================================================================
return [url for url in re.findall(r'<a href="(/[^/]+/[^"]+?)"', html)]
================================================================================
return [url for url in re.findall(r'<a href="(/.*)",', html)]
================================================================================
return re.findall(r'<a href="(.*?)" class="url"[^>]*>', html)
```
### Değerlendirme (Evaluation)

BLEU skoru (BLEU score), üretilen metinlerin kalitesini ölçmek için kullanılan bir metriktir. Ancak, kod yazarken değişken ve sınıf isimlerinde çok fazla özgürlüğümüz vardır ve bir programın başarısı tutarlı bir adlandırma şemasına bağlı değildir. Bu nedenle, BLEU skoru kod üretimi için uygun bir metrik değildir.

Bunun yerine, kodun kalitesini ölçmek için unit testler (unit tests) kullanılabilir. OpenAI Codex modelleri de bu şekilde değerlendirilmiştir.

### Kod Açıklamaları

* `first_block()` fonksiyonu, üretilen kodun ilk bloğunu ayıklamak için kullanılır.
* `complete_code()` fonksiyonu, CodeParrot tarafından üretilen tamamlamaları yazdırmak için kullanılır.
* `gen_kwargs` sözlüğü, üretim için kullanılan hiperparametreleri içerir (ör. `temperature`, `top_p`, `top_k`, vb.).
* `re.split()` fonksiyonu, bir dizeyi belirli bir desene göre bölmek için kullanılır.
* `re.findall()` fonksiyonu, bir dizede belirli bir deseni bulmak için kullanılır.

### Kullanılan Kodlar ve Açıklamaları

1. `from transformers import pipeline, set_seed` : Transformers kütüphanesinden `pipeline` ve `set_seed` fonksiyonlarını içe aktarır.
2. `model_ckpt = 'transformersbook/codeparrot-small'` : Kullanılacak modelin checkpoint'ini belirler.
3. `generation = pipeline('text-generation', model=model_ckpt, device=0)` : `text-generation` görevi için bir pipeline oluşturur ve modeli belirler.
4. `def first_block(string):` : Üretilen kodun ilk bloğunu ayıklamak için kullanılan fonksiyon.
5. `def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):` : CodeParrot tarafından üretilen tamamlamaları yazdırmak için kullanılan fonksiyon.
6. `set_seed(seed)` : Üretim için kullanılan tohum değerini belirler.
7. `gen_kwargs = {"temperature": 0.4, "top_p": 0.95, "top_k": 0, "num_beams": 1, "do_sample": True}` : Üretim için kullanılan hiperparametreleri içeren sözlük.
8. `code_gens = generation(prompt, num_return_sequences=num_completions, max_length=max_length, **gen_kwargs)` : Belirli bir prompt için aday tamamlamalar üretir.
9. `re.split('\n class|\n def|\n #|\n @|\n print|\n if', string)[0].rstrip()` : Üretilen kodun ilk bloğunu ayıklar.
10. `print(('\n' + '=' * 80 + '\n').join(code_strings))` : Üretilen tamamlamaları yazdırır.

---

## Conclusion

# Sonuç (Conclusion)

Bu bölümde, Python için kod tamamlama (code autocomplete) işlevi oluşturmayı başardık. İlk olarak, büyük bir dil modelini önceden eğitmek (pretraining) için uygun, özel büyük ölçekli bir veri kümesi (dataset) oluşturduk. Ardından, bu veri kümesiyle Python kodunu verimli bir şekilde kodlayabilen (encoding) özel bir tokenleştirici (tokenizer) oluşturduk. Son olarak, Accelerate'in yardımıyla her şeyi bir araya getirdik ve çoklu-GPU altyapısında (multi-GPU infrastructure) sıfırdan küçük ve büyük GPT-2 modellerini eğitmek için bir eğitim betiği (training script) yazdık. Bu işlem, iki yüz satırdan daha az kodla gerçekleştirildi.

Model çıktılarını inceleyerek, makul kod devamları (code continuations) üretebildiğini gördük ve modelin sistematik olarak nasıl değerlendirilebileceğini tartıştık. Artık Hub'daki önceden eğitilmiş birçok modeli fine-tune etmekle kalmayıp, yeterli veri ve hesaplama kaynağı olduğunda sıfırdan özel bir model önceden eğitme bilgisine de sahipsiniz. Artık transformerlarla hemen hemen her Doğal Dil İşleme (NLP - Natural Language Processing) kullanım senaryosuna hazırsınız.

## Önemli Noktalar (Key Points)

* Özel büyük ölçekli bir veri kümesi oluşturuldu (custom large-scale dataset).
* Özel bir tokenleştirici oluşturuldu (custom tokenizer).
* GPT-2 modeli sıfırdan eğitildi (training GPT-2 model from scratch).
* Accelerate kullanılarak çoklu-GPU altyapısında eğitim gerçekleştirildi (training on multi-GPU infrastructure with Accelerate).
* Model çıktıları incelendi ve makul kod devamları üretebildiği görüldü (investigating model outputs).

## Kullanılan Kodlar (Used Codes)

Bu bölümde kullanılan kodlar belirtilmemiştir, ancak Accelerate kullanıldığı 언급 edilmiştir. Aşağıda örnek bir Accelerate kodu ve açıklaması yer almaktadır:

```python
from accelerate import Accelerator

accelerator = Accelerator()

# Model, optimizer ve dataloader'ı Accelerate'e hazırlama
model, optimizer, train_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader
)

# Eğitim döngüsü içinde Accelerate'i kullanma
for batch in train_dataloader:
    inputs, labels = batch
    inputs, labels = inputs.to(accelerator.device), labels.to(accelerator.device)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = loss_fn(outputs, labels)
    accelerator.backward(loss)
    optimizer.step()
```

1. `from accelerate import Accelerator`: Accelerate kütüphanesinden `Accelerator` sınıfını içe aktarır.
2. `accelerator = Accelerator()`: `Accelerator` sınıfının bir örneğini oluşturur.
3. `model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)`: Model, optimizer ve dataloader'ı Accelerate'e hazırlar.
4. `inputs, labels = inputs.to(accelerator.device), labels.to(accelerator.device)`: Girdi ve etiketleri Accelerate'in belirlediği cihaza (GPU veya CPU) taşır.
5. `accelerator.backward(loss)`: Kaybın geriye doğru yayılımını Accelerate ile gerçekleştirir.

Bu kodlar, çoklu-GPU altyapısında model eğitimi gerçekleştirmek için kullanılır. Accelerate, eğitim sürecini hızlandırmak ve kolaylaştırmak için çeşitli optimizasyonlar sağlar.

---

