## Building a GitHub Issues Tagger

# GitHub Issues Tagger Oluşturma (Building a GitHub Issues Tagger)

Bu bölümde, GitHub Issues etiketleme problemi ele alınmaktadır. Problemin tanımı, veri kümesinin hazırlanması ve temizlenmesi anlatılmaktadır.

## Problem Tanımı (Problem Definition)

GitHub Issues sayfasında yer alan issue'lar bir başlık, açıklama ve bu issue'u karakterize eden etiketler içerir. Bu, supervised learning görevi olarak düşünülebilir: verilen bir issue'nun başlığı ve açıklaması temel alınarak bir veya daha fazla etiket tahmini yapmak.

## Veri Kümesinin Hazırlanması (Preparing the Dataset)

Veri kümesi, GitHub REST API kullanılarak elde edilir. Issues endpoint'i çağrıldığında, her bir issue hakkında birçok alan içeren JSON nesneleri listesi döndürülür. Bu alanlar arasında issue'nun durumu (açık veya kapalı), kimin açtığı, başlık, gövde ve etiketler bulunur.

### Kod: Veri Kümesinin İndirilmesi
```python
import pandas as pd
dataset_url = "https://git.io/nlp-with-transformers"
df_issues = pd.read_json(dataset_url, lines=True)
print(f"DataFrame shape: {df_issues.shape}")
```

*   `pd.read_json()`: JSON formatındaki verileri okumak için kullanılır.
*   `lines=True`: JSON verilerinin satır satır okunmasını sağlar.

### Veri Kümesinin Temizlenmesi (Cleaning the Dataset)

İndirilen veri kümesinde birçok alan bulunur, ancak biz sadece etiketler sütunuyla ilgileniyoruz. Her bir satırda etiketler sütunu, her bir etiket hakkında metadata içeren JSON nesneleri listesi içerir.

#### Kod: Etiketlerin Temizlenmesi
```python
df_issues["labels"] = df_issues["labels"].apply(lambda x: [meta["name"] for meta in x])
```

*   `apply()`: Veri kümesindeki her bir satıra bir fonksiyonu uygulamak için kullanılır.
*   `lambda x: [meta["name"] for meta in x]`: Her bir etiket listesindeki "name" alanını çıkarmak için kullanılır.

## Etiket Dağılımı (Label Distribution)

Etiket dağılımını incelemek için, etiketler sütununu "patlatırız" (explode) ve her bir etiketin kaç kez geçtiğini sayarız.

### Kod: Etiket Dağılımı
```python
df_counts = df_issues["labels"].explode().value_counts()
print(f"Number of labels: {len(df_counts)}")
```

*   `explode()`: Liste halinde olan etiketleri ayrı satırlara böler.
*   `value_counts()`: Her bir etiketin kaç kez geçtiğini sayar.

## Etiket Filtreleme (Label Filtering)

Veri kümesinde 65 benzersiz etiket bulunur, ancak bazı etiketler çok nadir geçmektedir. Bu nedenle, etiket sayısını azaltmak için bir etiket filtreleme işlemi uygulanır.

### Kod: Etiket Filtreleme
```python
label_map = {
    "Core: Tokenization": "tokenization",
    "New model": "new model",
    # ...
}

def filter_labels(x):
    return [label_map[label] for label in x if label in label_map]

df_issues["labels"] = df_issues["labels"].apply(filter_labels)
```

*   `label_map`: Etiketleri daha genel kategorilere eşlemek için kullanılır.
*   `filter_labels()`: Etiketleri filtrelemek için kullanılır.

## Eğitim ve Doğrulama Kümelerinin Oluşturulması (Creating Training and Validation Sets)

Eğitim ve doğrulama kümeleri oluşturmak için, Scikit-multilearn kütüphanesinden `iterative_train_test_split()` fonksiyonu kullanılır.

### Kod: Eğitim ve Doğrulama Kümeleri
```python
from skmultilearn.model_selection import iterative_train_test_split

def balanced_split(df, test_size=0.5):
    ind = np.expand_dims(np.arange(len(df)), axis=1)
    labels = mlb.transform(df["labels"])
    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels, test_size)
    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:, 0]]

df_train, df_tmp = balanced_split(df_sup, test_size=0.5)
df_valid, df_test = balanced_split(df_tmp, test_size=0.5)
```

*   `iterative_train_test_split()`: Dengeli eğitim ve test kümeleri oluşturmak için kullanılır.
*   `balanced_split()`: Dengeli eğitim ve doğrulama kümeleri oluşturmak için kullanılır.

## Veri Kümesinin Son Hali (Final Dataset)

Son olarak, veri kümesi `DatasetDict` formatına dönüştürülür.

### Kod: Veri Kümesinin Son Hali
```python
from datasets import Dataset, DatasetDict

ds = DatasetDict({
    "train": Dataset.from_pandas(df_train.reset_index(drop=True)),
    "valid": Dataset.from_pandas(df_valid.reset_index(drop=True)),
    "test": Dataset.from_pandas(df_test.reset_index(drop=True)),
    "unsup": Dataset.from_pandas(df_unsup.reset_index(drop=True))
})
```

*   `Dataset.from_pandas()`: Pandas DataFrame'i `Dataset` formatına dönüştürür.
*   `DatasetDict`: Farklı kümeleri bir arada tutmak için kullanılır.

---

## Implementing a Naive Bayesline

# Naive Bayes Sınıflandırıcısı ile Temel Bir Model Oluşturma (Implementing a Naive Bayes Baseline)

Yeni bir Doğal Dil İşleme (NLP - Natural Language Processing) projesi başlattığınızda, güçlü temel modeller (baseline) oluşturmak her zaman iyi bir fikirdir. Bunun iki ana nedeni vardır: 
- Basit bir model veya düzenli ifadeler (regular expressions) ve elle hazırlanmış kurallar (handcrafted rules) kullanarak problemi çözmek zaten oldukça iyi sonuçlar verebilir. Bu gibi durumlarda, daha karmaşık modeller olan transformer gibi modelleri kullanmak gerekmez, çünkü bu modeller üretim ortamlarında daha zor uygulanır ve bakım gerektirir.
- Temel modeller, daha karmaşık modelleri keşfederken hızlı kontroller sağlar.

## Naive Bayes Sınıflandırıcısı Nedir? (What is a Naive Bayes Classifier?)
Naive Bayes sınıflandırıcısı, metin sınıflandırma (text classification) için harika bir temel modeldir çünkü çok basit, hızlı bir şekilde eğitilebilir ve girdilerdeki değişimlere karşı oldukça sağlamdır (robust).

## Çoklu Etiket Sınıflandırması (Multilabel Classification)
Scikit-learn kütüphanesindeki Naive Bayes uygulaması, çoklu etiket sınıflandırmasını (multilabel classification) kutudan çıktığı haliyle desteklemez. Ancak Scikit-multilearn kütüphanesini kullanarak problemi bir-versus-gerisi (one-versus-rest) sınıflandırma görevi olarak yeniden şekillendirebiliriz. Bu yaklaşımda, L etiket için L ikili sınıflandırıcı (binary classifier) eğitiriz.

### Etiketlerin Hazırlanması (Preparing Labels)
Öncelikle, eğitim setlerimizde yeni bir `label_ids` sütunu oluşturmak için çoklu etiket ikili hale getiriciyi (multilabel binarizer) kullanacağız. `map()` fonksiyonunu kullanarak tüm işlemleri tek seferde halledebiliriz:
```python
def prepare_labels(batch):
    batch["label_ids"] = mlb.transform(batch["labels"])
    return batch

ds = ds.map(prepare_labels, batched=True)
```

- `prepare_labels` fonksiyonu, her bir batch'i alır ve `labels` sütununu `label_ids` sütununa dönüştürür.
- `mlb.transform()` fonksiyonu, etiketleri ikili vektörlere dönüştürür.
- `ds.map()` fonksiyonu, `prepare_labels` fonksiyonunu veri setinin her bir batch'ine uygular.

## Performans Ölçümü (Measuring Performance)
Sınıflandırıcılarımızın performansını ölçmek için mikro (micro) ve makro (macro) F1 skorlarını kullanacağız. Mikro F1 skoru, sık görülen etiketlerdeki performansı izlerken, makro F1 skoru tüm etiketlerdeki performansı frekanstan bağımsız olarak izler.

```python
from collections import defaultdict

macro_scores, micro_scores = defaultdict(list), defaultdict(list)
```

- `defaultdict` sınıfı, varsayılan değerleri olan bir sözlük oluşturur. Burada, skorları saklamak için kullanılır.

## Naive Bayes Modelinin Eğitimi (Training the Naive Bayes Model)
Şimdi, temel modelimizi eğitmeye hazırız! Modeli eğitmek ve sınıflandırıcımızı artan eğitim seti boyutlarında değerlendirmek için aşağıdaki kodu kullanacağız:
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
from skmultilearn.problem_transform import BinaryRelevance
from sklearn.feature_extraction.text import CountVectorizer

for train_slice in train_slices:
    # Eğitim slice'ını ve test verilerini al
    ds_train_sample = ds["train"].select(train_slice)
    y_train = np.array(ds_train_sample["label_ids"])
    y_test = np.array(ds["test"]["label_ids"])
    
    # Metinleri token sayıları olarak kodlamak için basit bir count vectorizer kullan
    count_vect = CountVectorizer()
    X_train_counts = count_vect.fit_transform(ds_train_sample["text"])
    X_test_counts = count_vect.transform(ds["test"]["text"])
    
    # Modeli oluştur ve eğit!
    classifier = BinaryRelevance(classifier=MultinomialNB())
    classifier.fit(X_train_counts, y_train)
    
    # Tahminler yap ve değerlendir
    y_pred_test = classifier.predict(X_test_counts)
    clf_report = classification_report(y_test, y_pred_test, target_names=mlb.classes_, zero_division=0, output_dict=True)
    
    # Skorları sakla
    macro_scores["Naive Bayes"].append(clf_report["macro avg"]["f1-score"])
    micro_scores["Naive Bayes"].append(clf_report["micro avg"]["f1-score"])
```

- `CountVectorizer`, metinleri token sayıları olarak kodlar. Bu, kelime torbası (bag-of-words) yaklaşımı olarak bilinir.
- `BinaryRelevance`, çoklu etiket sınıflandırmasını bir-versus-gerisi sınıflandırma görevine dönüştürür.
- `MultinomialNB`, Naive Bayes sınıflandırıcısıdır.

## Sonuçların Gösterilmesi (Plotting the Results)
Sonuçları göstermek için aşağıdaki yardımcı fonksiyonu kullanacağız:
```python
import matplotlib.pyplot as plt

def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):
    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)
    for run in micro_scores.keys():
        if run == current_model:
            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)
            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)
        else:
            ax0.plot(sample_sizes, micro_scores[run], label=run, linestyle="dashed")
            ax1.plot(sample_sizes, macro_scores[run], label=run, linestyle="dashed")
    ax0.set_title("Micro F1 scores")
    ax1.set_title("Macro F1 scores")
    ax0.set_ylabel("Test set F1 score")
    ax0.legend(loc="lower right")
    for ax in [ax0, ax1]:
        ax.set_xlabel("Number of training samples")
        ax.set_xscale("log")
        ax.set_xticks(sample_sizes)
        ax.set_xticklabels(sample_sizes)
        ax.minorticks_off()
    plt.tight_layout()
    plt.show()

plot_metrics(micro_scores, macro_scores, train_samples, "Naive Bayes")
```

- `plot_metrics` fonksiyonu, mikro ve makro F1 skorlarını eğitim örnek sayısına göre çizer.
- `sample_sizes` parametresi, eğitim seti boyutlarını içerir.

---

## Working with No Labeled Data

# Sıfır-Gözetimli Sınıflandırma (Zero-Shot Classification)

Sıfır-gözetimli sınıflandırma, hiç etiketli veri bulunmayan durumlarda kullanılan bir tekniktir. Bu teknik, önceden eğitilmiş bir modelin (pre-trained model) başka bir görev için kullanılmasını sağlar.

## Sıfır-Gözetimli Sınıflandırma Nasıl Çalışır?

Sıfır-gözetimli sınıflandırma, önceden eğitilmiş bir modelin (BERT gibi) maskelenmiş dil modelleme (masked language modeling) görevi için kullanılmasını sağlar. Bu görevde, model bir metinde eksik olan kelimeyi tahmin etmeye çalışır.

Örneğin, "Bu bölüm [MASK] konusu hakkındaydı." cümlesinde, model [MASK] yerine uygun bir kelime tahmin etmeye çalışır.

## Kod Örneği

```python
from transformers import pipeline

pipe = pipeline("fill-mask", model="bert-base-uncased")

movie_desc = "The main characters of the movie madacascar are a lion, a zebra, a giraffe, and a hippo. "
prompt = "The movie is about [MASK]."

output = pipe(movie_desc + prompt)

for element in output:
    print(f"Token {element['token_str']}: \t {element['score']:.3f}%")
```

Bu kod, "madacascar" filmi hakkında bir açıklama yapar ve filmin konusu hakkında bir tahminde bulunmaya çalışır. Çıktı:

```
Token animals:  0.103%
Token lions:    0.066%
Token birds:    0.025%
Token love:     0.015%
Token hunting:  0.013%
```

Model, filmin konusu hakkında bir tahminde bulunmak için "animals" kelimesini önerir.

## Doğal Dil Çıkarımı (Natural Language Inference - NLI) ile Sıfır-Gözetimli Sınıflandırma

Doğal dil çıkarımı, iki metin pasajının birbirini takip edip etmediğini veya çeliştiğini belirlemeye çalışan bir görevdir. Sıfır-gözetimli sınıflandırma için, önceden eğitilmiş bir NLI modelini kullanabiliriz.

```python
from transformers import pipeline

pipe = pipeline("zero-shot-classification", device=0)

sample = ds["train"][0]
output = pipe(sample["text"], all_labels, multi_label=True)

print(output["sequence"][:400])
print("\n Predictions:")
for label, score in zip(output["labels"], output["scores"]):
    print(f"{label}, {score:.2f}")
```

Bu kod, bir metni sınıflandırmak için sıfır-gözetimli sınıflandırma kullanır. Çıktı:

```
Predictions:
new model, 0.98
tensorflow or tf, 0.37
examples, 0.34
usage, 0.30
pytorch, 0.25
documentation, 0.25
model training, 0.24
tokenization, 0.17
pipeline, 0.16
```

Model, metni "new model" olarak sınıflandırır.

## Sıfır-Gözetimli Sınıflandırma için Kod Açıklamaları

*   `pipeline("fill-mask", model="bert-base-uncased")`: Maskelenmiş dil modelleme görevi için BERT modelini yükler.
*   `pipe(movie_desc + prompt)`: Metni ve prompt'u modele geçirir ve tahminde bulunmasını sağlar.
*   `pipeline("zero-shot-classification", device=0)`: Sıfır-gözetimli sınıflandırma görevi için NLI modelini yükler.
*   `pipe(sample["text"], all_labels, multi_label=True)`: Metni sınıflandırmak için sıfır-gözetimli sınıflandırma kullanır.

## Sonuç

Sıfır-gözetimli sınıflandırma, hiç etiketli veri bulunmayan durumlarda kullanılan bir tekniktir. Bu teknik, önceden eğitilmiş bir modelin başka bir görev için kullanılmasını sağlar. Doğal dil çıkarımı ile sıfır-gözetimli sınıflandırma, metinleri sınıflandırmak için kullanılabilir.

---

## Working with a Few Labels

# Az Etiketli Veri ile Çalışma (Working with a Few Labels)

NLP projelerinde genellikle en az birkaç etiketli örnekle çalışılır. Bu bölümde, sahip olduğumuz birkaç etiketli örneği en iyi şekilde nasıl kullanabileceğimize bakacağız.

## Veri Artırma (Data Augmentation)

Veri artırma, mevcut eğitim örneklerinden yeni eğitim örnekleri oluşturmak için kullanılan bir tekniktir. Metin sınıflandırma görevlerinde, veri artırma teknikleri kullanarak modelin performansını artırabiliriz.

### Geri Çeviri (Back Translation)

Geri çeviri, bir metni kaynak dilden bir veya daha fazla hedef dile çevirip sonra tekrar kaynak dile çevirme işlemidir. Bu teknik, yüksek kaynaklı diller için iyi çalışır.

### Token Değiştirme (Token Perturbations)

Token değiştirme, metindeki kelimeleri rastgele değiştirmek, eklemek, yerlerini değiştirmek veya silmek gibi basit dönüşümler uygulamayı içerir.

## Kod Örneği: Veri Artırma

```python
from transformers import set_seed
import nlpaug.augmenter.word as naw

set_seed(3)
aug = naw.ContextualWordEmbsAug(model_path="distilbert-base-uncased", device="cpu", action="substitute")
text = "Transformers are the most popular toys"
print(f"Original text: {text}")
print(f"Augmented text: {aug.augment(text)}")
```

*   `set_seed(3)`: Üretken işlemler için tohum değeri belirler.
*   `naw.ContextualWordEmbsAug`: DistilBERT'in bağlamsal kelime gömmelerini kullanarak kelime değiştirme işlemi yapan bir artırıcıdır.
*   `aug.augment(text)`: Girdi metnini artırır.

## Embedding ile Çalışma

Büyük dil modellerinin embedding'leri, metin sınıflandırma görevlerinde kullanılabilir. Bu bölümde, GPT-2 modelini kullanarak metinleri embedding'lemek ve en yakın komşuları bulmak için FAISS indeksini kullanacağız.

### Embedding'leri Hesaplama

```python
import torch
from transformers import AutoTokenizer, AutoModel

model_ckpt = "miguelvictor/python-gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)

def mean_pooling(model_output, attention_mask):
    # Token gömmelerini çıkar
    token_embeddings = model_output[0]
    # Dikkat maskesini hesapla
    input_mask_expanded = (attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float())
    # Gömme vektörlerini topla, maskeli tokenleri yoksay
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    # Ortalamayı tek bir vektör olarak döndür
    return sum_embeddings / sum_mask

def embed_text(examples):
    inputs = tokenizer(examples["text"], padding=True, truncation=True, max_length=128, return_tensors="pt")
    with torch.no_grad():
        model_output = model(**inputs)
    pooled_embeds = mean_pooling(model_output, inputs["attention_mask"])
    return {"embedding": pooled_embeds.cpu().numpy()}
```

*   `mean_pooling`: Token gömmelerini ortalama havuzlama kullanarak tek bir vektörde birleştirir.
*   `embed_text`: Metinleri embedding'lere dönüştürür.

## FAISS İndeksi Oluşturma

```python
embs_train.add_faiss_index("embedding")
```

*   `add_faiss_index`: "embedding" sütununa FAISS indeksi ekler.

## En Yakın Komşuları Bulma

```python
scores, samples = embs_train.get_nearest_examples("embedding", query, k=k)
```

*   `get_nearest_examples`: Belirtilen sorgu embedding'ine en yakın `k` örneği bulur.

## İnce Ayar (Fine-Tuning)

Pretrained bir transformer modelini ince ayar yaparak modelin performansını artırabiliriz.

### Kod Örneği: İnce Ayar

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_ckpt = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, max_length=128)

ds_enc = ds.map(tokenize, batched=True)
ds_enc = ds_enc.remove_columns(['labels', 'text'])
```

*   `tokenize`: Metinleri tokenleştirir.

## Sonuçların Karşılaştırılması

Farklı yöntemlerin performansını karşılaştırdığımızda, veri artırma ve embedding tabanlı yaklaşımların Naive Bayes sınıflandırıcısına göre daha iyi sonuçlar verdiğini görüyoruz. İnce ayar yapılmış transformer modeli de competitive sonuçlar vermektedir.

---

## Leveraging Unlabeled Data

# Etiketlenmemiş Verileri Kullanma (Leveraging Unlabeled Data)

## Giriş

Derin öğrenme modellerinin eğitiminde büyük hacimli yüksek kaliteli etiketli veriye (labeled data) erişim en iyi senaryodur. Ancak bu, etiketlenmemiş verilerin (unlabeled data) değersiz olduğu anlamına gelmez. Transfer öğrenimi (transfer learning) sayesinde, önceden eğitilmiş modelleri (pre-trained models) farklı görevlerde kullanabiliriz.

## Alan Uyarlama (Domain Adaptation)

Eğer bir downstream görevi, önceden eğitme metinlerine (pretraining texts) benzer bir yapıda ise, transfer öğrenimi daha iyi çalışır. Bu nedenle, önceden eğitme görevini downstream görevine yaklaştırarak transferi iyileştirebiliriz. BERT modeli, BookCorpus ve İngilizce Wikipedia üzerinde önceden eğitilmiştir, ancak kod ve GitHub sorunları içeren metinler bu veri kümelerinde küçük bir niş oluşturur. 

### Kod Parçası 1: Tokenization
```python
def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, max_length=128, return_special_tokens_mask=True)

ds_mlm = ds.map(tokenize, batched=True)
ds_mlm = ds_mlm.remove_columns(["labels", "text", "label_ids"])
```
Bu kodda, `tokenize` fonksiyonu metinleri tokenleştirirken (`tokenizer`), özel token maskelerini (`return_special_tokens_mask=True`) döndürür. Bu, daha sonra maskelenmiş dil modellemesi (masked language modeling) için kullanılır.

## Maskelenmiş Dil Modellemesi (Masked Language Modeling)

Maskelenmiş dil modellemesi için, girdi dizisinde tokenleri maskelemek ve hedef tokenleri çıktı olarak almak için bir mekanizmaya ihtiyacımız vardır. Bunu, `DataCollatorForLanguageModeling` kullanarak yapabiliriz.

### Kod Parçası 2: Data Collator
```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```
Bu kodda, `DataCollatorForLanguageModeling` sınıfını kullanarak, tokenleri maskelemek ve hedef tokenleri oluşturmak için bir data collator oluştururuz.

## Model Eğitimi

Modeli eğitmek için, `TrainingArguments` ve `Trainer` sınıflarını kullanırız.

### Kod Parçası 3: Model Eğitimi
```python
from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=f"{model_ckpt}-issues-128",
    per_device_train_batch_size=32,
    logging_strategy="epoch",
    evaluation_strategy="epoch",
    save_strategy="no",
    num_train_epochs=16,
    push_to_hub=True,
    log_level="error",
    report_to="none"
)

trainer = Trainer(
    model=AutoModelForMaskedLM.from_pretrained("bert-base-uncased"),
    tokenizer=tokenizer,
    args=training_args,
    data_collator=data_collator,
    train_dataset=ds_mlm["unsup"],
    eval_dataset=ds_mlm["train"]
)

trainer.train()
trainer.push_to_hub("Training complete!")
```
Bu kodda, `AutoModelForMaskedLM` sınıfını kullanarak, maskelenmiş dil modellemesi için bir model oluştururuz ve `Trainer` sınıfını kullanarak modeli eğitiriz.

## Sonuçlar

Eğitilen modelin performansını değerlendirmek için, `Trainer` sınıfının `predict` metodunu kullanırız.

### Kod Parçası 4: Sonuçların Değerlendirilmesi
```python
pred = trainer.predict(ds_enc['test'])
metrics = compute_metrics(pred)
```
Bu kodda, `Trainer` sınıfının `predict` metodunu kullanarak, test veri kümesi üzerinde tahminler yapar ve `compute_metrics` fonksiyonunu kullanarak metrikleri hesaplarız.

## İleri Teknikler

Bu bölümde, etiketlenmemiş verileri daha da iyi kullanabilmek için bazı ileri teknikler tanıtılmıştır. Bunlar arasında:

*   **Gözetimsiz Veri Artırma (Unsupervised Data Augmentation, UDA)**: Bu teknik, bir modelin tahminlerinin tutarlı olması gerektiğini varsayar. Bunu sağlamak için, orijinal ve bozulan örnekler arasındaki KL diverjansını minimize eder.
*   **Belirsizlik-Bilinçli Kendi Kendine Eğitim (Uncertainty-Aware Self-Training, UST)**: Bu teknik, etiketli veriler üzerinde bir öğretmen modeli eğitir ve sonra bu modeli kullanarak etiketlenmemiş veriler üzerinde pseudo-etiketler oluşturur. Daha sonra, bir öğrenci modeli pseudo-etiketli veriler üzerinde eğitilir.

Bu teknikler, etiketlenmemiş verileri daha etkili bir şekilde kullanarak model performansını artırabilir.

---

## Conclusion

# Düşük Veri Rejiminde (Low-Data Regime) Transformer Modellerinin Gücü

Bu bölümde, etiketli veri miktarının az olduğu veya hiç olmadığı durumlarda bile umudun yitirilmediğini gördük. BERT dil modeli (BERT language model) veya Python kodu üzerinde eğitilmiş GPT-2 gibi başka görevler için önceden eğitilmiş modelleri kullanarak GitHub issue sınıflandırması gibi yeni bir görevde tahminler yapabiliriz. Ayrıca, normal bir sınıflandırma başlığı (classification head) ile model eğitirken ek bir destek elde etmek için alan adaptasyonu (domain adaptation) kullanabiliriz.

## Önemli Noktalar

* Az miktarda etiketli veri ile bile transformer modelleri güçlüdür.
* Önceden eğitilmiş modelleri kullanmak, yeni bir görevde tahminler yapmak için etkilidir.
* Alan adaptasyonu, modelin performansını artırmak için kullanılabilir.
* Değerlendirme pipeline'ı kurmak ve hızlı bir şekilde yinelemek önemlidir.
* Hugging Face Hub'da 10.000'den fazla model bulunmaktadır ve benzer bir problem üzerinde daha önce çalışılmış olma ihtimali yüksektir.

## Kullanılan Kodlar ve Açıklamaları

Bu bölümde spesifik bir kod örneği bulunmamaktadır, ancak Hugging Face Transformers kütüphanesinin esnek API'sinden bahsedilmektedir. Bu kütüphane, modelleri hızlı bir şekilde yüklemeye ve karşılaştırmaya olanak tanır.

Örneğin, bir model yüklemek için aşağıdaki kod kullanılabilir:
```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("model_adi")
```
Bu kod, `model_adi` adlı modeli yükler ve `AutoModelForSequenceClassification` sınıfını kullanarak sequence classification görevi için hazırlar.

* `from transformers import AutoModelForSequenceClassification`: Bu satır, `transformers` kütüphanesinden `AutoModelForSequenceClassification` sınıfını içe aktarır.
* `model = AutoModelForSequenceClassification.from_pretrained("model_adi")`: Bu satır, `model_adi` adlı modeli yükler ve `model` değişkenine atar.

## Teknik Terimler

* BERT (Bidirectional Encoder Representations from Transformers): Transformer tabanlı bir dil modeli.
* GPT-2 (Generative Pre-trained Transformer 2): Transformer tabanlı bir dil modeli.
* Domain Adaptation (Alan Adaptasyonu): Bir modelin farklı bir veri kümesine veya göreve uyum sağlaması için eğitilmesi.
* Classification Head (Sınıflandırma Başlığı): Bir modelin sınıflandırma görevi için kullanılan son katmanı.

---

