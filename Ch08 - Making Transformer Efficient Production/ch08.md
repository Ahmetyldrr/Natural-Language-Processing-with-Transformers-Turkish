## Intent Detection as a Case Study

# Niyet Tespiti (Intent Detection) Örnek Olay İncelemesi

Şirketimizin çağrı merkezine gelen müşterilerin hesap bakiyelerini sorgulayabilmeleri veya rezervasyon yapabilmeleri için insan operatörlere ihtiyaç duymadan metin tabanlı bir asistan oluşturmak istediğimizi varsayalım. Müşterilerin amaçlarını anlamak için, asistanımızın geniş bir yelpazedeki doğal dil metinlerini (natural language text) önceden tanımlanmış eylemler veya niyetler (intents) olarak sınıflandırabilmesi gerekir.

## Niyet Sınıflandırma (Intent Classification)

Örneğin, bir müşteri yaklaşan bir seyahat hakkında aşağıdaki gibi bir mesaj gönderebilir: "Hey, 1 Kasım'dan 15 Kasım'a kadar Paris'te bir araç kiralamak istiyorum ve 15 kişilik bir vasıtaya ihtiyacım var." Asistanımızın niyet sınıflandırıcısı (intent classifier) bunu otomatik olarak "Araç Kiralama" (Car Rental) niyeti olarak sınıflandırabilir ve bu da bir eylem ve yanıtı tetikleyebilir.

## Dışarıdaki Sorguları İşleme (Handling Out-of-Scope Queries)

Üretim ortamında sağlam (robust) olmak için, sınıflandırıcımızın kapsam dışı sorguları (out-of-scope queries) da işleyebilmesi gerekir. Bu, bir müşterinin önceden tanımlanmış niyetlere uymayan bir sorgu yapması durumunda sistemin bir geri dönüş yanıtı vermesi anlamına gelir.

## Temel Model Olarak BERT Kullanma (Using BERT as a Baseline Model)

Temel bir model olarak, CLINC150 veri kümesinde (dataset) yaklaşık %94 doğruluk elde eden bir BERT-base modelini (BERT-base model) ince ayarladık (fine-tuned). Bu veri kümesi, 150 niyet ve 10 alan (domain) gibi bankacılık ve seyahat gibi konularda 22.500 kapsam içi sorguyu ve ayrıca bir "kapsam dışı" (out-of-scope, oos) niyet sınıfına ait 1.200 sorguyu içerir.

### Kod
```python
from transformers import pipeline

bert_ckpt = "transformersbook/bert-base-uncased-finetuned-clinc"
pipe = pipeline("text-classification", model=bert_ckpt)

query = """Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van"""
print(pipe(query))
```

### Kod Açıklaması

1. `from transformers import pipeline`: Bu satır, Hugging Face'in Transformers kütüphanesinden `pipeline` fonksiyonunu içe aktarır. `pipeline`, önceden eğitilmiş modelleri kullanarak çeşitli NLP görevlerini gerçekleştirmeyi kolaylaştıran bir araçtır.
2. `bert_ckpt = "transformersbook/bert-base-uncased-finetuned-clinc"`: Bu satır, ince ayarlanmış BERT modelinin kontrol noktasını (checkpoint) belirler. Bu model, CLINC150 veri kümesinde eğitilmiştir.
3. `pipe = pipeline("text-classification", model=bert_ckpt)`: Bu satır, `pipeline` fonksiyonunu kullanarak bir metin sınıflandırma (text classification) ardışık düzeni (pipeline) oluşturur. `model` parametresi, kullanılacak modeli belirtir.
4. `query = """Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van"""`: Bu satır, sınıflandırılacak metin sorgusunu tanımlar.
5. `print(pipe(query))`: Bu satır, tanımlanan sorguyu `pipeline` ardışık düzenine geçirir ve modelin tahmini niyetini ve güven skorunu yazdırır.

Bu kod, bir metin sorgusunu alıp, önceden eğitilmiş bir BERT modelini kullanarak niyet sınıflandırması yapar ve sonucu döndürür.

---

## Creating a Performance Benchmark

# Performans Karşılaştırması (Performance Benchmark) Oluşturma

Makine öğrenimi modellerini üretim ortamlarında (production environments) dağıtmak, çeşitli kısıtlamalar arasında bir denge kurmayı içerir. Bu kısıtlamaların en yaygın olanları şunlardır:
- Modelin üretim verilerini yansıtan iyi hazırlanmış bir test setinde nasıl performans gösterdiği (accuracy).
- Modelin tahminleri ne kadar hızlı bir şekilde verebildiği (latency).
- Büyük parametreli modellerin (örneğin GPT-2 veya T5) disk depolama ve RAM gereksinimleri (model size).

Bu kısıtlamalara dikkat edilmemesi, uygulamanın kullanıcı deneyimini olumsuz etkileyebilir veya yüksek maliyetlere yol açabilir.

## Performans Karşılaştırması Sınıfı (PerformanceBenchmark Class)

Bu kısıtlamaları optimize etmek için çeşitli sıkıştırma tekniklerini (compression techniques) keşfetmeden önce, belirli bir işlem hattı (pipeline) ve test seti için her bir metriği ölçen basit bir karşılaştırma aracı oluşturalım.

```python
class PerformanceBenchmark:
    def __init__(self, pipeline, dataset, optim_type="BERT baseline"):
        self.pipeline = pipeline
        self.dataset = dataset
        self.optim_type = optim_type

    def compute_accuracy(self):
        # Daha sonra tanımlanacak
        pass

    def compute_size(self):
        # Daha sonra tanımlanacak
        pass

    def time_pipeline(self):
        # Daha sonra tanımlanacak
        pass

    def run_benchmark(self):
        metrics = {}
        metrics[self.optim_type] = self.compute_size()
        metrics[self.optim_type].update(self.time_pipeline())
        metrics[self.optim_type].update(self.compute_accuracy())
        return metrics
```

### Doğruluk (Accuracy) Hesaplama

Doğruluk hesaplama metodunu uygulamak için CLINC150 veri setini kullanacağız.

```python
from datasets import load_dataset, load_metric

clinc = load_dataset("clinc_oos", "plus")
accuracy_score = load_metric("accuracy")

def compute_accuracy(self):
    preds, labels = [], []
    for example in self.dataset:
        pred = self.pipeline(example["text"])[0]["label"]
        label = example["intent"]
        preds.append(intents.str2int(pred))
        labels.append(label)
    accuracy = accuracy_score.compute(predictions=preds, references=labels)
    print(f"Test setindeki doğruluk - {accuracy['accuracy']:.3f}")
    return accuracy
```

### Model Boyutu (Model Size) Hesaplama

Model boyutunu hesaplamak için PyTorch'un `torch.save()` fonksiyonunu kullanacağız.

```python
import torch
from pathlib import Path

def compute_size(self):
    state_dict = self.pipeline.model.state_dict()
    tmp_path = Path("model.pt")
    torch.save(state_dict, tmp_path)
    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)
    tmp_path.unlink()
    print(f"Model boyutu (MB) - {size_mb:.2f}")
    return {"size_mb": size_mb}
```

### İşlem Hattı Zamanı (Pipeline Latency) Hesaplama

İşlem hattının gecikmesini ölçmek için `perf_counter()` fonksiyonunu kullanacağız.

```python
import numpy as np
from time import perf_counter

def time_pipeline(self, query="What is the pin number for my account?"):
    latencies = []
    # Isınma (Warmup)
    for _ in range(10):
        _ = self.pipeline(query)
    # Zamanlı çalışma (Timed run)
    for _ in range(100):
        start_time = perf_counter()
        _ = self.pipeline(query)
        latency = perf_counter() - start_time
        latencies.append(latency)
    time_avg_ms = 1000 * np.mean(latencies)
    time_std_ms = 1000 * np.std(latencies)
    print(f"Ortalama gecikme (ms) - {time_avg_ms:.2f} +/- {time_std_ms:.2f}")
    return {"time_avg_ms": time_avg_ms, "time_std_ms": time_std_ms}
```

## Karşılaştırma Çalıştırma (Running the Benchmark)

Şimdi `PerformanceBenchmark` sınıfını kullanarak BERT taban çizgisini (baseline) değerlendirelim.

```python
pb = PerformanceBenchmark(pipe, clinc["test"])
perf_metrics = pb.run_benchmark()
```

Bu, model boyutu, ortalama gecikme ve test setindeki doğruluk dahil olmak üzere performans metriklerini hesaplayacaktır.

### Kodların Açıklaması

- `compute_accuracy`: Modelin doğruluğunu hesaplar. Veri setindeki her örnek için modelin tahminini alır, gerçek etiketi ile karşılaştırır ve doğruluk skorunu hesaplar.
- `compute_size`: Modelin boyutunu hesaplar. Modelin durum sözlüğünü (state dictionary) diske kaydeder ve dosya boyutunu ölçer.
- `time_pipeline`: Modelin işlem hattının gecikmesini ölçer. Belirli bir sorgu için modelin tahmin üretme süresini ölçer ve ortalama gecikmeyi hesaplar.
- `run_benchmark`: Tüm performans metriklerini hesaplar ve bir sözlük içinde döndürür.

Bu karşılaştırma aracı, çeşitli sıkıştırma tekniklerinin etkisini değerlendirmek için kullanılabilir ve modelin üretim ortamında nasıl performans göstereceğine dair değerli bilgiler sağlar.

---

## Making Models Smaller via Knowledge Distillation

# Bilgi Destilasyonu (Knowledge Distillation) ile Modelleri Küçültme

Bilgi destilasyonu, daha yavaş, daha büyük ama daha iyi performans gösteren bir öğretmen modelinin davranışını taklit etmek için daha küçük bir öğrenci modeli eğitmek için genel amaçlı bir yöntemdir.

## Bilgi Destilasyonu Nasıl Çalışır?

Bilgi destilasyonu, öğretmen modelinin öğrenci modeline "karanlık bilgi" (dark knowledge) aktarmasını sağlar. Bu, öğretmen modelinin ürettiği "yumuşak olasılıklar" (soft probabilities) kullanarak yapılır.

Öğretmen modelinin ürettiği logits değerleri `z(x) = [z1(x), ..., zN(x)]` softmax fonksiyonu kullanılarak olasılıklara dönüştürülür:

`p_i(x) = exp(z_i(x)) / Σ_j exp(z_j(x))`

Ancak, öğretmen modelinin ürettiği olasılıklar genellikle bir sınıfa yüksek olasılık atarken diğer sınıflara düşük olasılık atar. Bu nedenle, olasılıkları "yumuşatmak" (soften) için `T` sıcaklık hiperparametresi kullanılır:

`p_i(x) = exp(z_i(x)/T) / Σ_j exp(z_j(x)/T)`

Yüksek `T` değerleri, olasılık dağılımını yumuşatır ve öğretmen modelinin öğrendiği karar sınırları hakkında daha fazla bilgi sağlar.

Öğrenci modelinin ürettiği olasılıklar `q_i(x)` ile öğretmen modelinin ürettiği olasılıklar `p_i(x)` arasındaki fark Kullback-Leibler (KL) diverjans ile ölçülür:

`L_KD = T^2 * KL(q_i(x) || p_i(x))`

Öğrenci modelinin kaybı, bilgi destilasyonu kaybı `L_KD` ve çapraz entropi kaybı `L_CE` ağırlıklı ortalamasıdır:

`L = α * L_CE + (1-α) * L_KD`

## Kod Uygulaması

Bilgi destilasyonu uygulamak için `DistillationTrainingArguments` ve `DistillationTrainer` sınıfları oluşturulur.

```python
from transformers import TrainingArguments

class DistillationTrainingArguments(TrainingArguments):
    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.alpha = alpha
        self.temperature = temperature
```

```python
import torch.nn as nn
import torch.nn.functional as F
from transformers import Trainer

class DistillationTrainer(Trainer):
    def __init__(self, *args, teacher_model=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model

    def compute_loss(self, model, inputs, return_outputs=False):
        outputs_stu = model(**inputs)
        loss_ce = outputs_stu.loss
        logits_stu = outputs_stu.logits

        with torch.no_grad():
            outputs_tea = self.teacher_model(**inputs)
            logits_tea = outputs_tea.logits

        loss_fct = nn.KLDivLoss(reduction="batchmean")
        loss_kd = self.args.temperature ** 2 * loss_fct(
            F.log_softmax(logits_stu / self.args.temperature, dim=-1),
            F.softmax(logits_tea / self.args.temperature, dim=-1)
        )

        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd
        return (loss, outputs_stu) if return_outputs else loss
```

Kodda, `DistillationTrainer` sınıfının `compute_loss` metodu, öğrenci modelinin kaybını hesaplar. Bu metod, öğretmen modelinin logits değerlerini kullanarak bilgi destilasyonu kaybını hesaplar ve öğrenci modelinin kaybını döndürür.

## Hyperparameter Arama

Hyperparametre arama işlemi için Optuna kütüphanesi kullanılır.

```python
import optuna

def hp_space(trial):
    return {
        "num_train_epochs": trial.suggest_int("num_train_epochs", 5, 10),
        "alpha": trial.suggest_float("alpha", 0, 1),
        "temperature": trial.suggest_int("temperature", 2, 20)
    }

best_run = distilbert_trainer.hyperparameter_search(
    n_trials=20, direction="maximize", hp_space=hp_space
)
```

## Sonuçlar

Bilgi destilasyonu ile öğrenci modeli eğitildikten sonra, modelin performansı test edilir.

Model boyutu (MB) - 255.89
Ortalama gecikme (ms) - 25.96 +/- 1.63
Test seti doğruluğu - 0.868

Sonuçlar, bilgi destilasyonu ile öğrenci modelinin öğretmen modelinin performansına ulaştığını gösterir.

---

## Making Models Faster with Quantization

# Quantization ile Modelleri Hızlandırma (Making Models Faster with Quantization)

Quantization, bir modelin çıkarım (inference) sürecini hızlandırmak için kullanılan bir tekniktir. Bu teknik, modelin ağırlık (weights) ve aktivasyon (activations) değerlerini düşük hassasiyetli (low-precision) veri tiplerine dönüştürerek işlemleri daha verimli hale getirir.

## Quantization'un Temel İlkeleri (Basic Principles of Quantization)

Quantization, floating-point (FP32) değerleri fixed-point (INT8) değerlere dönüştürme işlemidir. Bu işlem, modelin ağırlık ve aktivasyon değerlerini daha düşük hassasiyetli bir veri tipine dönüştürerek işlemleri hızlandırır.

### Floating-Point ve Fixed-Point Sayılar (Floating-Point and Fixed-Point Numbers)

Floating-point sayılar, bir işaret (sign), üs (exponent) ve anlamlı rakamlar (significand) bileşenlerinden oluşur. Örneğin, 137.035 sayısını FP32 formatında temsil etmek için aşağıdaki aritmetik işlem kullanılır:

`1.37035 * 10^2`

Burada, 1.37035 anlamlı rakamları, 2 ise 10 tabanının üssünü temsil eder.

Fixed-point sayılar ise, bir ölçek faktörü (scaling factor) ile çarpılan bir tam sayı olarak temsil edilir. Örneğin, 137.035 sayısını fixed-point formatında temsil etmek için aşağıdaki işlem kullanılır:

`137035 * (1/1000)`

### Quantization İşlemi (Quantization Process)

Quantization işlemi, FP32 değerlerini INT8 değerlere dönüştürme işlemidir. Bu işlem, aşağıdaki formül kullanılarak gerçekleştirilir:

`q = (f / S) + Z`

Burada, `q` quantized değeri, `f` orijinal FP32 değerini, `S` ölçek faktörünü ve `Z` sıfır noktasını (zero point) temsil eder.

## Quantization'un Uygulanması (Applying Quantization)

PyTorch kütüphanesinde quantization işlemi `quantize_per_tensor()` fonksiyonu kullanılarak gerçekleştirilir.

```python
import torch
from torch import quantize_per_tensor

# ağırlık değerleri
weights = ...

# ölçek faktörü ve sıfır noktası hesaplanır
scale = (weights.max() - weights.min()) / (127 - (-128))
zero_point = 0

# quantization işlemi
quantized_weights = quantize_per_tensor(weights, scale, zero_point, torch.qint8)
```

Kod açıklamaları:

*   `weights`: Quantize edilecek ağırlık değerleri
*   `scale`: Ölçek faktörü, FP32 değerlerin INT8 değerlere dönüştürülmesinde kullanılır
*   `zero_point`: Sıfır noktası, FP32 değerlerin INT8 değerlere dönüştürülmesinde kullanılır
*   `quantize_per_tensor()`: PyTorch'un quantization işlemi için kullanılan fonksiyonu
*   `torch.qint8`: Quantized değerlerin veri tipi

## Quantization'un Performans Etkisi (Performance Impact of Quantization)

Quantization, modelin çıkarım sürecini hızlandırır ve bellek kullanımını azaltır. Aşağıdaki kod, FP32 ve INT8 değerlerin çarpım işleminin performansını karşılaştırır:

```python
import torch
import time

# FP32 değerlerin çarpımı
weights_fp32 = ...
start_time = time.time()
result_fp32 = weights_fp32 @ weights_fp32
end_time = time.time()
print(f"FP32 çarpım zamanı: {end_time - start_time} saniye")

# INT8 değerlerin çarpımı
weights_int8 = ...
q_fn = torch.nn.quantized.QFunctional()
start_time = time.time()
result_int8 = q_fn.mul(weights_int8, weights_int8)
end_time = time.time()
print(f"INT8 çarpım zamanı: {end_time - start_time} saniye")
```

Kod açıklamaları:

*   `weights_fp32`: FP32 formatındaki ağırlık değerleri
*   `weights_int8`: INT8 formatındaki ağırlık değerleri
*   `q_fn`: PyTorch'un quantized değerler için kullanılan fonksiyonel sınıfı
*   `mul()`: Quantized değerlerin çarpım işlemi için kullanılan fonksiyon

## Quantization Yöntemleri (Quantization Methods)

Quantization'un üç ana yöntemi vardır:

1.  **Dinamik Quantization (Dynamic Quantization)**: Çıkarım sırasında quantization işlemi dinamik olarak gerçekleştirilir.
2.  **Statik Quantization (Static Quantization)**: Çıkarım öncesinde quantization şeması belirlenir ve çıkarım sırasında kullanılır.
3.  **Quantization-Aware Training (Quantization-Aware Training)**: Eğitim sırasında quantization işlemi simüle edilir.

## PyTorch'ta Dinamik Quantization (Dynamic Quantization in PyTorch)

PyTorch'ta dinamik quantization işlemi `quantize_dynamic()` fonksiyonu kullanılarak gerçekleştirilir.

```python
from torch.quantization import quantize_dynamic

# model yüklenir
model = ...

# dinamik quantization işlemi
model_quantized = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
```

Kod açıklamaları:

*   `model`: Quantize edilecek model
*   `quantize_dynamic()`: PyTorch'un dinamik quantization işlemi için kullanılan fonksiyonu
*   `{torch.nn.Linear}`: Quantize edilecek katman tipleri
*   `dtype=torch.qint8`: Quantized değerlerin veri tipi

---

## Benchmarking Our Quantized Model

# Nicelenmiş Modelimizin Karşılaştırılması (Benchmarking Our Quantized Model)

Nicelenmiş (quantized) modelimizi şimdi benchmark (karşılaştırma) işlemine tabi tutalım ve sonuçları görselleştirelim.

## Kod Parçası
```python
pipe = pipeline("text-classification", model=model_quantized, tokenizer=tokenizer)
optim_type = "Distillation + quantization"
pb = PerformanceBenchmark(pipe, clinc["test"], optim_type=optim_type)
perf_metrics.update(pb.run_benchmark())
```
## Kod Açıklaması

1. `pipe = pipeline("text-classification", model=model_quantized, tokenizer=tokenizer)`: 
   - Bu satırda, `pipeline` fonksiyonu kullanılarak bir `pipe` objesi oluşturulur. 
   - `model=model_quantized` parametresi, nicelenmiş modelin (`model_quantized`) kullanıldığını belirtir.
   - `tokenizer=tokenizer` parametresi, kullanılan tokenleştiricinin (`tokenizer`) ne olduğunu belirtir.

2. `optim_type = "Distillation + quantization"`: 
   - Bu satırda, optimizasyon türü (`optim_type`) "Distillation + quantization" olarak belirlenir.

3. `pb = PerformanceBenchmark(pipe, clinc["test"], optim_type=optim_type)`: 
   - `PerformanceBenchmark` sınıfından bir `pb` objesi oluşturulur.
   - `pipe` parametresi, daha önce oluşturulan `pipe` objesini temsil eder.
   - `clinc["test"]` parametresi, test veri setini (`clinc` veri setinin test kısmını) temsil eder.
   - `optim_type=optim_type` parametresi, optimizasyon türünü belirtir.

4. `perf_metrics.update(pb.run_benchmark())`: 
   - `pb` objesi üzerinde `run_benchmark` metodu çağrılır ve bu metodun döndürdüğü değerler `perf_metrics` üzerine güncellenir (`update` metodu ile).

## Sonuçlar

- Model boyutu (Model size) : 132.40 MB
- Ortalama gecikme (Average latency) : 12.54 ms ± 0.73 ms
- Test seti üzerindeki doğruluk (Accuracy on test set) : 0.876

Nicelenmiş modelimizin, destile (distilled) modelimizin neredeyse yarısı kadar boyuta sahip olduğu ve doğrulukta hafif bir artış sağladığı görülmektedir.

## Görselleştirme

`plot_metrics(perf_metrics, optim_type)` fonksiyonu kullanılarak performans metrikleri (`perf_metrics`) görselleştirilir.

## İleri Optimizasyon

Şimdi, ONNX Runtime adı verilen güçlü bir çerçeve (framework) kullanarak optimizasyonumuzu sınırına kadar zorlayalım.

---

## Optimizing Inference with ONNX and the ONNX Runtime

# Optimizing Inference with ONNX and the ONNX Runtime

ONNX (Open Neural Network Exchange), derin öğrenme modellerini çeşitli çerçevelerde (PyTorch, TensorFlow gibi) temsil etmek için ortak bir operatör seti ve dosya formatı tanımlar. Bir model ONNX formatına dışa aktarıldığında, bu operatörler, veri akışını sinir ağından temsil eden bir hesaplamalı grafik (ara gösterim) oluşturmak için kullanılır.

## ONNX ile Model Dışa Aktarma

ONNX formatına dışa aktarma işlemi, Transformers kütüphanesindeki `convert_graph_to_onnx.convert()` fonksiyonu ile gerçekleştirilir. Bu fonksiyon, modeli bir Pipeline olarak başlatır, yer tutucu girdileri pipeline üzerinden geçirir, dinamik eksenleri tanımlar ve ağı ağ parametreleri ile kaydeder.

```python
import os
from psutil import cpu_count
os.environ["OMP_NUM_THREADS"] = f"{cpu_count()}"
os.environ["OMP_WAIT_POLICY"] = "ACTIVE"

from transformers.convert_graph_to_onnx import convert
model_ckpt = "transformersbook/distilbert-base-uncased-distilled-clinc"
onnx_model_path = Path("onnx/model.onnx")
convert(
    framework="pt",
    model=model_ckpt,
    tokenizer=tokenizer,
    output=onnx_model_path,
    opset=12,
    pipeline_name="text-classification"
)
```

*   `os.environ["OMP_NUM_THREADS"] = f"{cpu_count()}"` : OpenMP için iş parçacık sayısını ayarlar.
*   `os.environ["OMP_WAIT_POLICY"] = "ACTIVE"` : Bekleyen iş parçacıklarının aktif olmasını sağlar.
*   `convert()` : Modeli ONNX formatına dönüştürür.

## ONNX Modeli ile Çıkarım Yapmak

ONNX modeliyle çıkarım yapmak için bir `InferenceSession` örneği oluşturulur.

```python
from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions

def create_model_for_provider(model_path, provider="CPUExecutionProvider"):
    options = SessionOptions()
    options.intra_op_num_threads = 1
    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL
    session = InferenceSession(str(model_path), options, providers=[provider])
    session.disable_fallback()
    return session

onnx_model = create_model_for_provider(onnx_model_path)
```

*   `create_model_for_provider()` : Belirtilen sağlayıcı için bir `InferenceSession` örneği oluşturur.
*   `InferenceSession` : ONNX modeliyle çıkarım yapmak için kullanılır.

## ONNX Modelinin Performansını Değerlendirmek

ONNX modelinin performansını değerlendirmek için bir `OnnxPerformanceBenchmark` sınıfı oluşturulur.

```python
class OnnxPerformanceBenchmark(PerformanceBenchmark):
    def __init__(self, *args, model_path, **kwargs):
        super().__init__(*args, **kwargs)
        self.model_path = model_path

    def compute_size(self):
        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)
        print(f"Model size (MB) - {size_mb:.2f}")
        return {"size_mb": size_mb}
```

*   `OnnxPerformanceBenchmark` : ONNX modelinin performansını değerlendirmek için kullanılır.
*   `compute_size()` : Modelin boyutunu hesaplar.

## Nicemleme (Quantization)

ONNX Runtime, dinamik, statik ve nicemleme farkında eğitim gibi üç nicemleme yöntemi sunar. Burada dinamik nicemleme uygulanır.

```python
from onnxruntime.quantization import quantize_dynamic, QuantType

model_input = "onnx/model.onnx"
model_output = "onnx/model.quant.onnx"
quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)
```

*   `quantize_dynamic()` : Dinamik nicemleme uygular.
*   `QuantType.QInt8` : Ağırlıkları int8 türüne dönüştürür.

## Nicemlenmiş Modelin Performansını Değerlendirmek

Nicemlenmiş modelin performansını değerlendirmek için aynı `OnnxPerformanceBenchmark` sınıfı kullanılır.

```python
onnx_quantized_model = create_model_for_provider(model_output)
pipe = OnnxPipeline(onnx_quantized_model, tokenizer)
optim_type = "Distillation + ORT (quantized)"
pb = OnnxPerformanceBenchmark(pipe, clinc["test"], optim_type, model_path=model_output)
perf_metrics.update(pb.run_benchmark())
```

*   `create_model_for_provider()` : Nicemlenmiş model için bir `InferenceSession` örneği oluşturur.
*   `OnnxPerformanceBenchmark` : Nicemlenmiş modelin performansını değerlendirir.

Bu analiz, ONNX ve ONNX Runtime kullanarak transformer modellerinin çıkarımını hızlandırma tekniklerini gösterir. Nicemleme ve diğer optimizasyon teknikleri, model boyutunu ve gecikmesini azaltmada etkilidir.

---

## Making Models Sparser with Weight Pruning

# Modeli Seyrekleştirme: Ağırlık Budama (Weight Pruning)

Modeli hızlandırmak için kullanılan yöntemlerden biri de ağırlık budama (weight pruning) yöntemidir. Bu yöntem, modelin parametre sayısını azaltarak modelin boyutunu küçültür.

## Ağırlık Budama Nedir?

Ağırlık budama, modelin ağırlık matrisindeki en önemsiz ağırlıkları tanımlayarak ve kaldırarak modelin seyrekleştirilmesini sağlar. Bu sayede, modelin boyutu küçültülür ve model daha hızlı çalışır hale gelir.

## Ağırlık Budama Nasıl Çalışır?

Ağırlık budama yöntemi, bir önem skoru matrisi (importance score matrix) **S** hesaplar ve daha sonra en önemli ağırlıkları seçer. Bu skorlar, ağırlıkların büyüklüğüne göre hesaplanır: **S** = |**W**|. Daha sonra, bu skorlara göre bir maske matrisi (mask matrix) **M** oluşturulur: **M** = Top k(**S**).

## Magnitude Pruning (Büyüklük Budama)

Magnitude pruning, ağırlık budama yöntemlerinden biridir. Bu yöntem, ağırlıkların büyüklüğüne göre önem skoru hesaplar ve en küçük ağırlıkları kaldırır.

```python
import numpy as np

# Ağırlık matrisi
W = np.random.rand(10, 10)

# Önem skoru matrisi
S = np.abs(W)

# Maske matrisi
M = np.where(S > np.percentile(S, 50), 1, 0)

# Seyrek ağırlık matrisi
W_sparse = W * M
```

Kod açıklaması:

1. `W = np.random.rand(10, 10)`: 10x10 boyutunda rastgele bir ağırlık matrisi oluşturur.
2. `S = np.abs(W)`: Ağırlık matrisinin mutlak değerini alarak önem skoru matrisini hesaplar.
3. `M = np.where(S > np.percentile(S, 50), 1, 0)`: Önem skoru matrisindeki değerlerin %50'sinden büyük olanları 1, diğerlerini 0 olarak maske matrisine atar.
4. `W_sparse = W * M`: Ağırlık matrisini maske matrisi ile çarparak seyrek ağırlık matrisini oluşturur.

## Movement Pruning (Hareket Budama)

Movement pruning, ağırlık budama yöntemlerinden bir diğeridir. Bu yöntem, hem ağırlıkları hem de önem skorlarını öğrenir ve ağırlıkları budar.

```python
import torch
import torch.nn as nn

class MovementPruning(nn.Module):
    def __init__(self, weights):
        super(MovementPruning, self).__init__()
        self.weights = nn.Parameter(weights)
        self.scores = nn.Parameter(torch.zeros_like(weights))

    def forward(self, x):
        mask = torch.where(self.scores > torch.percentile(self.scores, 50), 1, 0)
        sparse_weights = self.weights * mask
        return x @ sparse_weights

# Ağırlık matrisi
weights = torch.randn(10, 10)

# Movement pruning modeli
model = MovementPruning(weights)

# İleri besleme
x = torch.randn(10)
output = model(x)
```

Kod açıklaması:

1. `class MovementPruning(nn.Module)`: Movement pruning modelini tanımlar.
2. `self.weights = nn.Parameter(weights)`: Ağırlıkları parametre olarak tanımlar.
3. `self.scores = nn.Parameter(torch.zeros_like(weights))`: Önem skorlarını parametre olarak tanımlar.
4. `mask = torch.where(self.scores > torch.percentile(self.scores, 50), 1, 0)`: Önem skoru matrisindeki değerlerin %50'sinden büyük olanları 1, diğerlerini 0 olarak maske matrisine atar.
5. `sparse_weights = self.weights * mask`: Ağırlıkları maske matrisi ile çarparak seyrek ağırlıkları oluşturur.

## Sonuç

Ağırlık budama yöntemleri, modelin boyutunu küçültmek ve modelin hızını artırmak için kullanılır. Magnitude pruning ve movement pruning gibi yöntemler, modelin seyrekleştirilmesini sağlar. Bu yöntemler, özellikle büyük modellerde ve kaynak kısıtlı uygulamalarda önemlidir.

---

## Conclusion

# Üretim Ortamlarında Transformer Optimizasyonu Sonuçları (Transformer Optimization Results in Production Environments)

Üretim ortamlarında transformer modellerinin optimizasyonu, gecikme (latency) ve bellek ayak izi (memory footprint) olmak üzere iki boyutlu sıkıştırma içerir. İnce ayarlı bir modelden başlayarak, distilasyon (distillation), quantizasyon (quantization) ve ORT (ONNX Runtime) aracılığıyla optimizasyonlar uygulayarak her ikisini de önemli ölçüde azalttık.

# Kullanılan Teknikler (Techniques Used)

*   Distilasyon (Distillation): Büyük bir modelin bilgisini daha küçük bir modele aktarmak için kullanılan bir tekniktir. 
*   Quantizasyon (Quantization): Modelin ağırlıklarını ve aktivasyonlarını daha düşük hassasiyetli temsil etmek için kullanılan bir tekniktir. Örneğin, `float32` tipindeki ağırlıkları `int8` tipine dönüştürmek gibi.
*   ORT (ONNX Runtime): Modeli optimize etmek ve çalıştırmak için kullanılan bir runtime ortamıdır.

# Kod Örnekleri (Code Examples)

ORT kullanarak quantizasyon ve model optimizasyonu yapmak için aşağıdaki kod örneği kullanılabilir:
```python
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

# Modeli yükle
model = onnx.load("model.onnx")

# Quantizasyon uygula
quantized_model = quantize_dynamic(
    model,
    weight_type=QuantType.QInt8,
    activation_type=QuantType.QInt8
)

# Optimize edilmiş modeli kaydet
onnx.save(quantized_model, "quantized_model.onnx")
```
Bu kod örneğinde, `quantize_dynamic` fonksiyonu kullanılarak modelin ağırlıkları ve aktivasyonları `int8` tipine dönüştürülür.

# Kod Açıklaması (Code Explanation)

1.  `import onnx`: ONNX kütüphanesini içe aktarır.
2.  `from onnxruntime.quantization import quantize_dynamic, QuantType`: ONNX Runtime quantizasyon modülünden `quantize_dynamic` ve `QuantType` sınıflarını içe aktarır.
3.  `model = onnx.load("model.onnx")`: `model.onnx` dosyasından modeli yükler.
4.  `quantized_model = quantize_dynamic(...)`: Modelin quantizasyonunu uygular. 
    *   `weight_type=QuantType.QInt8`: Ağırlıkların `int8` tipine dönüştürülmesini sağlar.
    *   `activation_type=QuantType.QInt8`: Aktivasyonların `int8` tipine dönüştürülmesini sağlar.
5.  `onnx.save(quantized_model, "quantized_model.onnx")`: Optimize edilmiş modeli `quantized_model.onnx` dosyasına kaydeder.

# Sonuç (Conclusion)

Üretim ortamlarında transformer modellerinin optimizasyonu için distilasyon, quantizasyon ve ORT gibi teknikler kullanılabilir. Quantizasyon ve ORT kullanarak modelin gecikme ve bellek ayak izi önemli ölçüde azaltılabilir. Gelecekte, donanımın seyrek matris işlemleri için optimize edilmesiyle, budama (pruning) gibi tekniklerin daha etkili hale gelmesi beklenmektedir.

---

